{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wnk4Bjim_rRG",
        "outputId": "99fa937c-0f7b-45ed-871f-d9c977a9a94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Training images: 300\n",
            "Training masks: 300\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set dataset path\n",
        "data_dir = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack\"  # Update this path if needed\n",
        "\n",
        "train_img_dir = os.path.join(data_dir, \"train_img\")\n",
        "train_mask_dir = os.path.join(data_dir, \"train_lab\")\n",
        "test_img_dir = os.path.join(data_dir, \"test_img\")\n",
        "test_mask_dir = os.path.join(data_dir, \"test_lab\")\n",
        "\n",
        "# Check if the dataset is correctly loaded\n",
        "print(\"Training images:\", len(os.listdir(train_img_dir)))\n",
        "print(\"Training masks:\", len(os.listdir(train_mask_dir)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack\"  # Update if needed\n",
        "train_img_dir = os.path.join(data_dir, \"train_img\")\n",
        "train_mask_dir = os.path.join(data_dir, \"train_lab\")\n",
        "test_img_dir = os.path.join(data_dir, \"test_img\")\n",
        "test_mask_dir = os.path.join(data_dir, \"test_lab\")\n"
      ],
      "metadata": {
        "id": "B40ME8cnD0L2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack\"\n",
        "train_img_dir = os.path.join(data_dir, \"train_img\")\n",
        "train_mask_dir = os.path.join(data_dir, \"train_lab\")\n",
        "img_size = (256, 256)\n",
        "batch_size = 16\n",
        "def load_images(image_dir, mask_dir, img_size):\n",
        "    images = []\n",
        "    masks = []\n",
        "    for img_name in os.listdir(image_dir):\n",
        "        img_path = os.path.join(image_dir, img_name)\n",
        "        mask_path = os.path.join(mask_dir, img_name)\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "        img = cv2.resize(img, img_size)\n",
        "        img = img / 255.0\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, img_size)\n",
        "        mask = mask / 255.0\n",
        "        mask = np.expand_dims(mask, axis=-1)\n",
        "        images.append(img)\n",
        "        masks.append(mask)\n",
        "    return np.array(images), np.array(masks)\n",
        "X, Y = load_images(train_img_dir, train_mask_dir, img_size)\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "def build_unet_model(input_shape):\n",
        "    inputs = layers.Input(input_shape)\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
        "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
        "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
        "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
        "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
        "    u6 = layers.UpSampling2D((2, 2))(c5)\n",
        "    u6 = layers.concatenate([u6, c4])\n",
        "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
        "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
        "    u7 = layers.UpSampling2D((2, 2))(c6)\n",
        "    u7 = layers.concatenate([u7, c3])\n",
        "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
        "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
        "    u8 = layers.UpSampling2D((2, 2))(c7)\n",
        "    u8 = layers.concatenate([u8, c2])\n",
        "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
        "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
        "    u9 = layers.UpSampling2D((2, 2))(c8)\n",
        "    u9 = layers.concatenate([u9, c1])\n",
        "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
        "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
        "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "model = build_unet_model((256, 256, 3))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=batch_size)\n",
        "def predict_and_show(image):\n",
        "    pred_mask = model.predict(np.expand_dims(image, axis=0))[0]\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(pred_mask.squeeze(), cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Predicted Crack Mask\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "SPPSmtJCD9qF",
        "outputId": "128cbe82-c16f-43df-98b2-80c6737874b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8b62834fc6cc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_img_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mask_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_unet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-8b62834fc6cc>\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(image_dir, mask_dir, img_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for img_name in os.listdir(train_mask_dir):\n",
        "    mask_path = os.path.join(train_mask_dir, img_name)\n",
        "    if not os.path.exists(mask_path):\n",
        "        print(\"Missing:\", mask_path)\n"
      ],
      "metadata": {
        "id": "SLjwMK38EKYX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_mask_path = os.path.join(train_mask_dir, os.listdir(train_mask_dir)[0])\n",
        "mask = cv2.imread(sample_mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "if mask is None:\n",
        "    print(f\"Failed to load mask: {sample_mask_path}\")\n",
        "else:\n",
        "    plt.imshow(mask, cmap='gray')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "-KtWBCh8ENOJ",
        "outputId": "379188d3-2417-4bd9-f232-e88668cbf287"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corrupted_files = []\n",
        "for img_name in os.listdir(train_mask_dir):\n",
        "    mask_path = os.path.join(train_mask_dir, img_name)\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if mask is None:\n",
        "        corrupted_files.append(mask_path)\n",
        "\n",
        "print(\"Corrupted files:\", corrupted_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3-xmuNlEUio",
        "outputId": "80e54477-caf4-432b-f087-1108e2b9313c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrupted files: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for img_name in os.listdir(train_mask_dir):\n",
        "    if not img_name.endswith(\".png\"):  # Change based on your dataset format\n",
        "        print(f\"Skipping non-image file: {img_name}\")\n",
        "        continue  # Skips non-image files\n",
        "\n",
        "    mask_path = os.path.join(train_mask_dir, img_name)\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if mask is None:\n",
        "        print(f\"Failed to load mask: {mask_path}\")\n"
      ],
      "metadata": {
        "id": "nSWWy4xAEbcB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check if images and masks are loaded correctly\n",
        "print(\"Training images:\", len(os.listdir(train_img_dir)))\n",
        "print(\"Training masks:\", len(os.listdir(train_mask_dir)))\n",
        "\n",
        "# Image size for resizing\n",
        "img_size = (256, 256)\n",
        "\n",
        "# Load & Preprocess Images\n",
        "def load_images(image_dir, mask_dir, img_size):\n",
        "    images = []\n",
        "    masks = []\n",
        "    for img_name in os.listdir(image_dir):\n",
        "        img_path = os.path.join(image_dir, img_name)\n",
        "        mask_path = os.path.join(mask_dir, img_name)\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, img_size)\n",
        "        img = img / 255.0  # Normalize\n",
        "\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        mask = cv2.resize(mask, img_size)\n",
        "        mask = mask / 255.0  # Normalize\n",
        "        mask = np.expand_dims(mask, axis=-1)  # Add channel dimension\n",
        "\n",
        "        images.append(img)\n",
        "        masks.append(mask)\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "# Load Training Data\n",
        "X, Y = load_images(train_img_dir, train_mask_dir, img_size)\n",
        "\n",
        "# Split into Training & Validation Set\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# U-Net Model\n",
        "def build_unet_model(input_shape=(256, 256, 3)):\n",
        "    inputs = layers.Input(input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
        "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
        "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
        "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    # Bottleneck\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
        "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
        "\n",
        "    # Decoder\n",
        "    u4 = layers.UpSampling2D((2, 2))(c3)\n",
        "    c4 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u4)\n",
        "    c4 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c4)\n",
        "\n",
        "    u5 = layers.UpSampling2D((2, 2))(c4)\n",
        "    c5 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u5)\n",
        "    c5 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c5)\n",
        "\n",
        "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Compile Model\n",
        "model = build_unet_model()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, batch_size=8)\n",
        "\n",
        "# Save Model\n",
        "model.save(\"/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/unet_crack_segmentation.h5\")\n",
        "\n",
        "# Load & Predict on Test Images\n",
        "def predict_and_visualize(model, img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, img_size) / 255.0\n",
        "    img_input = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "\n",
        "    pred_mask = model.predict(img_input)[0]  # Get first image prediction\n",
        "    pred_mask = (pred_mask > 0.5).astype(np.uint8)  # Thresholding\n",
        "\n",
        "    # Plot original & predicted mask\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"Original Image\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(pred_mask.squeeze(), cmap='gray')\n",
        "    plt.title(\"Predicted Crack Mask\")\n",
        "    plt.show()\n",
        "\n",
        "# Test on a random image\n",
        "test_img_path = os.path.join(test_img_dir, os.listdir(test_img_dir)[0])\n",
        "predict_and_visualize(model, test_img_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "Lj2s4nnSE5Ci",
        "outputId": "32f6097b-620e-488b-a792-bc494fd90080"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: 300\n",
            "Training masks: 300\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1a5d5930e8cf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Load Training Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_img_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mask_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Split into Training & Validation Set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-1a5d5930e8cf>\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(image_dir, mask_dir, img_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m  \u001b[0;31m# Normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add channel dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LInyh-uQFXE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "missing_files = []\n",
        "for img_name in os.listdir(train_img_dir):\n",
        "    mask_path = os.path.join(train_mask_dir, img_name)\n",
        "    if not os.path.exists(mask_path):\n",
        "        missing_files.append(mask_path)\n",
        "\n",
        "if missing_files:\n",
        "    print(\"Missing mask files:\", missing_files)\n",
        "else:\n",
        "    print(\"All masks are present.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwYRdLNUFTQo",
        "outputId": "a98c1092-1c29-4d18-9a52-1194e0afa54a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing mask files: ['/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG27-11.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-12.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG27-10.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG88.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG_6536-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG27-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG25-9.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-13.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG27-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG27-7.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG27-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG36-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG39-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG27-12.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-9.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-18.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-7.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG74.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG49.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG27-8.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-15.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG27-9.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-17.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG25-7.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-11.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG36-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG56.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG27-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG39-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG27-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG25-8.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-16.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-14.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-8.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG36-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG27-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG27-13.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG33-10.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG11-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG25-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-19.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG25-10.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG12-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG25-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG13-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG13-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-16.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-8.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-11.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG25-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG20.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-18.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG11-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG13-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG25-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG25-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-13.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-8.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG14-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG12.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG16.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG11-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG11-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-14.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-9.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG12-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG13-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-7.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG14-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-9.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-20.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG18.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG21.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG13-7.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG13-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG14-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-20.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-10.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-15.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-7.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG19.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG11-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG13-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-17.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG11-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG17.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-12.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG25-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG136.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG14-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/IMG108.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9064-19.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11289-8.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11192-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-14.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11289-7.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11289-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11289-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11200-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-10.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11187.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11190-7.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-11.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11204.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11190-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11193.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-12.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11190-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11197-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11188-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11188-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11190-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11188.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11189-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-15.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11289-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11202.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11193-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-13.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11193-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11193-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11197-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11190-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11196.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11190-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-18.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11192.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11189.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11190.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11289-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-16.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11190-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11203.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11189-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11197-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/7Q3A9060-17.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11289-10.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11289-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11200-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11289-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11198.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11197-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11192-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11289-9.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11162-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11167-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11166.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11187-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11179-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11178-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11165.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11179-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11178-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11175-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11178.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11166-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11179-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11162.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11187-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11182-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11187-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11163.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11187-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11166-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11179-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11177.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11168.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11178-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11163-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11167-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11185.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11181.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11179.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11165-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11165-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11175.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11183.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11180.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11187-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11164-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11176.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11167.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11182-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11184.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11170-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11184-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11168-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11175-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11187-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11164-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11179-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11178-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11170.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11184-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11175-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11185-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11186.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11170-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11148.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11151.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11153.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11140-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11138.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11156.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11136.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11155-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11140.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11151-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11159.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11151-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11151-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11155-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11141-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11134-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11154.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11141.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11155-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11134-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11162-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11162-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11134-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11155-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11135.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11142.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11150.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11140-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11149.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11143.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11152.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11137.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11134-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11157.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11155.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11156-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11144.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11134-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11151-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11141-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11158.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11162-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11139.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11134.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11157-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11141-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11133.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11134-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11156-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11141-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11122.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11120.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11112.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11124.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11123-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11126.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11116-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11121.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11116-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11116.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11123-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11122-1.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11123-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11115.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11118.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11114.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11123-6.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11117.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11122-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11123.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11123-5.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11130.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11119.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11122-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11131.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11116-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11122-4.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11132.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11122-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11116-3.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11123-2.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11113.jpg', '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/11111.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = \"/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_lab/\"\n",
        "print(os.listdir(path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHwuvBQSFw-c",
        "outputId": "402dc636-4472-40e2-83ca-9c68624aa32d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['11111.png', '11152.png', '11156.png', '11141-1.png', '11170-2.png', '11117.png', '11134-1.png', '11166.png', '11123.png', '11116-2.png', '11184-2.png', '11186.png', '11155-1.png', '11178-2.png', '11179-2.png', '11122-4.png', '11178-1.png', '11185-1.png', '11112.png', '11168-2.png', '11116-4.png', '11151.png', '11151-1.png', '11162-4.png', '11163-1.png', '11154.png', '11162.png', '11150.png', '11148.png', '11164-2.png', '11136.png', '11178.png', '11168.png', '11184-1.png', '11187-3.png', '11123-1.png', '11175-2.png', '11181.png', '11166-2.png', '11187-4.png', '11132.png', '11179-1.png', '11123-4.png', '11167-2.png', '11179-4.png', '11119.png', '11143.png', '11179-5.png', '11140.png', '11118.png', '11153.png', '11157.png', '11165-2.png', '11138.png', '11155-2.png', '11164-4.png', '11167.png', '11134-5.png', '11134-4.png', '11114.png', '11123-6.png', '11142.png', '11167-1.png', '11131.png', '11116-3.png', '11122-2.png', '11165.png', '11162-2.png', '11123-3.png', '11182-2.png', '11163.png', '11124.png', '11162-1.png', '11126.png', '11133.png', '11156-1.png', '11155-3.png', '11135.png', '11122-5.png', '11134-2.png', '11116.png', '11151-4.png', '11115.png', '11113.png', '11157-1.png', '11144.png', '11162-3.png', '11175-1.png', '11141-3.png', '11134.png', '11155.png', '11184.png', '11134-6.png', '11141.png', '11178-3.png', '11179-3.png', '11139.png', '11170.png', '11180.png', '11178-4.png', '11116-1.png', '11182-1.png', '11183.png', '11149.png', '11175-3.png', '11122.png', '11134-3.png', '11141-2.png', '11122-1.png', '11187-1.png', '11158.png', '11159.png', '11140-1.png', '11176.png', '11175.png', '11137.png', '11155-4.png', '11121.png', '11165-1.png', '11122-3.png', '11156-2.png', '11177.png', '11140-6.png', '11123-5.png', '11151-2.png', '11130.png', '11185.png', '11179.png', '11170-1.png', '11141-4.png', '11166-1.png', '11187-2.png', '11120.png', '11151-3.png', '11187-5.png', '11123-2.png', '7Q3A9060-4.png', 'IMG25-1.png', '11289-3.png', 'IMG25-10.png', 'IMG11-2.png', '7Q3A9060-1.png', '11192-1.png', '11197-4.png', '7Q3A9060-18.png', '7Q3A9064-5.png', '7Q3A9060-8.png', 'IMG88.png', 'IMG25-2.png', 'IMG13-5.png', 'IMG13-1.png', '11289-10.png', '11202.png', '11190-2.png', 'IMG14-2.png', 'IMG25-5.png', 'IMG11-3.png', '11187-6.png', '11204.png', 'IMG27-8.png', '11203.png', 'IMG33-6.png', '7Q3A9064-4.png', 'IMG5.png', '11190-7.png', 'IMG49.png', '11289-8.png', '7Q3A9060-14.png', '7Q3A9064-16.png', 'IMG33-12.png', 'IMG11-1.png', 'IMG27-3.png', '7Q3A9064-13.png', 'IMG33-14.png', '11289-9.png', 'IMG27-5.png', '7Q3A9064-20.png', '7Q3A9060-20.png', 'IMG27-6.png', 'IMG33-5.png', 'IMG14-3.png', '11187.png', '11192.png', '7Q3A9064-7.png', '11193-2.png', 'IMG27-11.png', 'IMG33-2.png', 'IMG19.png', 'IMG17.png', 'IMG25-7.png', 'IMG16.png', 'IMG25-8.png', '11197-2.png', 'IMG14-4.png', 'IMG33-15.png', '7Q3A9064-14.png', '7Q3A9060-5.png', 'IMG74.png', '11190-4.png', '11197-1.png', '11200-2.png', '11193-3.png', '11289-1.png', 'IMG11-6.png', '7Q3A9060-16.png', 'IMG20.png', '11196.png', 'IMG12-2.png', '11188.png', 'IMG27-4.png', '7Q3A9060-15.png', '7Q3A9064-10.png', '7Q3A9060-6.png', '7Q3A9064-15.png', 'IMG33-7.png', '11189-2.png', '11289-4.png', 'IMG18.png', '7Q3A9060-3.png', 'IMG27-1.png', 'IMG25-3.png', 'IMG33-18.png', 'IMG13-3.png', '11189-1.png', '11188-1.png', 'IMG27-10.png', 'IMG33-9.png', 'IMG33-17.png', '7Q3A9064-1.png', '11189.png', 'IMG36-3.png', 'IMG27-7.png', 'IMG12.png', 'IMG12-1.png', '11193-1.png', '7Q3A9064-2.png', 'IMG136.png', '11190-3.png', 'IMG33-3.png', '7Q3A9060-19.png', 'IMG25-6.png', 'IMG27-9.png', '11190-5.png', 'IMG33-11.png', '7Q3A9064-3.png', '11192-2.png', 'IMG13-7.png', 'IMG33-16.png', '7Q3A9064-18.png', '7Q3A9064-11.png', '11289-6.png', '11190-1.png', 'IMG_6536-6.png', 'IMG108.png', '7Q3A9060-13.png', 'IMG13-4.png', '11193.png', '11198.png', 'IMG36-1.png', 'IMG56.png', '11200-1.png', '11289-5.png', 'IMG33-10.png', '11188-2.png', 'IMG36-2.png', 'IMG33-4.png', 'IMG25-4.png', 'IMG25-9.png', 'IMG11-4.png', 'IMG39-1.png', 'IMG11-5.png', 'IMG33-8.png', 'IMG21.png', '7Q3A9060-7.png', '7Q3A9064-19.png', 'IMG27-13.png', '7Q3A9060-9.png', '11289-7.png', '11190.png', '7Q3A9064-12.png', '7Q3A9064-9.png', 'IMG27-2.png', '7Q3A9064-17.png', '11289-2.png', 'IMG33-1.png', 'IMG13-6.png', '7Q3A9060-10.png', 'IMG14-1.png', 'IMG27-12.png', '7Q3A9060-12.png', 'IMG13-2.png', '11197-3.png', '7Q3A9060-11.png', 'IMG33-13.png', '7Q3A9060-2.png', '7Q3A9060-17.png', '11190-6.png', '7Q3A9064-8.png', 'IMG39-2.png', '7Q3A9064-6.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# List of image filenames provided by the user\n",
        "image_files = [\n",
        "    '11111.png', '11152.png', '11156.png', '11141-1.png', '11170-2.png', '11117.png', '11134-1.png', '11166.png',\n",
        "    '11123.png', '11116-2.png', '11184-2.png', '11186.png', '11155-1.png', '11178-2.png', '11179-2.png',\n",
        "    '11122-4.png', '11178-1.png', '11185-1.png', '11112.png', '11168-2.png', '11116-4.png', '11151.png',\n",
        "    '11151-1.png', '11162-4.png', '11163-1.png', '11154.png', '11162.png', '11150.png', '11148.png',\n",
        "    '11164-2.png', '11136.png', '11178.png', '11168.png', '11184-1.png', '11187-3.png', '11123-1.png',\n",
        "    '11175-2.png', '11181.png', '11166-2.png', '11187-4.png', '11132.png', '11179-1.png', '11123-4.png',\n",
        "    '11167-2.png', '11179-4.png', '11119.png', '11143.png', '11179-5.png', '11140.png', '11118.png',\n",
        "    '11153.png', '11157.png', '11165-2.png', '11138.png', '11155-2.png', '11164-4.png', '11167.png',\n",
        "    '11134-5.png', '11134-4.png', '11114.png', '11123-6.png', '11142.png', '11167-1.png', '11131.png',\n",
        "    '11116-3.png', '11122-2.png', '11165.png', '11162-2.png', '11123-3.png', '11182-2.png', '11163.png',\n",
        "    '11124.png', '11162-1.png', '11126.png', '11133.png', '11156-1.png', '11155-3.png', '11135.png',\n",
        "    '11122-5.png', '11134-2.png', '11116.png', '11151-4.png', '11115.png', '11113.png', '11157-1.png',\n",
        "    '11144.png', '11162-3.png', '11175-1.png', '11141-3.png', '11134.png', '11155.png', '11184.png',\n",
        "    '11134-6.png', '11141.png', '11178-3.png', '11179-3.png', '11139.png', '11170.png', '11180.png',\n",
        "    '11178-4.png', '11116-1.png', '11182-1.png', '11183.png', '11149.png', '11175-3.png', '11122.png',\n",
        "    '11134-3.png', '11141-2.png', '11122-1.png', '11187-1.png', '11158.png', '11159.png', '11140-1.png',\n",
        "    '11176.png', '11175.png', '11137.png', '11155-4.png', '11121.png', '11165-1.png', '11122-3.png',\n",
        "    '11156-2.png', '11177.png', '11140-6.png', '11123-5.png', '11151-2.png', '11130.png', '11185.png',\n",
        "    '11179.png', '11170-1.png', '11141-4.png', '11166-1.png', '11187-2.png', '11120.png', '11151-3.png',\n",
        "    '11187-5.png', '11123-2.png', '7Q3A9060-4.png', 'IMG25-1.png', '11289-3.png', 'IMG25-10.png',\n",
        "    'IMG11-2.png', '7Q3A9060-1.png', '11192-1.png', '11197-4.png', '7Q3A9060-18.png', '7Q3A9064-5.png',\n",
        "    '7Q3A9060-8.png', 'IMG88.png', 'IMG25-2.png', 'IMG13-5.png', 'IMG13-1.png', '11289-10.png', '11202.png'\n",
        "]\n",
        "\n",
        "# Sorting images by numerical order first\n",
        "image_files_sorted = sorted(image_files)\n",
        "\n",
        "# Categorizing images by prefixes\n",
        "categories = defaultdict(list)\n",
        "\n",
        "for img in image_files_sorted:\n",
        "    prefix = img.split('-')[0] if '-' in img else ''.join(filter(str.isdigit, img))\n",
        "    categories[prefix].append(img)\n",
        "\n",
        "# Converting categories to a structured dict\n",
        "structured_images = dict(categories)\n",
        "structured_images\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBE01sTjGRL2",
        "outputId": "3831f77d-d055-4cf8-8b85-a0dbe9999669"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'11111': ['11111.png'],\n",
              " '11112': ['11112.png'],\n",
              " '11113': ['11113.png'],\n",
              " '11114': ['11114.png'],\n",
              " '11115': ['11115.png'],\n",
              " '11116': ['11116-1.png',\n",
              "  '11116-2.png',\n",
              "  '11116-3.png',\n",
              "  '11116-4.png',\n",
              "  '11116.png'],\n",
              " '11117': ['11117.png'],\n",
              " '11118': ['11118.png'],\n",
              " '11119': ['11119.png'],\n",
              " '11120': ['11120.png'],\n",
              " '11121': ['11121.png'],\n",
              " '11122': ['11122-1.png',\n",
              "  '11122-2.png',\n",
              "  '11122-3.png',\n",
              "  '11122-4.png',\n",
              "  '11122-5.png',\n",
              "  '11122.png'],\n",
              " '11123': ['11123-1.png',\n",
              "  '11123-2.png',\n",
              "  '11123-3.png',\n",
              "  '11123-4.png',\n",
              "  '11123-5.png',\n",
              "  '11123-6.png',\n",
              "  '11123.png'],\n",
              " '11124': ['11124.png'],\n",
              " '11126': ['11126.png'],\n",
              " '11130': ['11130.png'],\n",
              " '11131': ['11131.png'],\n",
              " '11132': ['11132.png'],\n",
              " '11133': ['11133.png'],\n",
              " '11134': ['11134-1.png',\n",
              "  '11134-2.png',\n",
              "  '11134-3.png',\n",
              "  '11134-4.png',\n",
              "  '11134-5.png',\n",
              "  '11134-6.png',\n",
              "  '11134.png'],\n",
              " '11135': ['11135.png'],\n",
              " '11136': ['11136.png'],\n",
              " '11137': ['11137.png'],\n",
              " '11138': ['11138.png'],\n",
              " '11139': ['11139.png'],\n",
              " '11140': ['11140-1.png', '11140-6.png', '11140.png'],\n",
              " '11141': ['11141-1.png',\n",
              "  '11141-2.png',\n",
              "  '11141-3.png',\n",
              "  '11141-4.png',\n",
              "  '11141.png'],\n",
              " '11142': ['11142.png'],\n",
              " '11143': ['11143.png'],\n",
              " '11144': ['11144.png'],\n",
              " '11148': ['11148.png'],\n",
              " '11149': ['11149.png'],\n",
              " '11150': ['11150.png'],\n",
              " '11151': ['11151-1.png',\n",
              "  '11151-2.png',\n",
              "  '11151-3.png',\n",
              "  '11151-4.png',\n",
              "  '11151.png'],\n",
              " '11152': ['11152.png'],\n",
              " '11153': ['11153.png'],\n",
              " '11154': ['11154.png'],\n",
              " '11155': ['11155-1.png',\n",
              "  '11155-2.png',\n",
              "  '11155-3.png',\n",
              "  '11155-4.png',\n",
              "  '11155.png'],\n",
              " '11156': ['11156-1.png', '11156-2.png', '11156.png'],\n",
              " '11157': ['11157-1.png', '11157.png'],\n",
              " '11158': ['11158.png'],\n",
              " '11159': ['11159.png'],\n",
              " '11162': ['11162-1.png',\n",
              "  '11162-2.png',\n",
              "  '11162-3.png',\n",
              "  '11162-4.png',\n",
              "  '11162.png'],\n",
              " '11163': ['11163-1.png', '11163.png'],\n",
              " '11164': ['11164-2.png', '11164-4.png'],\n",
              " '11165': ['11165-1.png', '11165-2.png', '11165.png'],\n",
              " '11166': ['11166-1.png', '11166-2.png', '11166.png'],\n",
              " '11167': ['11167-1.png', '11167-2.png', '11167.png'],\n",
              " '11168': ['11168-2.png', '11168.png'],\n",
              " '11170': ['11170-1.png', '11170-2.png', '11170.png'],\n",
              " '11175': ['11175-1.png', '11175-2.png', '11175-3.png', '11175.png'],\n",
              " '11176': ['11176.png'],\n",
              " '11177': ['11177.png'],\n",
              " '11178': ['11178-1.png',\n",
              "  '11178-2.png',\n",
              "  '11178-3.png',\n",
              "  '11178-4.png',\n",
              "  '11178.png'],\n",
              " '11179': ['11179-1.png',\n",
              "  '11179-2.png',\n",
              "  '11179-3.png',\n",
              "  '11179-4.png',\n",
              "  '11179-5.png',\n",
              "  '11179.png'],\n",
              " '11180': ['11180.png'],\n",
              " '11181': ['11181.png'],\n",
              " '11182': ['11182-1.png', '11182-2.png'],\n",
              " '11183': ['11183.png'],\n",
              " '11184': ['11184-1.png', '11184-2.png', '11184.png'],\n",
              " '11185': ['11185-1.png', '11185.png'],\n",
              " '11186': ['11186.png'],\n",
              " '11187': ['11187-1.png',\n",
              "  '11187-2.png',\n",
              "  '11187-3.png',\n",
              "  '11187-4.png',\n",
              "  '11187-5.png'],\n",
              " '11192': ['11192-1.png'],\n",
              " '11197': ['11197-4.png'],\n",
              " '11202': ['11202.png'],\n",
              " '11289': ['11289-10.png', '11289-3.png'],\n",
              " '7Q3A9060': ['7Q3A9060-1.png',\n",
              "  '7Q3A9060-18.png',\n",
              "  '7Q3A9060-4.png',\n",
              "  '7Q3A9060-8.png'],\n",
              " '7Q3A9064': ['7Q3A9064-5.png'],\n",
              " 'IMG11': ['IMG11-2.png'],\n",
              " 'IMG13': ['IMG13-1.png', 'IMG13-5.png'],\n",
              " 'IMG25': ['IMG25-1.png', 'IMG25-10.png', 'IMG25-2.png'],\n",
              " '88': ['IMG88.png']}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load dataset (assuming images are in 'data' folder with subfolders as labels)\n",
        "dataset = datasets.ImageFolder(root='data', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define a simple CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(32 * 64 * 64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Model setup\n",
        "num_classes = len(dataset.classes)\n",
        "model = CNN(num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        for images, labels in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Start training\n",
        "train_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "-j9pqzdlG60t",
        "outputId": "33f503d8-cc1a-4a57-99d5-18b45fcb0e52"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-38c7699c92c7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Load dataset (assuming images are in 'data' folder with subfolders as labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset = datasets.ImageFolder(root='DeepCrack', transform=transform)\n",
        "\n",
        "print(\"Exists:\", os.path.exists(data_path))\n",
        "print(\"Contents:\", os.listdir(data_path) if os.path.exists(data_path) else \"Folder not found!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "rpAsq9UyHJNw",
        "outputId": "c111f227-e668-43f0-954b-6f8eebddd64f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'DeepCrack'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5cbd6aa82f74>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DeepCrack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exists:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DeepCrack'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Resize images to 128x128\n",
        "    transforms.ToTensor(),          # Convert images to tensors\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = \"/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack\"  # Make sure this is the correct path\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"Dataset folder '{dataset_path}' not found!\")\n",
        "\n",
        "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Get number of classes\n",
        "num_classes = len(dataset.classes)\n",
        "print(f\"Detected {num_classes} classes: {dataset.classes}\")\n",
        "\n",
        "# Define a simple CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = CNN(num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"deepcrack_model.pth\")\n",
        "print(\"Model saved as deepcrack_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFqni_R3H0wz",
        "outputId": "16bd2323-7810-45ce-b1f7-ad7e303274b5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Detected 4 classes: ['test_img', 'test_lab', 'train_img', 'train_lab']\n",
            "Epoch 1/5, Loss: 0.838494186892229\n",
            "Epoch 2/5, Loss: 0.6222956373411066\n",
            "Epoch 3/5, Loss: 0.5483446463065988\n",
            "Epoch 4/5, Loss: 0.46201859765193043\n",
            "Epoch 5/5, Loss: 0.344190338955206\n",
            "Training complete!\n",
            "Model saved as deepcrack_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model architecture (same as used during training)\n",
        "class DeepCrackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCrackModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = DeepCrackModel()\n",
        "\n",
        "# Load the state dictionary\n",
        "model.load_state_dict(torch.load(\"/content/deepcrack_model.pth\", map_location=torch.device('cpu')))\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define the image transformation (should match training transforms)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Path to test images\n",
        "test_img_folder = \"DeepCrack/test_img\"\n",
        "output_folder = \"DeepCrack/output\"  # Folder to save the predicted outputs\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Process each test image\n",
        "for img_name in os.listdir(test_img_folder):\n",
        "    img_path = os.path.join(test_img_folder, img_name)\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Generate output\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "\n",
        "    # Convert output tensor to image format (assuming it's grayscale)\n",
        "    output_image = transforms.ToPILImage()(output.squeeze(0))\n",
        "\n",
        "    # Save output image\n",
        "    output_path = os.path.join(output_folder, img_name)\n",
        "    output_image.save(output_path)\n",
        "\n",
        "    print(f\"Processed: {img_name} -> Saved: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "2bQv4ijIJ59O",
        "outputId": "0b2fd57c-c667-4c79-eeb1-ade4372091bd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for DeepCrackModel:\n\tUnexpected key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([32, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 1, 3, 3]).\n\tsize mismatch for conv1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 64, 3, 3]).\n\tsize mismatch for conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([1]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-7a03cd6deeaa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load the state dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/deepcrack_model.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Set model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeepCrackModel:\n\tUnexpected key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([32, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 1, 3, 3]).\n\tsize mismatch for conv1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for conv2.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([1, 64, 3, 3]).\n\tsize mismatch for conv2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([1])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "checkpoint = torch.load(\"/content/deepcrack_model.pth\", map_location=torch.device('cpu'))\n",
        "print(checkpoint.keys())  # Check what keys are stored in the file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGy9PRjNKTV7",
        "outputId": "d7bf5f57-9ce6-45f4-dd3a-f721b9ac1d28"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DeepCrackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCrackModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * IMAGE_SIZE * IMAGE_SIZE, 128)  # Adjust IMAGE_SIZE\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set the correct IMAGE_SIZE (Replace with your actual size)\n",
        "IMAGE_SIZE = 256  # Change based on your dataset\n",
        "\n",
        "# Initialize model\n",
        "model = DeepCrackModel()\n",
        "\n",
        "# Load trained weights\n",
        "model.load_state_dict(torch.load(\"deepcrack_model.pth\", map_location=torch.device('cpu')))\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "Ls3iQY0RKf3N",
        "outputId": "66f6d182-7c7d-4205-d34a-c9a346b1cefb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for DeepCrackModel:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 65536]) from checkpoint, the shape in current model is torch.Size([128, 4194304]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([4, 128]) from checkpoint, the shape in current model is torch.Size([1, 128]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-96e8e5d85626>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Load trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"deepcrack_model.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Set model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeepCrackModel:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 65536]) from checkpoint, the shape in current model is torch.Size([128, 4194304]).\n\tsize mismatch for fc2.weight: copying a param with shape torch.Size([4, 128]) from checkpoint, the shape in current model is torch.Size([1, 128]).\n\tsize mismatch for fc2.bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([1])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set correct image size (replace with your actual training size)\n",
        "IMAGE_SIZE = 256  # Update this if needed\n",
        "\n",
        "class DeepCrackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCrackModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Fix the fc1 input size (calculate correctly)\n",
        "        fc1_input_size = (IMAGE_SIZE // 4) * (IMAGE_SIZE // 4) * 64  # Adjusted for 2 conv layers\n",
        "        self.fc1 = nn.Linear(fc1_input_size, 128)\n",
        "\n",
        "        # Fix fc2 output size to 4 (for 4 classes)\n",
        "        self.fc2 = nn.Linear(128, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = DeepCrackModel()\n",
        "\n",
        "# Load trained weights\n",
        "model.load_state_dict(torch.load(\"deepcrack_model.pth\", map_location=torch.device('cpu')))\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "9ef3XOCHKxfk",
        "outputId": "ca8bfcb2-fe13-4887-b4c8-caca7e8550c5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for DeepCrackModel:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 65536]) from checkpoint, the shape in current model is torch.Size([128, 262144]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-a32229638f26>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Load trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"deepcrack_model.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Set model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeepCrackModel:\n\tsize mismatch for fc1.weight: copying a param with shape torch.Size([128, 65536]) from checkpoint, the shape in current model is torch.Size([128, 262144])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Load a sample training image\n",
        "image_path = \"/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_img/7Q3A9060-1.jpg\"\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Print image size\n",
        "print(\"Image size used in training:\", image.size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfIU3-J-K801",
        "outputId": "b5b8372e-978e-4b5a-a3aa-501deb773943"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size used in training: (544, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DeepCrackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCrackModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calculate flattened size dynamically\n",
        "        self._to_linear = None\n",
        "        self._get_flattened_size()\n",
        "\n",
        "        self.fc1 = nn.Linear(self._to_linear, 128)  # Adjusted input size\n",
        "        self.fc2 = nn.Linear(128, 1)  # Assuming binary classification\n",
        "\n",
        "    def _get_flattened_size(self):\n",
        "        # Pass a dummy tensor through conv layers to determine output size\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros(1, 3, 544, 384)  # Using your actual image size\n",
        "            x = self.pool(F.relu(self.conv1(x)))\n",
        "            x = self.pool(F.relu(self.conv2(x)))\n",
        "            self._to_linear = x.numel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # No activation for raw output\n",
        "        return x\n",
        "\n",
        "# Initialize the model and print summary\n",
        "model = DeepCrackModel()\n",
        "print(\"Flattened feature size:\", model._to_linear)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWQGoS-aLeEz",
        "outputId": "0df93844-20c9-4cf1-c923-a9603fb963a0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened feature size: 835584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the DeepCrack Model\n",
        "class DeepCrackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCrackModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Compute the flattened feature size after convolution layers\n",
        "        self._to_linear = 835584  # Based on image size (544x384)\n",
        "\n",
        "        self.fc1 = nn.Linear(self._to_linear, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)  # Output single-channel prediction\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.shape[0], -1)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid for binary output\n",
        "        return x\n",
        "\n",
        "# Training setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((544, 384)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir) if img.endswith('.jpg') or img.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "dataset = ImageDataset(root_dir='/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/train_img', transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Initialize model\n",
        "model = DeepCrackModel().to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for images in dataloader:\n",
        "        images = images.to(device)\n",
        "        images, labels = images.to(device), labels.float().to(device).unsqueeze(1)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"deepcrack_model.pth\")\n",
        "print(\"Training complete! Model saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "e1QIvm53Ls5x",
        "outputId": "94f3cc23-c859-47ff-d31e-31257b3a1363"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (8x1671168 and 835584x128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-3a007ed2d888>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-3a007ed2d888>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sigmoid for binary output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x1671168 and 835584x128)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = torch.flatten(x, start_dim=1)  # Flatten before FC layer\n",
        "\n",
        "    print(\"Flattened shape:\", x.shape)  # Debugging line\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "T5YqRpRJNHlJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define DeepCrack model\n",
        "class DeepCrackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCrackModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(835584, 128)  # Adjusted input size\n",
        "        self.fc2 = nn.Linear(128, 1)  # Binary output (crack or no crack)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten before FC layer\n",
        "        print(\"Flattened shape:\", x.shape)  # Debugging line\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = DeepCrackModel().to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWvUz7r1NsH8",
        "outputId": "eb1641a8-f41d-4163-b21f-19c41fe682c2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((544, 384)),  # Ensure correct input size\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Set dataset path\n",
        "data_path = \"/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack\"\n",
        "\n",
        "# Load training dataset\n",
        "train_dataset = datasets.ImageFolder(root=os.path.join(data_path, \"train_img\"), transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "print(f\"Training data loaded: {len(train_dataset)} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "KIX5Q2CBNxjK",
        "outputId": "63bac740-36fd-45f5-a361-0457c98c21df"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_img'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-801bceb5bdc9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_img\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/INTUTE AI/MARBLE CRACK PROJECT/DeepCrack/train_img'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define DeepCrack model\n",
        "class DeepCrackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCrackModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(835584, 128)  # Adjusted input size\n",
        "        self.fc2 = nn.Linear(128, 1)  # Binary output (crack or no crack)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten before FC layer\n",
        "        print(\"Flattened shape:\", x.shape)  # Debugging line\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = DeepCrackModel().to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-deXGZ0OE4W",
        "outputId": "fe46bbd2-fd67-4eb8-b4f3-256635406e8a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((544, 384)),  # Ensure correct input size\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Custom dataset loader for train images\n",
        "class CrackDataset(Dataset):\n",
        "    def __init__(self, img_folder, transform=None):\n",
        "        self.img_folder = img_folder\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(img_folder) if f.endswith((\".jpg\", \".png\"))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_folder, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert(\"L\")  # Convert to grayscale\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image  # No label\n",
        "\n",
        "# Set dataset path\n",
        "data_path = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/train_img\"\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = CrackDataset(img_folder=data_path, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "print(f\"Training data loaded: {len(train_dataset)} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KtIo0R2OIyD",
        "outputId": "7a929380-4fec-4a9d-e690-62f0777e4d5f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data loaded: 300 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepCrackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCrackModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.flatten_size = 52224  # Update this based on printed shape\n",
        "\n",
        "        self.fc1 = nn.Linear(self.flatten_size, 128)  # Adjusted size\n",
        "        self.fc2 = nn.Linear(128, 1)  # Assuming binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten before FC layer\n",
        "        print(\"Flattened shape:\", x.shape)  # Debugging\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "s5lw70ihOtfV"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} started...\")  # Debugging\n",
        "    total_loss = 0\n",
        "\n",
        "    for images in train_loader:  # No labels, so we just process images\n",
        "        images = images.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)  # Forward pass\n",
        "        labels = torch.ones(images.shape[0], 1).to(device)  # Fake labels (adjust as needed)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader)}\")  # Track training\n",
        "\n",
        "# Save trained model\n",
        "torch.save(model.state_dict(), \"deepcrack_model.pth\")\n",
        "print(\"Training complete! Model saved as deepcrack_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "RmdBLVAeOg0k",
        "outputId": "802bb46c-bb24-4402-e0ab-a4c0fef3820d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 started...\n",
            "Flattened shape: torch.Size([8, 52224])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (8x52224 and 835584x128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-87e451bc29bf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Fake labels (adjust as needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-5634a4abe8f4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Flattened shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Debugging line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x52224 and 835584x128)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "self.fc1 = nn.Linear(52224, 128)  # Adjust input size to match the flattened shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "3IbfMxqyO--j",
        "outputId": "ab6066e1-6d14-4cf4-abf3-2acdc89c52ec"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'self' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-bcc7d20b8fce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m52224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust input size to match the flattened shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepCrackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCrackModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Update fc1 input size to match the flattened shape\n",
        "        self.fc1 = nn.Linear(52224, 128)  # Adjusted input size\n",
        "        self.fc2 = nn.Linear(128, 1)  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten before FC layer\n",
        "        print(\"Flattened shape:\", x.shape)  # Debugging line\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "5ZJ-EZ5RPHx2"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Define the model\n",
        "class DeepCrackModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCrackModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * 136 * 96, 128)  # Adjusted input size\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten before FC layer\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((544, 384)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/train_img\"\n",
        "\n",
        "# Custom dataset class\n",
        "class CrackDataset(Dataset):\n",
        "    def __init__(self, img_folder, transform=None):\n",
        "        self.img_folder = img_folder\n",
        "        self.transform = transform\n",
        "        self.image_files = [os.path.join(img_folder, f) for f in os.listdir(img_folder) if f.endswith(('.jpg', '.png'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = CrackDataset(img_folder=data_path, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepCrackModel().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} started...\")\n",
        "    for images in train_loader:\n",
        "        images = images.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)  # Forward pass\n",
        "        labels = torch.ones(images.shape[0], 1).to(device)  # Fake labels (adjust as needed)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"deepcrack_model.pth\")\n",
        "print(\"Training complete! Model saved.\")\n",
        "\n",
        "# Load the model for inference\n",
        "model.load_state_dict(torch.load(\"deepcrack_model.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Inference function\n",
        "def predict(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "    output = model(image)\n",
        "    return output.item()\n",
        "\n",
        "# Example prediction\n",
        "test_image_path = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/test_img/sample.jpg\"\n",
        "print(\"Prediction:\", predict(test_image_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "Bt50aLnDPc9p",
        "outputId": "f5c093e3-a0a3-455d-ade7-472815f85b4d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 started...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (8x6684672 and 1671168x128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-c05b57e1fccd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Fake labels (adjust as needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-c05b57e1fccd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten before FC layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x6684672 and 1671168x128)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = torch.flatten(x, start_dim=1)  # Flatten before FC layer\n",
        "    print(\"Flattened shape:\", x.shape)  # Debugging line\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "h-KGCV1rPtOr"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "    print(\"Input shape before conv layers:\", x.shape)  # Debugging line\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    print(\"Shape before flattening:\", x.shape)  # Debugging line\n",
        "\n",
        "    x = torch.flatten(x, start_dim=1)  # Flatten before FC layer\n",
        "\n",
        "    print(\"Flattened shape:\", x.shape)  # Debugging line\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "HtZGNOsLQDX4"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, _ in train_loader:  # Get a batch of images\n",
        "    images = images.to(device)\n",
        "    outputs = model(images)\n",
        "    break  # Run only for one batch to check\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "HqfvSIVWQGGE",
        "outputId": "ce35b190-5f3a-42dd-b432-d150a903406f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-ae10bca80af0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Get a batch of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Run only for one batch to check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    print(len(batch))  # This will tell us how many values are being returned\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRW5VjlLQRM_",
        "outputId": "c105db8c-9ef8-4093-85ab-56a0f333c3d6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    for i, item in enumerate(batch):\n",
        "        print(f\"Item {i}: Type={type(item)}, Shape={item.shape if isinstance(item, torch.Tensor) else 'N/A'}\")\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_sTtynHQaPu",
        "outputId": "030980aa-137d-4122-e9f0-6a4c295b49e5"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Item 0: Type=<class 'torch.Tensor'>, Shape=torch.Size([3, 544, 384])\n",
            "Item 1: Type=<class 'torch.Tensor'>, Shape=torch.Size([3, 544, 384])\n",
            "Item 2: Type=<class 'torch.Tensor'>, Shape=torch.Size([3, 544, 384])\n",
            "Item 3: Type=<class 'torch.Tensor'>, Shape=torch.Size([3, 544, 384])\n",
            "Item 4: Type=<class 'torch.Tensor'>, Shape=torch.Size([3, 544, 384])\n",
            "Item 5: Type=<class 'torch.Tensor'>, Shape=torch.Size([3, 544, 384])\n",
            "Item 6: Type=<class 'torch.Tensor'>, Shape=torch.Size([3, 544, 384])\n",
            "Item 7: Type=<class 'torch.Tensor'>, Shape=torch.Size([3, 544, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CrackDataset(Dataset):\n",
        "    def __init__(self, img_folder, transform=None):\n",
        "        self.img_folder = img_folder\n",
        "        self.transform = transform\n",
        "        self.image_files = [os.path.join(img_folder, f) for f in os.listdir(img_folder) if f.endswith((\".jpg\", \".png\"))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # Open image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image  # Only returning the image tensor\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = CrackDataset(img_folder='/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/train_img', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Ensure batch size is set\n"
      ],
      "metadata": {
        "id": "u6A6zv9oQkqW"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    print(f\"Batch shape: {batch.shape}\")  # Should be [8, 3, 544, 384]\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWG9if8SQneA",
        "outputId": "0bef1cc5-a92b-475d-b02f-2862d32c0ad7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch shape: torch.Size([8, 3, 544, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader)).to(device)  # Get one batch and move to device\n",
        "outputs = model(batch)  # Forward pass\n",
        "print(\"Model output shape:\", outputs.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "RunB6cO5Qs2i",
        "outputId": "40a2b489-7ba7-4ca9-e7d2-5a726e2414d1"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (8x6684672 and 1671168x128)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-d49eb64ed98a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get one batch and move to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model output shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-c05b57e1fccd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten before FC layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (8x6684672 and 1671168x128)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = torch.flatten(x, start_dim=1)  # Flatten before FC layer\n",
        "\n",
        "    print(\"Flattened shape:\", x.shape)  # Debugging line\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "_GmLIIaMQ69Z"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Flattened shape: torch.Size([8, XXXXXXX])\n"
      ],
      "metadata": {
        "id": "Coz6ds7WQ8v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "    print(\"Input shape to model:\", x.shape)  # Step 1: Check input shape\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = torch.flatten(x, start_dim=1)  # Flatten before FC layer\n",
        "\n",
        "    print(\"Flattened shape:\", x.shape)  # Step 2: Check flattened shape\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "7VfB4MZjkFcu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if dataset loads correctly\n",
        "for idx, item in enumerate(train_loader):\n",
        "    print(f\"Batch {idx}: Type={type(item)}, Length={len(item)}\")\n",
        "    for i, data in enumerate(item):\n",
        "        print(f\"  Item {i}: Type={type(data)}, Shape={data.shape if isinstance(data, torch.Tensor) else 'N/A'}\")\n",
        "    if idx == 0:\n",
        "        break  # Only check the first batch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "_4-32XQNk-24",
        "outputId": "fa6a0b3b-87bc-4d80-c47b-3bc0ab2d8043"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-07839c446c82>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check if dataset loads correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {idx}: Type={type(item)}, Length={len(item)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Item {i}: Type={type(data)}, Shape={data.shape if isinstance(data, torch.Tensor) else 'N/A'}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define transformation (resize to match model input)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((544, 384)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Path to dataset\n",
        "data_path = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/train_img\"\n",
        "\n",
        "# Load dataset\n",
        "train_dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "print(\"DataLoader initialized successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "GiJtaB_VlKQT",
        "outputId": "bcd61d07-5e70-42b6-ca49-e5138b414edc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Couldn't find any class folder in /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/train_img.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-283ed6920f2e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/train_img."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/train_img\"\n",
        "print(\"Path Exists:\", os.path.exists(data_path))\n",
        "print(\"Files in Path:\", os.listdir(data_path) if os.path.exists(data_path) else \"Folder not found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20jnyTaQlrXk",
        "outputId": "3dd92e52-43a3-415e-bd60-c365eeebbc60"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path Exists: True\n",
            "Files in Path: ['IMG27-11.jpg', 'IMG33-12.jpg', 'IMG27-10.jpg', 'IMG88.jpg', 'IMG_6536-6.jpg', 'IMG33-4.jpg', 'IMG27-3.jpg', 'IMG33-6.jpg', 'IMG25-9.jpg', 'IMG33-13.jpg', 'IMG27-1.jpg', 'IMG33-3.jpg', 'IMG27-7.jpg', 'IMG27-2.jpg', 'IMG36-1.jpg', 'IMG39-1.jpg', 'IMG27-12.jpg', 'IMG33-9.jpg', 'IMG33-18.jpg', 'IMG33-7.jpg', 'IMG74.jpg', 'IMG49.jpg', 'IMG27-8.jpg', 'IMG33-2.jpg', 'IMG33-15.jpg', 'IMG27-9.jpg', 'IMG33-17.jpg', 'IMG25-7.jpg', 'IMG33-11.jpg', 'IMG33-1.jpg', 'IMG36-2.jpg', 'IMG56.jpg', 'IMG27-5.jpg', 'IMG39-2.jpg', 'IMG27-6.jpg', 'IMG33-5.jpg', 'IMG25-8.jpg', 'IMG33-16.jpg', 'IMG33-14.jpg', 'IMG33-8.jpg', 'IMG36-3.jpg', 'IMG5.jpg', 'IMG27-4.jpg', 'IMG27-13.jpg', 'IMG33-10.jpg', 'IMG11-2.jpg', 'IMG25-5.jpg', '7Q3A9060-19.jpg', 'IMG25-10.jpg', '7Q3A9064-4.jpg', 'IMG12-2.jpg', 'IMG25-4.jpg', 'IMG13-4.jpg', 'IMG13-2.jpg', '7Q3A9064-16.jpg', '7Q3A9064-8.jpg', '7Q3A9064-11.jpg', 'IMG25-2.jpg', 'IMG20.jpg', '7Q3A9064-18.jpg', 'IMG11-4.jpg', 'IMG13-6.jpg', 'IMG25-1.jpg', 'IMG25-3.jpg', '7Q3A9064-13.jpg', '7Q3A9060-8.jpg', 'IMG14-1.jpg', 'IMG12.jpg', 'IMG16.jpg', '7Q3A9060-6.jpg', '7Q3A9064-6.jpg', 'IMG11-3.jpg', '7Q3A9064-2.jpg', 'IMG11-1.jpg', '7Q3A9060-2.jpg', '7Q3A9064-14.jpg', '7Q3A9064-9.jpg', 'IMG12-1.jpg', 'IMG13-5.jpg', '7Q3A9060-7.jpg', 'IMG14-2.jpg', '7Q3A9060-9.jpg', '7Q3A9060-20.jpg', 'IMG18.jpg', '7Q3A9064-3.jpg', 'IMG21.jpg', 'IMG13-7.jpg', 'IMG13-3.jpg', '7Q3A9064-1.jpg', '7Q3A9060-3.jpg', '7Q3A9064-5.jpg', 'IMG14-4.jpg', '7Q3A9064-20.jpg', '7Q3A9064-10.jpg', '7Q3A9064-15.jpg', '7Q3A9064-7.jpg', 'IMG19.jpg', 'IMG11-6.jpg', '7Q3A9060-4.jpg', 'IMG13-1.jpg', '7Q3A9064-17.jpg', 'IMG11-5.jpg', '7Q3A9060-5.jpg', 'IMG17.jpg', '7Q3A9064-12.jpg', 'IMG25-6.jpg', 'IMG136.jpg', 'IMG14-3.jpg', 'IMG108.jpg', '7Q3A9064-19.jpg', '11289-8.jpg', '11192-2.jpg', '7Q3A9060-14.jpg', '11289-7.jpg', '11289-5.jpg', '11289-2.jpg', '11200-2.jpg', '7Q3A9060-10.jpg', '11187.jpg', '11190-7.jpg', '7Q3A9060-11.jpg', '11204.jpg', '11190-3.jpg', '11193.jpg', '7Q3A9060-12.jpg', '11190-1.jpg', '11197-2.jpg', '11188-2.jpg', '11188-1.jpg', '11190-5.jpg', '11188.jpg', '11189-2.jpg', '7Q3A9060-15.jpg', '11289-3.jpg', '11202.jpg', '11193-3.jpg', '7Q3A9060-13.jpg', '11193-2.jpg', '11193-1.jpg', '11197-4.jpg', '11190-2.jpg', '11196.jpg', '11190-4.jpg', '7Q3A9060-18.jpg', '11192.jpg', '11189.jpg', '11190.jpg', '11289-4.jpg', '7Q3A9060-16.jpg', '11190-6.jpg', '11203.jpg', '11189-1.jpg', '7Q3A9060-1.jpg', '11197-1.jpg', '7Q3A9060-17.jpg', '11289-10.jpg', '11289-6.jpg', '11200-1.jpg', '11289-1.jpg', '11198.jpg', '11197-3.jpg', '11192-1.jpg', '11289-9.jpg', '11162-4.jpg', '11167-1.jpg', '11166.jpg', '11187-4.jpg', '11179-2.jpg', '11178-1.jpg', '11165.jpg', '11179-3.jpg', '11178-2.jpg', '11175-3.jpg', '11178.jpg', '11166-2.jpg', '11179-1.jpg', '11162.jpg', '11187-1.jpg', '11182-1.jpg', '11187-2.jpg', '11163.jpg', '11187-6.jpg', '11166-1.jpg', '11179-4.jpg', '11177.jpg', '11168.jpg', '11178-3.jpg', '11163-1.jpg', '11167-2.jpg', '11185.jpg', '11181.jpg', '11179.jpg', '11165-1.jpg', '11165-2.jpg', '11175.jpg', '11183.jpg', '11180.jpg', '11187-3.jpg', '11164-2.jpg', '11176.jpg', '11167.jpg', '11182-2.jpg', '11184.jpg', '11170-1.jpg', '11184-1.jpg', '11168-2.jpg', '11175-1.jpg', '11187-5.jpg', '11164-4.jpg', '11179-5.jpg', '11178-4.jpg', '11170.jpg', '11184-2.jpg', '11175-2.jpg', '11185-1.jpg', '11186.jpg', '11170-2.jpg', '11148.jpg', '11151.jpg', '11153.jpg', '11140-6.jpg', '11138.jpg', '11156.jpg', '11136.jpg', '11155-4.jpg', '11140.jpg', '11151-4.jpg', '11159.jpg', '11151-1.jpg', '11151-2.jpg', '11155-3.jpg', '11141-3.jpg', '11134-2.jpg', '11154.jpg', '11141.jpg', '11155-1.jpg', '11134-1.jpg', '11162-1.jpg', '11162-2.jpg', '11134-3.jpg', '11155-2.jpg', '11135.jpg', '11142.jpg', '11150.jpg', '11140-1.jpg', '11149.jpg', '11143.jpg', '11152.jpg', '11137.jpg', '11134-6.jpg', '11157.jpg', '11155.jpg', '11156-2.jpg', '11144.jpg', '11134-5.jpg', '11151-3.jpg', '11141-1.jpg', '11158.jpg', '11162-3.jpg', '11139.jpg', '11134.jpg', '11157-1.jpg', '11141-4.jpg', '11133.jpg', '11134-4.jpg', '11156-1.jpg', '11141-2.jpg', '11122.jpg', '11120.jpg', '11112.jpg', '11124.jpg', '11123-1.jpg', '11126.jpg', '11116-2.jpg', '11121.jpg', '11116-1.jpg', '11116.jpg', '11123-3.jpg', '11122-1.jpg', '11123-4.jpg', '11115.jpg', '11118.jpg', '11114.jpg', '11123-6.jpg', '11117.jpg', '11122-5.jpg', '11123.jpg', '11123-5.jpg', '11130.jpg', '11119.jpg', '11122-3.jpg', '11131.jpg', '11116-4.jpg', '11122-4.jpg', '11132.jpg', '11122-2.jpg', '11116-3.jpg', '11123-2.jpg', '11113.jpg', '11111.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Custom Dataset Class (No Labels Needed)\n",
        "class CrackDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = sorted(os.listdir(root_dir))  # Get all images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # Convert to RGB\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image  # Returns only the image\n",
        "\n",
        "# Define Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((544, 384)),  # Resize to match model input\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load Dataset\n",
        "data_path = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/train_img\"\n",
        "train_dataset = CrackDataset(root_dir=data_path, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Check if DataLoader Works\n",
        "for batch in train_loader:\n",
        "    print(\" Batch shape:\", batch.shape)  # Should be [8, 3, 544, 384]\n",
        "    break  # Stop after first batch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvUuEHP4mUI5",
        "outputId": "6bb59864-24ca-41c3-bc7c-e12260a1a42e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Batch shape: torch.Size([8, 3, 544, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)  # Move model to GPU if available\n",
        "\n",
        "# Get one batch\n",
        "batch = next(iter(train_loader)).to(device)\n",
        "\n",
        "# Forward pass through the model\n",
        "outputs = model(batch)\n",
        "print(\" Model output shape:\", outputs.shape)  # Verify model output dimensions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "sEkkoDztnUOl",
        "outputId": "3eb5ebcd-7621-41f7-c246-2d71d5c8a97d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b07098b2f016>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move model to GPU if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get one batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn  # Importing nn module for building models\n",
        "import torch.nn.functional as F  # Provides additional activation functions\n"
      ],
      "metadata": {
        "id": "rLBs5V_4nlV6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model class (if not already defined)\n",
        "class CrackDetectionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CrackDetectionModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(16 * 272 * 192, 128)  # Adjust based on input image size\n",
        "        self.fc2 = nn.Linear(128, 1)  # Output layer (binary classification)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten for FC layer\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize Model\n",
        "model = CrackDetectionModel()\n",
        "\n",
        "# Move model to GPU/CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Check Model Summary\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvIcn__WnaqE",
        "outputId": "539f2ea7-ceb5-47eb-bec1-942d3707339a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrackDetectionModel(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu): ReLU()\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=835584, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CrackDetectionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CrackDetectionModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Dummy input to calculate fc1 input size\n",
        "        self._to_linear = None  # Placeholder\n",
        "        self._get_fc_input_size()\n",
        "\n",
        "        self.fc1 = nn.Linear(self._to_linear, 128)  # Dynamic feature size\n",
        "        self.fc2 = nn.Linear(128, 1)  # Binary classification\n",
        "\n",
        "    def _get_fc_input_size(self):\n",
        "        \"\"\" Pass a dummy tensor through conv layers to get the flattened size. \"\"\"\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn(1, 3, 544, 384)  # Batch size 1, input image shape\n",
        "            x = self.pool(self.relu(self.conv1(x)))  # Pass through conv layers\n",
        "            self._to_linear = x.view(1, -1).shape[1]  # Flatten and get feature size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))  # Conv -> ReLU -> Pool\n",
        "        x = x.view(x.size(0), -1)  # Flatten for fully connected layer\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = CrackDetectionModel()\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ-L0nSJnvUv",
        "outputId": "4dbcfed9-6ed7-47b2-93a5-d224f8992989"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrackDetectionModel(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu): ReLU()\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=835584, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CrackDetectionModel(nn.Module):\n",
        "    def __init__(self, input_size=(3, 544, 384)):  # Default input image size\n",
        "        super(CrackDetectionModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Compute fc1 input features dynamically\n",
        "        self._to_linear = self._get_fc_input_size(input_size)\n",
        "        self.fc1 = nn.Linear(self._to_linear, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def _get_fc_input_size(self, input_size):\n",
        "        \"\"\" Pass a dummy tensor through conv layers to get the flattened size. \"\"\"\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn(1, *input_size)  # Create a fake input tensor\n",
        "            x = self.pool(self.relu(self.conv1(x)))  # Pass through conv layers\n",
        "            return x.view(1, -1).shape[1]  # Get flattened size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))  # Conv -> ReLU -> Pool\n",
        "        x = x.view(x.size(0), -1)  # Flatten for fc1\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and print model\n",
        "model = CrackDetectionModel()\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUpbHSBwn4CN",
        "outputId": "24de6488-db38-44a0-d209-7164345dfde1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrackDetectionModel(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu): ReLU()\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=835584, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CrackDetectionModel(nn.Module):\n",
        "    def __init__(self, input_size=(3, 544, 384)):  # Default input size\n",
        "        super(CrackDetectionModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Compute correct input size for fc1 dynamically\n",
        "        self._to_linear = self._get_fc_input_size(input_size)\n",
        "        self.fc1 = nn.Linear(self._to_linear, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def _get_fc_input_size(self, input_size):\n",
        "        \"\"\" Pass a dummy tensor through conv layers to get the correct input size for fc1. \"\"\"\n",
        "        with torch.no_grad():\n",
        "            x = torch.randn(1, *input_size)  # Fake input tensor\n",
        "            x = self.pool(self.relu(self.conv1(x)))  # Pass through Conv + Pool layers\n",
        "            return x.view(1, -1).shape[1]  # Flatten and return correct size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))  # Conv -> ReLU -> Pool\n",
        "        x = x.view(x.size(0), -1)  # Flatten before fc1\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize and print model\n",
        "model = CrackDetectionModel()\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvjwzPixn-mQ",
        "outputId": "be61e77f-5667-42ad-a7d5-46f3750dca05"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrackDetectionModel(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu): ReLU()\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=835584, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_fc_input_size(self, input_size):\n",
        "    \"\"\" Pass a dummy tensor through conv layers to get the correct input size for fc1. \"\"\"\n",
        "    with torch.no_grad():\n",
        "        x = torch.randn(1, *input_size)  # Fake input tensor\n",
        "        print(f\"Input shape before conv: {x.shape}\")  # Debugging step\n",
        "\n",
        "        x = self.pool(self.relu(self.conv1(x)))  # Pass through Conv + Pool layers\n",
        "        print(f\"Shape after conv and pooling: {x.shape}\")  # Debugging step\n",
        "\n",
        "        return x.view(1, -1).shape[1]  # Flatten and return correct size\n"
      ],
      "metadata": {
        "id": "0Xtt3c5yoEy3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_fc_input_size(self, input_size):\n",
        "    x = torch.randn(1, *input_size)\n",
        "    print(f\"Input shape before conv: {x.shape}\")\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    print(f\"Shape after conv1: {x.shape}\")\n",
        "\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "    print(f\"Shape after pool: {x.shape}\")\n",
        "\n",
        "    return x.view(1, -1).shape[1]\n"
      ],
      "metadata": {
        "id": "9ZMftZdDoRln"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CrackDetectionModel((3, 544, 384))  # Check this!\n"
      ],
      "metadata": {
        "id": "lV5UUwrEoUwj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "print(\"Before calling _get_fc_input_size()\")\n",
        "time.sleep(1)\n",
        "size = model._get_fc_input_size((3, 544, 384))\n",
        "print(f\"Computed fc input size: {size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDr1rcivoX6B",
        "outputId": "e0b5cd29-e4ae-4559-b8a7-e710a1a9262f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before calling _get_fc_input_size()\n",
            "Computed fc input size: 835584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 3, 544, 384)  # Simulating one image\n",
        "output = model(x)\n",
        "print(f\"Model output: {output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBlk8gklogBn",
        "outputId": "03a5f27b-1154-49c3-cc0b-a3b9cae8f334"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: tensor([[-0.0179]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10  # or any number of epochs you want\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} started\")  # Debug print\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        print(f\"Processing Batch {batch_idx}\")  # Debug print\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "djQVK56uomlU",
        "outputId": "cbbb1b68-c1e4-4581-f8a5-2d94e42b02e0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 started\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-cf43d523387d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs} started\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Debug print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing Batch {batch_idx}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Debug print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    print(type(batch), len(batch))  # Check the type and length\n",
        "    break  # Only print for one batch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lb910Zio5QM",
        "outputId": "cea9ab00-0bd4-4aa5-f9de-853060454ffa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'> 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if your DataLoader returns labels\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "if isinstance(batch, tuple) and len(batch) == 2:\n",
        "    has_labels = True\n",
        "else:\n",
        "    has_labels = False\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # Define number of epochs\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} started\")  # Debug print\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        print(f\"Processing Batch {batch_idx}\")  # Debug print\n",
        "\n",
        "        # Handle both cases (with and without labels)\n",
        "        if has_labels:\n",
        "            images, labels = batch\n",
        "            labels = labels.to(device)\n",
        "        else:\n",
        "            images = batch  # No labels in dataset\n",
        "\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Print output for debugging\n",
        "        print(f\"Batch {batch_idx} Output: {outputs}\")\n",
        "\n",
        "        # If labels exist, compute loss\n",
        "        if has_labels:\n",
        "            loss = criterion(outputs, labels.float())  # Ensure labels are float for regression/classification loss\n",
        "            print(f\"Batch {batch_idx} Loss: {loss.item()}\")\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74dDHIXcpJny",
        "outputId": "9e71d563-1c8f-4109-f8c5-79d290dae945"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 started\n",
            "Processing Batch 0\n",
            "Batch 0 Output: tensor([[-0.0392],\n",
            "        [-0.0361],\n",
            "        [-0.0203],\n",
            "        [-0.0585],\n",
            "        [-0.0268],\n",
            "        [-0.0300],\n",
            "        [-0.0226],\n",
            "        [-0.0692]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 1\n",
            "Batch 1 Output: tensor([[-0.0159],\n",
            "        [-0.0336],\n",
            "        [-0.0126],\n",
            "        [-0.0246],\n",
            "        [-0.0178],\n",
            "        [-0.0321],\n",
            "        [-0.0264],\n",
            "        [-0.0364]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 2\n",
            "Batch 2 Output: tensor([[-0.0288],\n",
            "        [-0.0412],\n",
            "        [-0.0285],\n",
            "        [-0.0144],\n",
            "        [-0.0310],\n",
            "        [-0.0303],\n",
            "        [-0.0384],\n",
            "        [-0.0345]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 3\n",
            "Batch 3 Output: tensor([[-0.0268],\n",
            "        [-0.0255],\n",
            "        [-0.0227],\n",
            "        [-0.0198],\n",
            "        [-0.0310],\n",
            "        [-0.0541],\n",
            "        [-0.0272],\n",
            "        [-0.0284]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 4\n",
            "Batch 4 Output: tensor([[-0.0280],\n",
            "        [-0.0360],\n",
            "        [-0.0333],\n",
            "        [-0.0244],\n",
            "        [-0.0227],\n",
            "        [-0.0266],\n",
            "        [-0.0418],\n",
            "        [-0.0256]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 5\n",
            "Batch 5 Output: tensor([[-0.0250],\n",
            "        [-0.0389],\n",
            "        [-0.0273],\n",
            "        [-0.0118],\n",
            "        [-0.0175],\n",
            "        [-0.0461],\n",
            "        [-0.0453],\n",
            "        [-0.0245]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 6\n",
            "Batch 6 Output: tensor([[-0.0327],\n",
            "        [-0.0248],\n",
            "        [-0.0215],\n",
            "        [-0.0271],\n",
            "        [-0.0435],\n",
            "        [-0.0102],\n",
            "        [-0.0199],\n",
            "        [-0.0075]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 7\n",
            "Batch 7 Output: tensor([[-0.0119],\n",
            "        [-0.0328],\n",
            "        [-0.0207],\n",
            "        [-0.0205],\n",
            "        [-0.0148],\n",
            "        [-0.0224],\n",
            "        [-0.0259],\n",
            "        [-0.0374]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 8\n",
            "Batch 8 Output: tensor([[-0.0224],\n",
            "        [-0.0266],\n",
            "        [-0.0285],\n",
            "        [-0.0298],\n",
            "        [-0.0124],\n",
            "        [-0.0336],\n",
            "        [-0.0201],\n",
            "        [-0.0716]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 9\n",
            "Batch 9 Output: tensor([[-0.0319],\n",
            "        [-0.0065],\n",
            "        [-0.0504],\n",
            "        [-0.0379],\n",
            "        [-0.0625],\n",
            "        [-0.0417],\n",
            "        [-0.0048],\n",
            "        [-0.0260]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 10\n",
            "Batch 10 Output: tensor([[-0.0235],\n",
            "        [-0.0362],\n",
            "        [-0.0510],\n",
            "        [-0.0227],\n",
            "        [-0.0419],\n",
            "        [-0.0237],\n",
            "        [-0.0441],\n",
            "        [-0.0406]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 11\n",
            "Batch 11 Output: tensor([[-0.0355],\n",
            "        [-0.0358],\n",
            "        [-0.0365],\n",
            "        [-0.0229],\n",
            "        [-0.0365],\n",
            "        [-0.0177],\n",
            "        [-0.0226],\n",
            "        [-0.0337]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 12\n",
            "Batch 12 Output: tensor([[-0.0259],\n",
            "        [-0.0283],\n",
            "        [-0.0369],\n",
            "        [-0.0263],\n",
            "        [-0.0145],\n",
            "        [-0.0263],\n",
            "        [-0.0075],\n",
            "        [-0.0232]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 13\n",
            "Batch 13 Output: tensor([[-0.0049],\n",
            "        [-0.0509],\n",
            "        [-0.0453],\n",
            "        [-0.0307],\n",
            "        [-0.0220],\n",
            "        [-0.0345],\n",
            "        [-0.0342],\n",
            "        [-0.0406]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 14\n",
            "Batch 14 Output: tensor([[-0.0475],\n",
            "        [-0.0332],\n",
            "        [-0.0368],\n",
            "        [-0.0468],\n",
            "        [-0.0396],\n",
            "        [-0.0347],\n",
            "        [-0.0281],\n",
            "        [-0.0451]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 15\n",
            "Batch 15 Output: tensor([[-0.0255],\n",
            "        [-0.0340],\n",
            "        [-0.0243],\n",
            "        [-0.0331],\n",
            "        [-0.0315],\n",
            "        [-0.0294],\n",
            "        [-0.0352],\n",
            "        [-0.0290]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 16\n",
            "Batch 16 Output: tensor([[-0.0203],\n",
            "        [-0.0180],\n",
            "        [-0.0186],\n",
            "        [-0.0326],\n",
            "        [-0.0226],\n",
            "        [-0.0183],\n",
            "        [-0.0192],\n",
            "        [-0.0419]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 17\n",
            "Batch 17 Output: tensor([[-0.0399],\n",
            "        [-0.0320],\n",
            "        [-0.0397],\n",
            "        [-0.0231],\n",
            "        [-0.0356],\n",
            "        [-0.0145],\n",
            "        [-0.0186],\n",
            "        [-0.0183]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 18\n",
            "Batch 18 Output: tensor([[-0.0337],\n",
            "        [-0.0170],\n",
            "        [-0.0437],\n",
            "        [-0.0108],\n",
            "        [-0.0042],\n",
            "        [-0.0160],\n",
            "        [-0.0346],\n",
            "        [-0.0494]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 19\n",
            "Batch 19 Output: tensor([[-0.0283],\n",
            "        [-0.0149],\n",
            "        [-0.0296],\n",
            "        [-0.0250],\n",
            "        [-0.0243],\n",
            "        [-0.0173],\n",
            "        [-0.0394],\n",
            "        [-0.0309]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 20\n",
            "Batch 20 Output: tensor([[-0.0073],\n",
            "        [-0.0273],\n",
            "        [-0.0408],\n",
            "        [-0.0568],\n",
            "        [-0.0261],\n",
            "        [-0.0432],\n",
            "        [-0.0357],\n",
            "        [-0.0346]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 21\n",
            "Batch 21 Output: tensor([[-0.0385],\n",
            "        [-0.0276],\n",
            "        [-0.0185],\n",
            "        [-0.0316],\n",
            "        [-0.0368],\n",
            "        [-0.0167],\n",
            "        [-0.0350],\n",
            "        [-0.0159]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 22\n",
            "Batch 22 Output: tensor([[-0.0375],\n",
            "        [-0.0079],\n",
            "        [-0.0395],\n",
            "        [-0.0307],\n",
            "        [-0.0523],\n",
            "        [-0.0288],\n",
            "        [-0.0044],\n",
            "        [-0.0395]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 23\n",
            "Batch 23 Output: tensor([[ 0.0075],\n",
            "        [-0.0269],\n",
            "        [-0.0298],\n",
            "        [-0.0327],\n",
            "        [-0.0253],\n",
            "        [-0.0484],\n",
            "        [-0.0504],\n",
            "        [-0.0436]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 24\n",
            "Batch 24 Output: tensor([[ 0.0048],\n",
            "        [-0.0334],\n",
            "        [-0.0410],\n",
            "        [-0.0130],\n",
            "        [-0.0174],\n",
            "        [-0.0050],\n",
            "        [-0.0287],\n",
            "        [-0.0307]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 25\n",
            "Batch 25 Output: tensor([[-0.0139],\n",
            "        [-0.0331],\n",
            "        [-0.0327],\n",
            "        [ 0.0128],\n",
            "        [-0.0118],\n",
            "        [-0.0294],\n",
            "        [-0.0349],\n",
            "        [-0.0146]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 26\n",
            "Batch 26 Output: tensor([[-0.0251],\n",
            "        [-0.0201],\n",
            "        [-0.0293],\n",
            "        [-0.0209],\n",
            "        [-0.0180],\n",
            "        [-0.0456],\n",
            "        [-0.0455],\n",
            "        [-0.0137]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 27\n",
            "Batch 27 Output: tensor([[-0.0216],\n",
            "        [-0.0197],\n",
            "        [-0.0017],\n",
            "        [-0.0324],\n",
            "        [-0.0258],\n",
            "        [-0.0163],\n",
            "        [-0.0354],\n",
            "        [-0.0297]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 28\n",
            "Batch 28 Output: tensor([[-0.0200],\n",
            "        [-0.0312],\n",
            "        [-0.0383],\n",
            "        [-0.0453],\n",
            "        [-0.0172],\n",
            "        [-0.0384],\n",
            "        [-0.0246],\n",
            "        [-0.0442]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 29\n",
            "Batch 29 Output: tensor([[-0.0255],\n",
            "        [-0.0290],\n",
            "        [-0.0288],\n",
            "        [-0.0275],\n",
            "        [-0.0250],\n",
            "        [-0.0252],\n",
            "        [-0.0361],\n",
            "        [-0.0238]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 30\n",
            "Batch 30 Output: tensor([[-0.0177],\n",
            "        [-0.0497],\n",
            "        [-0.0475],\n",
            "        [-0.0281],\n",
            "        [-0.0188],\n",
            "        [-0.0127],\n",
            "        [-0.0138],\n",
            "        [-0.0186]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 31\n",
            "Batch 31 Output: tensor([[-0.0542],\n",
            "        [-0.0218],\n",
            "        [-0.0385],\n",
            "        [-0.0160],\n",
            "        [-0.0134],\n",
            "        [-0.0128],\n",
            "        [-0.0480],\n",
            "        [-0.0299]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 32\n",
            "Batch 32 Output: tensor([[-0.0304],\n",
            "        [-0.0152],\n",
            "        [-0.0430],\n",
            "        [-0.0261],\n",
            "        [-0.0257],\n",
            "        [-0.0213],\n",
            "        [-0.0297],\n",
            "        [-0.0188]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 33\n",
            "Batch 33 Output: tensor([[-0.0217],\n",
            "        [-0.0184],\n",
            "        [-0.0315],\n",
            "        [-0.0354],\n",
            "        [-0.0316],\n",
            "        [-0.0197],\n",
            "        [-0.0307],\n",
            "        [-0.0397]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 34\n",
            "Batch 34 Output: tensor([[-0.0246],\n",
            "        [-0.0326],\n",
            "        [-0.0260],\n",
            "        [-0.0263],\n",
            "        [-0.0342],\n",
            "        [-0.0272],\n",
            "        [-0.0200],\n",
            "        [ 0.0132]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 35\n",
            "Batch 35 Output: tensor([[-0.0402],\n",
            "        [-0.0101],\n",
            "        [-0.0126],\n",
            "        [-0.0341],\n",
            "        [-0.0296],\n",
            "        [-0.0396],\n",
            "        [-0.0254],\n",
            "        [-0.0199]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 36\n",
            "Batch 36 Output: tensor([[-0.0387],\n",
            "        [-0.0501],\n",
            "        [-0.0093],\n",
            "        [-0.0396],\n",
            "        [-0.0353],\n",
            "        [-0.0398],\n",
            "        [-0.0417],\n",
            "        [-0.0279]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 37\n",
            "Batch 37 Output: tensor([[-0.0228],\n",
            "        [-0.0293],\n",
            "        [-0.0186],\n",
            "        [-0.0167]], grad_fn=<AddmmBackward0>)\n",
            "Epoch 2/10 started\n",
            "Processing Batch 0\n",
            "Batch 0 Output: tensor([[-0.0346],\n",
            "        [-0.0307],\n",
            "        [-0.0297],\n",
            "        [-0.0327],\n",
            "        [-0.0276],\n",
            "        [-0.0283],\n",
            "        [-0.0417],\n",
            "        [-0.0280]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 1\n",
            "Batch 1 Output: tensor([[ 0.0128],\n",
            "        [-0.0362],\n",
            "        [-0.0451],\n",
            "        [-0.0252],\n",
            "        [-0.0392],\n",
            "        [-0.0246],\n",
            "        [-0.0220],\n",
            "        [-0.0259]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 2\n",
            "Batch 2 Output: tensor([[-0.0246],\n",
            "        [-0.0173],\n",
            "        [-0.0288],\n",
            "        [-0.0337],\n",
            "        [-0.0079],\n",
            "        [-0.0430],\n",
            "        [-0.0374],\n",
            "        [-0.0134]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 3\n",
            "Batch 3 Output: tensor([[-0.0361],\n",
            "        [-0.0258],\n",
            "        [-0.0266],\n",
            "        [-0.0299],\n",
            "        [-0.0126],\n",
            "        [-0.0243],\n",
            "        [-0.0461],\n",
            "        [-0.0197]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 4\n",
            "Batch 4 Output: tensor([[-0.0145],\n",
            "        [-0.0177],\n",
            "        [-0.0198],\n",
            "        [-0.0398],\n",
            "        [-0.0197],\n",
            "        [-0.0504],\n",
            "        [-0.0509],\n",
            "        [-0.0216]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 5\n",
            "Batch 5 Output: tensor([[-0.0184],\n",
            "        [-0.0285],\n",
            "        [-0.0387],\n",
            "        [-0.0692],\n",
            "        [-0.0396],\n",
            "        [-0.0263],\n",
            "        [-0.0149],\n",
            "        [-0.0316]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 6\n",
            "Batch 6 Output: tensor([[-0.0288],\n",
            "        [-0.0264],\n",
            "        [-0.0293],\n",
            "        [-0.0394],\n",
            "        [-0.0050],\n",
            "        [-0.0319],\n",
            "        [-0.0279],\n",
            "        [-0.0316]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 7\n",
            "Batch 7 Output: tensor([[-0.0285],\n",
            "        [-0.0226],\n",
            "        [-0.0224],\n",
            "        [-0.0192],\n",
            "        [-0.0183],\n",
            "        [-0.0273],\n",
            "        [-0.0435],\n",
            "        [-0.0148]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 8\n",
            "Batch 8 Output: tensor([[-0.0152],\n",
            "        [-0.0294],\n",
            "        [-0.0328],\n",
            "        [-0.0215],\n",
            "        [-0.0180],\n",
            "        [-0.0453],\n",
            "        [-0.0298],\n",
            "        [-0.0341]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 9\n",
            "Batch 9 Output: tensor([[-0.0042],\n",
            "        [-0.0418],\n",
            "        [-0.0358],\n",
            "        [-0.0261],\n",
            "        [-0.0324],\n",
            "        [-0.0213],\n",
            "        [-0.0410],\n",
            "        [-0.0238]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 10\n",
            "Batch 10 Output: tensor([[-0.0130],\n",
            "        [-0.0256],\n",
            "        [-0.0396],\n",
            "        [-0.0261],\n",
            "        [-0.0255],\n",
            "        [-0.0357],\n",
            "        [-0.0183],\n",
            "        [-0.0075]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 11\n",
            "Batch 11 Output: tensor([[-0.0455],\n",
            "        [-0.0232],\n",
            "        [-0.0475],\n",
            "        [-0.0128],\n",
            "        [-0.0408],\n",
            "        [-0.0399],\n",
            "        [-0.0354],\n",
            "        [-0.0209]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 12\n",
            "Batch 12 Output: tensor([[-0.0375],\n",
            "        [-0.0321],\n",
            "        [-0.0307],\n",
            "        [-0.0356],\n",
            "        [-0.0186],\n",
            "        [-0.0504],\n",
            "        [-0.0523],\n",
            "        [-0.0716]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 13\n",
            "Batch 13 Output: tensor([[-0.0442],\n",
            "        [-0.0345],\n",
            "        [-0.0585],\n",
            "        [-0.0126],\n",
            "        [-0.0227],\n",
            "        [-0.0368],\n",
            "        [-0.0480],\n",
            "        [-0.0360]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 14\n",
            "Batch 14 Output: tensor([[-0.0246],\n",
            "        [-0.0048],\n",
            "        [-0.0160],\n",
            "        [-0.0625],\n",
            "        [-0.0342],\n",
            "        [-0.0137],\n",
            "        [-0.0337],\n",
            "        [-0.0368]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 15\n",
            "Batch 15 Output: tensor([[-0.0044],\n",
            "        [-0.0396],\n",
            "        [-0.0065],\n",
            "        [-0.0235],\n",
            "        [-0.0406],\n",
            "        [-0.0389],\n",
            "        [-0.0287],\n",
            "        [-0.0188]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 16\n",
            "Batch 16 Output: tensor([[-0.0250],\n",
            "        [ 0.0075],\n",
            "        [-0.0384],\n",
            "        [-0.0309],\n",
            "        [-0.0167],\n",
            "        [-0.0124],\n",
            "        [-0.0017],\n",
            "        [-0.0288]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 17\n",
            "Batch 17 Output: tensor([[-0.0273],\n",
            "        [-0.0177],\n",
            "        [-0.0205],\n",
            "        [-0.0468],\n",
            "        [-0.0331],\n",
            "        [-0.0501],\n",
            "        [-0.0227],\n",
            "        [-0.0327]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 18\n",
            "Batch 18 Output: tensor([[-0.0355],\n",
            "        [-0.0385],\n",
            "        [-0.0354],\n",
            "        [-0.0075],\n",
            "        [-0.0272],\n",
            "        [-0.0419],\n",
            "        [-0.0127],\n",
            "        [-0.0327]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 19\n",
            "Batch 19 Output: tensor([[-0.0310],\n",
            "        [-0.0253],\n",
            "        [-0.0453],\n",
            "        [-0.0163],\n",
            "        [-0.0379],\n",
            "        [-0.0397],\n",
            "        [-0.0224],\n",
            "        [-0.0272]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 20\n",
            "Batch 20 Output: tensor([[-0.0199],\n",
            "        [-0.0290],\n",
            "        [-0.0331],\n",
            "        [-0.0342],\n",
            "        [-0.0296],\n",
            "        [-0.0432],\n",
            "        [-0.0290],\n",
            "        [-0.0185]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 21\n",
            "Batch 21 Output: tensor([[-0.0271],\n",
            "        [-0.0049],\n",
            "        [-0.0326],\n",
            "        [-0.0281],\n",
            "        [-0.0266],\n",
            "        [-0.0237],\n",
            "        [-0.0251],\n",
            "        [-0.0303]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 22\n",
            "Batch 22 Output: tensor([[-0.0260],\n",
            "        [-0.0170],\n",
            "        [-0.0406],\n",
            "        [-0.0180],\n",
            "        [-0.0412],\n",
            "        [-0.0361],\n",
            "        [-0.0419],\n",
            "        [-0.0248]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 23\n",
            "Batch 23 Output: tensor([[-0.0093],\n",
            "        [-0.0250],\n",
            "        [-0.0275],\n",
            "        [-0.0174],\n",
            "        [-0.0437],\n",
            "        [-0.0336],\n",
            "        [-0.0453],\n",
            "        [-0.0186]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 24\n",
            "Batch 24 Output: tensor([[-0.0369],\n",
            "        [-0.0200],\n",
            "        [ 0.0132],\n",
            "        [-0.0260],\n",
            "        [-0.0364],\n",
            "        [-0.0172],\n",
            "        [-0.0326],\n",
            "        [-0.0307]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 25\n",
            "Batch 25 Output: tensor([[-0.0255],\n",
            "        [-0.0340],\n",
            "        [-0.0310],\n",
            "        [-0.0200],\n",
            "        [-0.0365],\n",
            "        [-0.0227],\n",
            "        [-0.0300],\n",
            "        [-0.0268]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 26\n",
            "Batch 26 Output: tensor([[-0.0296],\n",
            "        [-0.0218],\n",
            "        [-0.0118],\n",
            "        [-0.0475],\n",
            "        [-0.0145],\n",
            "        [-0.0315],\n",
            "        [-0.0217],\n",
            "        [-0.0269]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 27\n",
            "Batch 27 Output: tensor([[-0.0119],\n",
            "        [-0.0297],\n",
            "        [-0.0175],\n",
            "        [-0.0186],\n",
            "        [-0.0395],\n",
            "        [-0.0257],\n",
            "        [-0.0349],\n",
            "        [-0.0542]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 28\n",
            "Batch 28 Output: tensor([[-0.0441],\n",
            "        [-0.0207],\n",
            "        [-0.0336],\n",
            "        [-0.0203],\n",
            "        [-0.0333],\n",
            "        [-0.0201],\n",
            "        [-0.0541],\n",
            "        [-0.0383]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 29\n",
            "Batch 29 Output: tensor([[-0.0385],\n",
            "        [-0.0320],\n",
            "        [-0.0402],\n",
            "        [-0.0365],\n",
            "        [-0.0188],\n",
            "        [-0.0231],\n",
            "        [-0.0307],\n",
            "        [-0.0384]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 30\n",
            "Batch 30 Output: tensor([[-0.0294],\n",
            "        [-0.0201],\n",
            "        [-0.0345],\n",
            "        [-0.0144],\n",
            "        [-0.0494],\n",
            "        [-0.0347],\n",
            "        [-0.0263],\n",
            "        [-0.0243]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 31\n",
            "Batch 31 Output: tensor([[-0.0334],\n",
            "        [-0.0167],\n",
            "        [-0.0315],\n",
            "        [-0.0146],\n",
            "        [-0.0568],\n",
            "        [-0.0244],\n",
            "        [-0.0102],\n",
            "        [-0.0159]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 32\n",
            "Batch 32 Output: tensor([[-0.0283],\n",
            "        [-0.0263],\n",
            "        [-0.0456],\n",
            "        [-0.0138],\n",
            "        [-0.0118],\n",
            "        [-0.0259],\n",
            "        [-0.0199],\n",
            "        [-0.0073]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 33\n",
            "Batch 33 Output: tensor([[-0.0226],\n",
            "        [-0.0332],\n",
            "        [-0.0298],\n",
            "        [-0.0108],\n",
            "        [-0.0293],\n",
            "        [-0.0178],\n",
            "        [-0.0101],\n",
            "        [-0.0268]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 34\n",
            "Batch 34 Output: tensor([[-0.0186],\n",
            "        [-0.0250],\n",
            "        [-0.0203],\n",
            "        [-0.0397],\n",
            "        [-0.0312],\n",
            "        [-0.0284],\n",
            "        [-0.0436],\n",
            "        [-0.0350]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 35\n",
            "Batch 35 Output: tensor([[-0.0395],\n",
            "        [-0.0352],\n",
            "        [-0.0226],\n",
            "        [-0.0353],\n",
            "        [-0.0417],\n",
            "        [-0.0245],\n",
            "        [-0.0139],\n",
            "        [ 0.0048]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 36\n",
            "Batch 36 Output: tensor([[-0.0254],\n",
            "        [-0.0160],\n",
            "        [-0.0229],\n",
            "        [-0.0497],\n",
            "        [-0.0159],\n",
            "        [-0.0484],\n",
            "        [-0.0255],\n",
            "        [-0.0346]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 37\n",
            "Batch 37 Output: tensor([[-0.0281],\n",
            "        [-0.0510],\n",
            "        [-0.0304],\n",
            "        [-0.0228]], grad_fn=<AddmmBackward0>)\n",
            "Epoch 3/10 started\n",
            "Processing Batch 0\n",
            "Batch 0 Output: tensor([[-0.0436],\n",
            "        [-0.0307],\n",
            "        [-0.0119],\n",
            "        [-0.0075],\n",
            "        [-0.0275],\n",
            "        [-0.0279],\n",
            "        [-0.0266],\n",
            "        [-0.0340]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 1\n",
            "Batch 1 Output: tensor([[-0.0253],\n",
            "        [-0.0173],\n",
            "        [-0.0124],\n",
            "        [-0.0205],\n",
            "        [-0.0399],\n",
            "        [-0.0442],\n",
            "        [-0.0263],\n",
            "        [-0.0328]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 2\n",
            "Batch 2 Output: tensor([[-0.0345],\n",
            "        [-0.0501],\n",
            "        [-0.0215],\n",
            "        [-0.0118],\n",
            "        [-0.0126],\n",
            "        [-0.0333],\n",
            "        [-0.0231],\n",
            "        [-0.0307]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 3\n",
            "Batch 3 Output: tensor([[-0.0321],\n",
            "        [-0.0327],\n",
            "        [-0.0237],\n",
            "        [-0.0350],\n",
            "        [-0.0216],\n",
            "        [-0.0163],\n",
            "        [-0.0268],\n",
            "        [-0.0183]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 4\n",
            "Batch 4 Output: tensor([[-0.0299],\n",
            "        [-0.0331],\n",
            "        [-0.0296],\n",
            "        [-0.0484],\n",
            "        [-0.0326],\n",
            "        [-0.0255],\n",
            "        [-0.0298],\n",
            "        [-0.0336]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 5\n",
            "Batch 5 Output: tensor([[-0.0326],\n",
            "        [-0.0568],\n",
            "        [-0.0256],\n",
            "        [-0.0209],\n",
            "        [-0.0316],\n",
            "        [-0.0441],\n",
            "        [-0.0258],\n",
            "        [-0.0226]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 6\n",
            "Batch 6 Output: tensor([[-0.0346],\n",
            "        [-0.0260],\n",
            "        [-0.0246],\n",
            "        [-0.0320],\n",
            "        [-0.0298],\n",
            "        [-0.0073],\n",
            "        [-0.0396],\n",
            "        [-0.0017]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 7\n",
            "Batch 7 Output: tensor([[-0.0336],\n",
            "        [-0.0276],\n",
            "        [-0.0177],\n",
            "        [-0.0346],\n",
            "        [-0.0272],\n",
            "        [-0.0126],\n",
            "        [-0.0235],\n",
            "        [-0.0504]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 8\n",
            "Batch 8 Output: tensor([[-0.0310],\n",
            "        [-0.0356],\n",
            "        [-0.0137],\n",
            "        [-0.0296],\n",
            "        [-0.0412],\n",
            "        [-0.0327],\n",
            "        [ 0.0128],\n",
            "        [-0.0207]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 9\n",
            "Batch 9 Output: tensor([[-0.0178],\n",
            "        [-0.0288],\n",
            "        [-0.0307],\n",
            "        [-0.0245],\n",
            "        [ 0.0048],\n",
            "        [-0.0283],\n",
            "        [-0.0345],\n",
            "        [-0.0437]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 10\n",
            "Batch 10 Output: tensor([[-0.0402],\n",
            "        [-0.0349],\n",
            "        [-0.0312],\n",
            "        [-0.0149],\n",
            "        [-0.0418],\n",
            "        [-0.0324],\n",
            "        [-0.0272],\n",
            "        [-0.0159]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 11\n",
            "Batch 11 Output: tensor([[-0.0453],\n",
            "        [-0.0224],\n",
            "        [-0.0368],\n",
            "        [-0.0180],\n",
            "        [-0.0541],\n",
            "        [-0.0453],\n",
            "        [-0.0285],\n",
            "        [-0.0048]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 12\n",
            "Batch 12 Output: tensor([[-0.0244],\n",
            "        [-0.0397],\n",
            "        [-0.0337],\n",
            "        [-0.0203],\n",
            "        [-0.0227],\n",
            "        [-0.0585],\n",
            "        [-0.0396],\n",
            "        [-0.0375]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 13\n",
            "Batch 13 Output: tensor([[-0.0139],\n",
            "        [-0.0145],\n",
            "        [-0.0203],\n",
            "        [-0.0342],\n",
            "        [-0.0396],\n",
            "        [-0.0294],\n",
            "        [-0.0385],\n",
            "        [-0.0243]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 14\n",
            "Batch 14 Output: tensor([[-0.0360],\n",
            "        [-0.0332],\n",
            "        [-0.0497],\n",
            "        [-0.0347],\n",
            "        [-0.0334],\n",
            "        [-0.0406],\n",
            "        [-0.0259],\n",
            "        [ 0.0075]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 15\n",
            "Batch 15 Output: tensor([[-0.0263],\n",
            "        [-0.0352],\n",
            "        [-0.0383],\n",
            "        [-0.0102],\n",
            "        [-0.0283],\n",
            "        [-0.0285],\n",
            "        [-0.0201],\n",
            "        [-0.0307]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 16\n",
            "Batch 16 Output: tensor([[-0.0316],\n",
            "        [-0.0138],\n",
            "        [-0.0358],\n",
            "        [-0.0108],\n",
            "        [-0.0293],\n",
            "        [-0.0227],\n",
            "        [-0.0361],\n",
            "        [-0.0183]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 17\n",
            "Batch 17 Output: tensor([[-0.0261],\n",
            "        [-0.0288],\n",
            "        [-0.0456],\n",
            "        [-0.0252],\n",
            "        [-0.0353],\n",
            "        [-0.0042],\n",
            "        [-0.0451],\n",
            "        [-0.0354]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 18\n",
            "Batch 18 Output: tensor([[-0.0319],\n",
            "        [-0.0130],\n",
            "        [-0.0692],\n",
            "        [-0.0310],\n",
            "        [-0.0152],\n",
            "        [-0.0494],\n",
            "        [-0.0263],\n",
            "        [-0.0213]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 19\n",
            "Batch 19 Output: tensor([[-0.0199],\n",
            "        [-0.0159],\n",
            "        [-0.0198],\n",
            "        [-0.0200],\n",
            "        [-0.0269],\n",
            "        [-0.0304],\n",
            "        [-0.0509],\n",
            "        [-0.0341]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 20\n",
            "Batch 20 Output: tensor([[-0.0384],\n",
            "        [-0.0146],\n",
            "        [-0.0455],\n",
            "        [-0.0504],\n",
            "        [-0.0365],\n",
            "        [-0.0128],\n",
            "        [-0.0167],\n",
            "        [-0.0224]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 21\n",
            "Batch 21 Output: tensor([[-0.0395],\n",
            "        [-0.0217],\n",
            "        [-0.0170],\n",
            "        [-0.0293],\n",
            "        [-0.0197],\n",
            "        [-0.0379],\n",
            "        [-0.0417],\n",
            "        [-0.0075]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 22\n",
            "Batch 22 Output: tensor([[-0.0475],\n",
            "        [-0.0309],\n",
            "        [-0.0523],\n",
            "        [-0.0303],\n",
            "        [-0.0385],\n",
            "        [-0.0384],\n",
            "        [-0.0255],\n",
            "        [-0.0542]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 23\n",
            "Batch 23 Output: tensor([[-0.0419],\n",
            "        [-0.0280],\n",
            "        [-0.0266],\n",
            "        [-0.0232],\n",
            "        [-0.0398],\n",
            "        [-0.0357],\n",
            "        [-0.0144],\n",
            "        [-0.0475]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 24\n",
            "Batch 24 Output: tensor([[-0.0331],\n",
            "        [-0.0290],\n",
            "        [-0.0251],\n",
            "        [-0.0364],\n",
            "        [-0.0200],\n",
            "        [-0.0228],\n",
            "        [-0.0160],\n",
            "        [-0.0315]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 25\n",
            "Batch 25 Output: tensor([[-0.0716],\n",
            "        [-0.0394],\n",
            "        [-0.0188],\n",
            "        [-0.0273],\n",
            "        [-0.0175],\n",
            "        [-0.0261],\n",
            "        [-0.0148],\n",
            "        [-0.0049]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 26\n",
            "Batch 26 Output: tensor([[-0.0199],\n",
            "        [-0.0271],\n",
            "        [-0.0354],\n",
            "        [-0.0288],\n",
            "        [-0.0134],\n",
            "        [-0.0226],\n",
            "        [-0.0453],\n",
            "        [-0.0260]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 27\n",
            "Batch 27 Output: tensor([[-0.0160],\n",
            "        [-0.0192],\n",
            "        [-0.0250],\n",
            "        [-0.0387],\n",
            "        [-0.0430],\n",
            "        [-0.0255],\n",
            "        [-0.0201],\n",
            "        [-0.0167]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 28\n",
            "Batch 28 Output: tensor([[-0.0362],\n",
            "        [-0.0410],\n",
            "        [-0.0065],\n",
            "        [-0.0369],\n",
            "        [-0.0184],\n",
            "        [-0.0186],\n",
            "        [-0.0229],\n",
            "        [-0.0281]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 29\n",
            "Batch 29 Output: tensor([[-0.0480],\n",
            "        [-0.0254],\n",
            "        [-0.0264],\n",
            "        [-0.0327],\n",
            "        [-0.0101],\n",
            "        [-0.0361],\n",
            "        [-0.0419],\n",
            "        [-0.0188]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 30\n",
            "Batch 30 Output: tensor([[-0.0093],\n",
            "        [-0.0417],\n",
            "        [-0.0392],\n",
            "        [-0.0273],\n",
            "        [-0.0432],\n",
            "        [-0.0468],\n",
            "        [ 0.0132],\n",
            "        [-0.0397]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 31\n",
            "Batch 31 Output: tensor([[-0.0177],\n",
            "        [-0.0250],\n",
            "        [-0.0294],\n",
            "        [-0.0220],\n",
            "        [-0.0226],\n",
            "        [-0.0218],\n",
            "        [-0.0297],\n",
            "        [-0.0435]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 32\n",
            "Batch 32 Output: tensor([[-0.0185],\n",
            "        [-0.0297],\n",
            "        [-0.0268],\n",
            "        [-0.0287],\n",
            "        [-0.0355],\n",
            "        [-0.0259],\n",
            "        [-0.0281],\n",
            "        [-0.0174]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 33\n",
            "Batch 33 Output: tensor([[-0.0172],\n",
            "        [-0.0365],\n",
            "        [-0.0118],\n",
            "        [-0.0050],\n",
            "        [-0.0625],\n",
            "        [-0.0238],\n",
            "        [-0.0284],\n",
            "        [-0.0248]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 34\n",
            "Batch 34 Output: tensor([[-0.0246],\n",
            "        [-0.0186],\n",
            "        [-0.0510],\n",
            "        [-0.0257],\n",
            "        [-0.0243],\n",
            "        [-0.0389],\n",
            "        [-0.0337],\n",
            "        [-0.0290]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 35\n",
            "Batch 35 Output: tensor([[-0.0406],\n",
            "        [-0.0246],\n",
            "        [-0.0079],\n",
            "        [-0.0186],\n",
            "        [-0.0250],\n",
            "        [-0.0227],\n",
            "        [-0.0408],\n",
            "        [-0.0300]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 36\n",
            "Batch 36 Output: tensor([[-0.0127],\n",
            "        [-0.0374],\n",
            "        [-0.0342],\n",
            "        [-0.0186],\n",
            "        [-0.0044],\n",
            "        [-0.0395],\n",
            "        [-0.0461],\n",
            "        [-0.0197]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 37\n",
            "Batch 37 Output: tensor([[-0.0145],\n",
            "        [-0.0180],\n",
            "        [-0.0315],\n",
            "        [-0.0368]], grad_fn=<AddmmBackward0>)\n",
            "Epoch 4/10 started\n",
            "Processing Batch 0\n",
            "Batch 0 Output: tensor([[-0.0108],\n",
            "        [-0.0263],\n",
            "        [-0.0350],\n",
            "        [-0.0523],\n",
            "        [-0.0126],\n",
            "        [-0.0199],\n",
            "        [-0.0453],\n",
            "        [-0.0307]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 1\n",
            "Batch 1 Output: tensor([[-0.0148],\n",
            "        [-0.0285],\n",
            "        [-0.0419],\n",
            "        [-0.0285],\n",
            "        [-0.0374],\n",
            "        [-0.0288],\n",
            "        [-0.0183],\n",
            "        [-0.0280]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 2\n",
            "Batch 2 Output: tensor([[-0.0250],\n",
            "        [-0.0102],\n",
            "        [-0.0197],\n",
            "        [-0.0186],\n",
            "        [-0.0260],\n",
            "        [-0.0101],\n",
            "        [-0.0218],\n",
            "        [-0.0354]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 3\n",
            "Batch 3 Output: tensor([[-0.0075],\n",
            "        [-0.0364],\n",
            "        [-0.0220],\n",
            "        [-0.0436],\n",
            "        [ 0.0048],\n",
            "        [-0.0261],\n",
            "        [-0.0127],\n",
            "        [-0.0341]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 4\n",
            "Batch 4 Output: tensor([[-0.0163],\n",
            "        [-0.0451],\n",
            "        [-0.0238],\n",
            "        [-0.0145],\n",
            "        [-0.0251],\n",
            "        [-0.0246],\n",
            "        [-0.0177],\n",
            "        [-0.0173]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 5\n",
            "Batch 5 Output: tensor([[-0.0216],\n",
            "        [-0.0065],\n",
            "        [-0.0203],\n",
            "        [-0.0347],\n",
            "        [-0.0475],\n",
            "        [-0.0259],\n",
            "        [-0.0174],\n",
            "        [-0.0365]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 6\n",
            "Batch 6 Output: tensor([[-0.0332],\n",
            "        [-0.0257],\n",
            "        [-0.0716],\n",
            "        [-0.0346],\n",
            "        [-0.0186],\n",
            "        [-0.0379],\n",
            "        [-0.0215],\n",
            "        [-0.0255]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 7\n",
            "Batch 7 Output: tensor([[-0.0437],\n",
            "        [-0.0263],\n",
            "        [-0.0442],\n",
            "        [-0.0139],\n",
            "        [-0.0203],\n",
            "        [-0.0337],\n",
            "        [-0.0017],\n",
            "        [-0.0337]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 8\n",
            "Batch 8 Output: tensor([[-0.0048],\n",
            "        [-0.0268],\n",
            "        [-0.0494],\n",
            "        [-0.0327],\n",
            "        [-0.0283],\n",
            "        [-0.0227],\n",
            "        [-0.0480],\n",
            "        [-0.0397]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 9\n",
            "Batch 9 Output: tensor([[-0.0368],\n",
            "        [-0.0144],\n",
            "        [-0.0385],\n",
            "        [-0.0186],\n",
            "        [-0.0349],\n",
            "        [-0.0224],\n",
            "        [-0.0272],\n",
            "        [-0.0226]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 10\n",
            "Batch 10 Output: tensor([[-0.0340],\n",
            "        [-0.0217],\n",
            "        [-0.0441],\n",
            "        [-0.0327],\n",
            "        [-0.0209],\n",
            "        [-0.0290],\n",
            "        [-0.0346],\n",
            "        [-0.0307]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 11\n",
            "Batch 11 Output: tensor([[-0.0384],\n",
            "        [-0.0226],\n",
            "        [-0.0360],\n",
            "        [-0.0384],\n",
            "        [-0.0430],\n",
            "        [-0.0272],\n",
            "        [-0.0130],\n",
            "        [-0.0276]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 12\n",
            "Batch 12 Output: tensor([[-0.0395],\n",
            "        [-0.0246],\n",
            "        [-0.0342],\n",
            "        [-0.0197],\n",
            "        [-0.0315],\n",
            "        [-0.0200],\n",
            "        [-0.0484],\n",
            "        [-0.0243]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 13\n",
            "Batch 13 Output: tensor([[-0.0279],\n",
            "        [-0.0312],\n",
            "        [-0.0258],\n",
            "        [-0.0303],\n",
            "        [-0.0293],\n",
            "        [-0.0185],\n",
            "        [-0.0188],\n",
            "        [-0.0266]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 14\n",
            "Batch 14 Output: tensor([[-0.0368],\n",
            "        [-0.0504],\n",
            "        [-0.0183],\n",
            "        [-0.0345],\n",
            "        [-0.0345],\n",
            "        [-0.0310],\n",
            "        [-0.0418],\n",
            "        [-0.0268]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 15\n",
            "Batch 15 Output: tensor([[-0.0200],\n",
            "        [-0.0294],\n",
            "        [-0.0327],\n",
            "        [-0.0201],\n",
            "        [-0.0243],\n",
            "        [-0.0273],\n",
            "        [-0.0542],\n",
            "        [-0.0290]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 16\n",
            "Batch 16 Output: tensor([[-0.0369],\n",
            "        [-0.0435],\n",
            "        [-0.0501],\n",
            "        [-0.0357],\n",
            "        [-0.0198],\n",
            "        [-0.0213],\n",
            "        [-0.0585],\n",
            "        [-0.0358]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 17\n",
            "Batch 17 Output: tensor([[-0.0333],\n",
            "        [-0.0261],\n",
            "        [-0.0283],\n",
            "        [-0.0160],\n",
            "        [-0.0406],\n",
            "        [-0.0321],\n",
            "        [-0.0361],\n",
            "        [-0.0353]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 18\n",
            "Batch 18 Output: tensor([[-0.0541],\n",
            "        [-0.0396],\n",
            "        [-0.0315],\n",
            "        [-0.0167],\n",
            "        [-0.0324],\n",
            "        [ 0.0075],\n",
            "        [-0.0266],\n",
            "        [-0.0254]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 19\n",
            "Batch 19 Output: tensor([[-0.0417],\n",
            "        [-0.0385],\n",
            "        [-0.0128],\n",
            "        [-0.0389],\n",
            "        [-0.0453],\n",
            "        [-0.0199],\n",
            "        [-0.0396],\n",
            "        [-0.0146]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 20\n",
            "Batch 20 Output: tensor([[-0.0201],\n",
            "        [-0.0296],\n",
            "        [-0.0304],\n",
            "        [-0.0475],\n",
            "        [-0.0134],\n",
            "        [-0.0205],\n",
            "        [-0.0273],\n",
            "        [-0.0354]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 21\n",
            "Batch 21 Output: tensor([[-0.0188],\n",
            "        [-0.0396],\n",
            "        [-0.0287],\n",
            "        [-0.0383],\n",
            "        [-0.0307],\n",
            "        [-0.0175],\n",
            "        [-0.0509],\n",
            "        [-0.0288]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 22\n",
            "Batch 22 Output: tensor([[-0.0235],\n",
            "        [-0.0256],\n",
            "        [-0.0263],\n",
            "        [-0.0159],\n",
            "        [-0.0255],\n",
            "        [-0.0050],\n",
            "        [-0.0417],\n",
            "        [-0.0244]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 23\n",
            "Batch 23 Output: tensor([[-0.0192],\n",
            "        [-0.0118],\n",
            "        [-0.0044],\n",
            "        [-0.0184],\n",
            "        [-0.0497],\n",
            "        [-0.0172],\n",
            "        [-0.0402],\n",
            "        [-0.0042]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 24\n",
            "Batch 24 Output: tensor([[-0.0275],\n",
            "        [-0.0124],\n",
            "        [-0.0625],\n",
            "        [-0.0468],\n",
            "        [-0.0119],\n",
            "        [-0.0178],\n",
            "        [ 0.0128],\n",
            "        [-0.0392]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 25\n",
            "Batch 25 Output: tensor([[-0.0419],\n",
            "        [-0.0149],\n",
            "        [-0.0365],\n",
            "        [-0.0288],\n",
            "        [-0.0126],\n",
            "        [-0.0355],\n",
            "        [-0.0375],\n",
            "        [-0.0455]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 26\n",
            "Batch 26 Output: tensor([[-0.0252],\n",
            "        [-0.0245],\n",
            "        [-0.0281],\n",
            "        [-0.0170],\n",
            "        [-0.0294],\n",
            "        [-0.0271],\n",
            "        [-0.0180],\n",
            "        [-0.0352]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 27\n",
            "Batch 27 Output: tensor([[-0.0397],\n",
            "        [-0.0224],\n",
            "        [-0.0361],\n",
            "        [-0.0331],\n",
            "        [-0.0264],\n",
            "        [-0.0387],\n",
            "        [-0.0259],\n",
            "        [-0.0269]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 28\n",
            "Batch 28 Output: tensor([[-0.0298],\n",
            "        [-0.0316],\n",
            "        [-0.0299],\n",
            "        [-0.0399],\n",
            "        [-0.0336],\n",
            "        [-0.0395],\n",
            "        [-0.0152],\n",
            "        [-0.0237]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 29\n",
            "Batch 29 Output: tensor([[-0.0118],\n",
            "        [-0.0232],\n",
            "        [-0.0336],\n",
            "        [-0.0079],\n",
            "        [-0.0412],\n",
            "        [-0.0226],\n",
            "        [-0.0159],\n",
            "        [-0.0250]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 30\n",
            "Batch 30 Output: tensor([[-0.0227],\n",
            "        [-0.0692],\n",
            "        [-0.0310],\n",
            "        [-0.0093],\n",
            "        [-0.0186],\n",
            "        [-0.0250],\n",
            "        [-0.0207],\n",
            "        [-0.0453]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 31\n",
            "Batch 31 Output: tensor([[-0.0297],\n",
            "        [-0.0326],\n",
            "        [-0.0300],\n",
            "        [-0.0075],\n",
            "        [-0.0228],\n",
            "        [-0.0307],\n",
            "        [-0.0297],\n",
            "        [-0.0394]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 32\n",
            "Batch 32 Output: tensor([[-0.0281],\n",
            "        [-0.0145],\n",
            "        [-0.0229],\n",
            "        [-0.0255],\n",
            "        [-0.0342],\n",
            "        [-0.0248],\n",
            "        [-0.0320],\n",
            "        [-0.0316]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 33\n",
            "Batch 33 Output: tensor([[-0.0073],\n",
            "        [-0.0504],\n",
            "        [-0.0328],\n",
            "        [-0.0319],\n",
            "        [-0.0408],\n",
            "        [-0.0432],\n",
            "        [-0.0356],\n",
            "        [-0.0137]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 34\n",
            "Batch 34 Output: tensor([[-0.0231],\n",
            "        [-0.0456],\n",
            "        [-0.0260],\n",
            "        [-0.0177],\n",
            "        [-0.0284],\n",
            "        [-0.0227],\n",
            "        [ 0.0132],\n",
            "        [-0.0334]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 35\n",
            "Batch 35 Output: tensor([[-0.0138],\n",
            "        [-0.0331],\n",
            "        [-0.0398],\n",
            "        [-0.0568],\n",
            "        [-0.0406],\n",
            "        [-0.0160],\n",
            "        [-0.0167],\n",
            "        [-0.0362]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 36\n",
            "Batch 36 Output: tensor([[-0.0510],\n",
            "        [-0.0296],\n",
            "        [-0.0293],\n",
            "        [-0.0246],\n",
            "        [-0.0253],\n",
            "        [-0.0461],\n",
            "        [-0.0180],\n",
            "        [-0.0298]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 37\n",
            "Batch 37 Output: tensor([[-0.0049],\n",
            "        [-0.0410],\n",
            "        [-0.0326],\n",
            "        [-0.0309]], grad_fn=<AddmmBackward0>)\n",
            "Epoch 5/10 started\n",
            "Processing Batch 0\n",
            "Batch 0 Output: tensor([[-0.0146],\n",
            "        [-0.0309],\n",
            "        [-0.0395],\n",
            "        [-0.0186],\n",
            "        [-0.0290],\n",
            "        [-0.0017],\n",
            "        [-0.0354],\n",
            "        [-0.0288]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 1\n",
            "Batch 1 Output: tensor([[-0.0419],\n",
            "        [-0.0108],\n",
            "        [-0.0259],\n",
            "        [-0.0220],\n",
            "        [-0.0272],\n",
            "        [-0.0310],\n",
            "        [-0.0192],\n",
            "        [-0.0235]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 2\n",
            "Batch 2 Output: tensor([[-0.0215],\n",
            "        [-0.0263],\n",
            "        [ 0.0075],\n",
            "        [-0.0226],\n",
            "        [-0.0475],\n",
            "        [-0.0246],\n",
            "        [-0.0501],\n",
            "        [-0.0285]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 3\n",
            "Batch 3 Output: tensor([[-0.0297],\n",
            "        [-0.0160],\n",
            "        [-0.0337],\n",
            "        [-0.0245],\n",
            "        [-0.0257],\n",
            "        [-0.0200],\n",
            "        [-0.0307],\n",
            "        [-0.0326]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 4\n",
            "Batch 4 Output: tensor([[-0.0263],\n",
            "        [-0.0152],\n",
            "        [-0.0229],\n",
            "        [-0.0224],\n",
            "        [-0.0250],\n",
            "        [-0.0231],\n",
            "        [-0.0312],\n",
            "        [-0.0336]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 5\n",
            "Batch 5 Output: tensor([[-0.0177],\n",
            "        [-0.0299],\n",
            "        [-0.0186],\n",
            "        [-0.0281],\n",
            "        [-0.0345],\n",
            "        [-0.0203],\n",
            "        [-0.0374],\n",
            "        [-0.0252]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 6\n",
            "Batch 6 Output: tensor([[-0.0307],\n",
            "        [-0.0387],\n",
            "        [-0.0392],\n",
            "        [-0.0250],\n",
            "        [-0.0268],\n",
            "        [-0.0197],\n",
            "        [-0.0269],\n",
            "        [-0.0183]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 7\n",
            "Batch 7 Output: tensor([[-0.0408],\n",
            "        [-0.0284],\n",
            "        [-0.0358],\n",
            "        [-0.0397],\n",
            "        [-0.0102],\n",
            "        [-0.0319],\n",
            "        [-0.0304],\n",
            "        [-0.0368]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 8\n",
            "Batch 8 Output: tensor([[-0.0145],\n",
            "        [-0.0246],\n",
            "        [-0.0332],\n",
            "        [-0.0402],\n",
            "        [-0.0568],\n",
            "        [-0.0327],\n",
            "        [-0.0294],\n",
            "        [-0.0320]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 9\n",
            "Batch 9 Output: tensor([[-0.0360],\n",
            "        [-0.0394],\n",
            "        [-0.0049],\n",
            "        [-0.0197],\n",
            "        [-0.0079],\n",
            "        [-0.0412],\n",
            "        [-0.0159],\n",
            "        [-0.0340]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 10\n",
            "Batch 10 Output: tensor([[-0.0283],\n",
            "        [-0.0101],\n",
            "        [-0.0266],\n",
            "        [-0.0260],\n",
            "        [-0.0203],\n",
            "        [-0.0243],\n",
            "        [-0.0342],\n",
            "        [-0.0180]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 11\n",
            "Batch 11 Output: tensor([[-0.0126],\n",
            "        [-0.0266],\n",
            "        [-0.0432],\n",
            "        [-0.0258],\n",
            "        [-0.0509],\n",
            "        [-0.0315],\n",
            "        [-0.0307],\n",
            "        [-0.0310]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 12\n",
            "Batch 12 Output: tensor([[-0.0139],\n",
            "        [-0.0261],\n",
            "        [-0.0375],\n",
            "        [-0.0198],\n",
            "        [-0.0337],\n",
            "        [-0.0159],\n",
            "        [-0.0163],\n",
            "        [-0.0261]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 13\n",
            "Batch 13 Output: tensor([[-0.0455],\n",
            "        [-0.0315],\n",
            "        [-0.0327],\n",
            "        [-0.0461],\n",
            "        [-0.0541],\n",
            "        [-0.0417],\n",
            "        [-0.0328],\n",
            "        [-0.0396]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 14\n",
            "Batch 14 Output: tensor([[-0.0321],\n",
            "        [-0.0127],\n",
            "        [-0.0285],\n",
            "        [-0.0418],\n",
            "        [-0.0396],\n",
            "        [-0.0494],\n",
            "        [-0.0188],\n",
            "        [-0.0379]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 15\n",
            "Batch 15 Output: tensor([[-0.0251],\n",
            "        [-0.0200],\n",
            "        [-0.0238],\n",
            "        [-0.0383],\n",
            "        [-0.0451],\n",
            "        [-0.0316],\n",
            "        [-0.0065],\n",
            "        [-0.0093]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 16\n",
            "Batch 16 Output: tensor([[-0.0336],\n",
            "        [-0.0177],\n",
            "        [-0.0260],\n",
            "        [-0.0244],\n",
            "        [-0.0186],\n",
            "        [-0.0406],\n",
            "        [-0.0346],\n",
            "        [-0.0279]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 17\n",
            "Batch 17 Output: tensor([[-0.0075],\n",
            "        [-0.0213],\n",
            "        [-0.0253],\n",
            "        [-0.0209],\n",
            "        [-0.0324],\n",
            "        [-0.0365],\n",
            "        [-0.0395],\n",
            "        [-0.0218]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 18\n",
            "Batch 18 Output: tensor([[-0.0453],\n",
            "        [-0.0468],\n",
            "        [-0.0275],\n",
            "        [-0.0480],\n",
            "        [-0.0430],\n",
            "        [-0.0384],\n",
            "        [-0.0316],\n",
            "        [-0.0542]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 19\n",
            "Batch 19 Output: tensor([[-0.0365],\n",
            "        [-0.0167],\n",
            "        [-0.0227],\n",
            "        [-0.0184],\n",
            "        [-0.0355],\n",
            "        [-0.0268],\n",
            "        [-0.0173],\n",
            "        [-0.0172]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 20\n",
            "Batch 20 Output: tensor([[-0.0288],\n",
            "        [-0.0361],\n",
            "        [-0.0354],\n",
            "        [-0.0368],\n",
            "        [-0.0271],\n",
            "        [-0.0259],\n",
            "        [-0.0410],\n",
            "        [-0.0175]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 21\n",
            "Batch 21 Output: tensor([[-0.0170],\n",
            "        [-0.0385],\n",
            "        [-0.0255],\n",
            "        [-0.0453],\n",
            "        [-0.0246],\n",
            "        [-0.0205],\n",
            "        [-0.0475],\n",
            "        [-0.0137]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 22\n",
            "Batch 22 Output: tensor([[-0.0237],\n",
            "        [-0.0334],\n",
            "        [-0.0183],\n",
            "        [-0.0585],\n",
            "        [-0.0419],\n",
            "        [-0.0044],\n",
            "        [-0.0397],\n",
            "        [-0.0217]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 23\n",
            "Batch 23 Output: tensor([[-0.0396],\n",
            "        [-0.0436],\n",
            "        [-0.0199],\n",
            "        [-0.0307],\n",
            "        [-0.0138],\n",
            "        [-0.0232],\n",
            "        [-0.0331],\n",
            "        [-0.0224]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 24\n",
            "Batch 24 Output: tensor([[-0.0297],\n",
            "        [-0.0130],\n",
            "        [-0.0399],\n",
            "        [-0.0276],\n",
            "        [-0.0398],\n",
            "        [-0.0333],\n",
            "        [-0.0042],\n",
            "        [-0.0256]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 25\n",
            "Batch 25 Output: tensor([[-0.0255],\n",
            "        [-0.0441],\n",
            "        [ 0.0128],\n",
            "        [-0.0497],\n",
            "        [-0.0345],\n",
            "        [-0.0523],\n",
            "        [-0.0296],\n",
            "        [-0.0167]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 26\n",
            "Batch 26 Output: tensor([[-0.0352],\n",
            "        [-0.0327],\n",
            "        [-0.0226],\n",
            "        [-0.0160],\n",
            "        [-0.0303],\n",
            "        [-0.0186],\n",
            "        [-0.0716],\n",
            "        [-0.0273]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 27\n",
            "Batch 27 Output: tensor([[-0.0118],\n",
            "        [-0.0353],\n",
            "        [ 0.0048],\n",
            "        [-0.0287],\n",
            "        [-0.0435],\n",
            "        [-0.0073],\n",
            "        [-0.0453],\n",
            "        [-0.0272]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 28\n",
            "Batch 28 Output: tensor([[-0.0048],\n",
            "        [-0.0349],\n",
            "        [-0.0207],\n",
            "        [-0.0201],\n",
            "        [-0.0145],\n",
            "        [-0.0342],\n",
            "        [-0.0437],\n",
            "        [-0.0254]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 29\n",
            "Batch 29 Output: tensor([[-0.0364],\n",
            "        [-0.0510],\n",
            "        [-0.0298],\n",
            "        [-0.0228],\n",
            "        [-0.0199],\n",
            "        [-0.0119],\n",
            "        [-0.0227],\n",
            "        [-0.0357]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 30\n",
            "Batch 30 Output: tensor([[-0.0243],\n",
            "        [-0.0417],\n",
            "        [-0.0692],\n",
            "        [-0.0280],\n",
            "        [-0.0298],\n",
            "        [-0.0149],\n",
            "        [-0.0300],\n",
            "        [-0.0385]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 31\n",
            "Batch 31 Output: tensor([[-0.0484],\n",
            "        [-0.0356],\n",
            "        [-0.0504],\n",
            "        [-0.0255],\n",
            "        [-0.0075],\n",
            "        [-0.0326],\n",
            "        [-0.0350],\n",
            "        [-0.0144]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 32\n",
            "Batch 32 Output: tensor([[-0.0456],\n",
            "        [-0.0384],\n",
            "        [-0.0290],\n",
            "        [-0.0263],\n",
            "        [ 0.0132],\n",
            "        [-0.0248],\n",
            "        [-0.0250],\n",
            "        [-0.0216]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 33\n",
            "Batch 33 Output: tensor([[-0.0296],\n",
            "        [-0.0050],\n",
            "        [-0.0174],\n",
            "        [-0.0185],\n",
            "        [-0.0118],\n",
            "        [-0.0389],\n",
            "        [-0.0442],\n",
            "        [-0.0293]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 34\n",
            "Batch 34 Output: tensor([[-0.0341],\n",
            "        [-0.0148],\n",
            "        [-0.0124],\n",
            "        [-0.0180],\n",
            "        [-0.0361],\n",
            "        [-0.0347],\n",
            "        [-0.0283],\n",
            "        [-0.0288]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 35\n",
            "Batch 35 Output: tensor([[-0.0362],\n",
            "        [-0.0227],\n",
            "        [-0.0281],\n",
            "        [-0.0406],\n",
            "        [-0.0293],\n",
            "        [-0.0625],\n",
            "        [-0.0369],\n",
            "        [-0.0178]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 36\n",
            "Batch 36 Output: tensor([[-0.0294],\n",
            "        [-0.0273],\n",
            "        [-0.0226],\n",
            "        [-0.0188],\n",
            "        [-0.0128],\n",
            "        [-0.0346],\n",
            "        [-0.0201],\n",
            "        [-0.0126]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 37\n",
            "Batch 37 Output: tensor([[-0.0504],\n",
            "        [-0.0134],\n",
            "        [-0.0331],\n",
            "        [-0.0264]], grad_fn=<AddmmBackward0>)\n",
            "Epoch 6/10 started\n",
            "Processing Batch 0\n",
            "Batch 0 Output: tensor([[-0.0365],\n",
            "        [-0.0093],\n",
            "        [-0.0250],\n",
            "        [-0.0227],\n",
            "        [-0.0293],\n",
            "        [-0.0331],\n",
            "        [-0.0044],\n",
            "        [-0.0288]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 1\n",
            "Batch 1 Output: tensor([[-0.0049],\n",
            "        [-0.0170],\n",
            "        [-0.0332],\n",
            "        [-0.0149],\n",
            "        [-0.0268],\n",
            "        [-0.0235],\n",
            "        [-0.0246],\n",
            "        [-0.0177]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 2\n",
            "Batch 2 Output: tensor([[-0.0126],\n",
            "        [-0.0180],\n",
            "        [-0.0437],\n",
            "        [-0.0310],\n",
            "        [-0.0048],\n",
            "        [-0.0199],\n",
            "        [-0.0410],\n",
            "        [-0.0368]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 3\n",
            "Batch 3 Output: tensor([[-0.0307],\n",
            "        [-0.0307],\n",
            "        [-0.0124],\n",
            "        [-0.0227],\n",
            "        [-0.0345],\n",
            "        [-0.0160],\n",
            "        [-0.0185],\n",
            "        [-0.0384]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 4\n",
            "Batch 4 Output: tensor([[-0.0468],\n",
            "        [-0.0197],\n",
            "        [-0.0226],\n",
            "        [-0.0327],\n",
            "        [-0.0186],\n",
            "        [-0.0226],\n",
            "        [-0.0396],\n",
            "        [-0.0224]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 5\n",
            "Batch 5 Output: tensor([[-0.0218],\n",
            "        [-0.0319],\n",
            "        [-0.0042],\n",
            "        [-0.0248],\n",
            "        [-0.0298],\n",
            "        [-0.0177],\n",
            "        [-0.0126],\n",
            "        [-0.0217]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 6\n",
            "Batch 6 Output: tensor([[-0.0281],\n",
            "        [-0.0625],\n",
            "        [-0.0261],\n",
            "        [ 0.0075],\n",
            "        [-0.0453],\n",
            "        [-0.0399],\n",
            "        [-0.0357],\n",
            "        [-0.0385]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 7\n",
            "Batch 7 Output: tensor([[-0.0200],\n",
            "        [-0.0451],\n",
            "        [-0.0408],\n",
            "        [-0.0453],\n",
            "        [-0.0139],\n",
            "        [-0.0284],\n",
            "        [-0.0186],\n",
            "        [-0.0523]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 8\n",
            "Batch 8 Output: tensor([[-0.0160],\n",
            "        [-0.0385],\n",
            "        [-0.0238],\n",
            "        [-0.0419],\n",
            "        [-0.0266],\n",
            "        [-0.0331],\n",
            "        [-0.0341],\n",
            "        [-0.0102]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 9\n",
            "Batch 9 Output: tensor([[-0.0250],\n",
            "        [-0.0430],\n",
            "        [-0.0108],\n",
            "        [-0.0396],\n",
            "        [-0.0203],\n",
            "        [ 0.0132],\n",
            "        [-0.0148],\n",
            "        [-0.0296]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 10\n",
            "Batch 10 Output: tensor([[-0.0297],\n",
            "        [-0.0243],\n",
            "        [-0.0288],\n",
            "        [-0.0299],\n",
            "        [-0.0283],\n",
            "        [-0.0228],\n",
            "        [-0.0118],\n",
            "        [-0.0312]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 11\n",
            "Batch 11 Output: tensor([[-0.0368],\n",
            "        [-0.0387],\n",
            "        [-0.0362],\n",
            "        [-0.0395],\n",
            "        [-0.0354],\n",
            "        [-0.0435],\n",
            "        [-0.0501],\n",
            "        [-0.0349]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 12\n",
            "Batch 12 Output: tensor([[-0.0346],\n",
            "        [-0.0263],\n",
            "        [-0.0383],\n",
            "        [-0.0293],\n",
            "        [-0.0245],\n",
            "        [-0.0138],\n",
            "        [-0.0146],\n",
            "        [-0.0345]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 13\n",
            "Batch 13 Output: tensor([[-0.0336],\n",
            "        [-0.0201],\n",
            "        [-0.0269],\n",
            "        [-0.0326],\n",
            "        [-0.0361],\n",
            "        [-0.0163],\n",
            "        [-0.0118],\n",
            "        [-0.0283]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 14\n",
            "Batch 14 Output: tensor([[-0.0327],\n",
            "        [-0.0346],\n",
            "        [-0.0275],\n",
            "        [ 0.0048],\n",
            "        [-0.0453],\n",
            "        [-0.0389],\n",
            "        [-0.0268],\n",
            "        [-0.0255]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 15\n",
            "Batch 15 Output: tensor([[-0.0259],\n",
            "        [-0.0266],\n",
            "        [-0.0207],\n",
            "        [-0.0294],\n",
            "        [-0.0137],\n",
            "        [-0.0397],\n",
            "        [-0.0379],\n",
            "        [-0.0361]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 16\n",
            "Batch 16 Output: tensor([[-0.0303],\n",
            "        [-0.0152],\n",
            "        [-0.0419],\n",
            "        [-0.0172],\n",
            "        [-0.0417],\n",
            "        [-0.0159],\n",
            "        [-0.0075],\n",
            "        [-0.0455]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 17\n",
            "Batch 17 Output: tensor([[-0.0436],\n",
            "        [-0.0375],\n",
            "        [-0.0128],\n",
            "        [-0.0183],\n",
            "        [-0.0365],\n",
            "        [-0.0174],\n",
            "        [-0.0475],\n",
            "        [-0.0497]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 18\n",
            "Batch 18 Output: tensor([[-0.0255],\n",
            "        [-0.0259],\n",
            "        [-0.0186],\n",
            "        [-0.0237],\n",
            "        [-0.0260],\n",
            "        [-0.0227],\n",
            "        [-0.0358],\n",
            "        [-0.0304]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 19\n",
            "Batch 19 Output: tensor([[-0.0716],\n",
            "        [-0.0342],\n",
            "        [-0.0209],\n",
            "        [-0.0073],\n",
            "        [-0.0199],\n",
            "        [-0.0175],\n",
            "        [-0.0585],\n",
            "        [-0.0226]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 20\n",
            "Batch 20 Output: tensor([[-0.0198],\n",
            "        [-0.0263],\n",
            "        [-0.0272],\n",
            "        [-0.0231],\n",
            "        [-0.0203],\n",
            "        [-0.0475],\n",
            "        [-0.0280],\n",
            "        [-0.0050]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 21\n",
            "Batch 21 Output: tensor([[-0.0273],\n",
            "        [-0.0542],\n",
            "        [-0.0167],\n",
            "        [-0.0017],\n",
            "        [-0.0364],\n",
            "        [-0.0197],\n",
            "        [-0.0355],\n",
            "        [-0.0406]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 22\n",
            "Batch 22 Output: tensor([[-0.0353],\n",
            "        [-0.0328],\n",
            "        [-0.0300],\n",
            "        [-0.0144],\n",
            "        [-0.0494],\n",
            "        [-0.0324],\n",
            "        [-0.0285],\n",
            "        [-0.0127]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 23\n",
            "Batch 23 Output: tensor([[-0.0461],\n",
            "        [-0.0232],\n",
            "        [-0.0352],\n",
            "        [-0.0350],\n",
            "        [-0.0229],\n",
            "        [-0.0287],\n",
            "        [-0.0188],\n",
            "        [-0.0276]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 24\n",
            "Batch 24 Output: tensor([[-0.0192],\n",
            "        [-0.0354],\n",
            "        [-0.0075],\n",
            "        [-0.0159],\n",
            "        [-0.0279],\n",
            "        [-0.0257],\n",
            "        [-0.0384],\n",
            "        [-0.0273]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 25\n",
            "Batch 25 Output: tensor([[-0.0480],\n",
            "        [-0.0298],\n",
            "        [-0.0315],\n",
            "        [-0.0392],\n",
            "        [-0.0252],\n",
            "        [-0.0263],\n",
            "        [-0.0510],\n",
            "        [-0.0271]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 26\n",
            "Batch 26 Output: tensor([[-0.0173],\n",
            "        [-0.0201],\n",
            "        [-0.0130],\n",
            "        [-0.0101],\n",
            "        [-0.0119],\n",
            "        [-0.0336],\n",
            "        [-0.0327],\n",
            "        [-0.0297]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 27\n",
            "Batch 27 Output: tensor([[-0.0326],\n",
            "        [-0.0320],\n",
            "        [-0.0406],\n",
            "        [-0.0342],\n",
            "        [-0.0224],\n",
            "        [-0.0180],\n",
            "        [-0.0337],\n",
            "        [-0.0288]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 28\n",
            "Batch 28 Output: tensor([[-0.0184],\n",
            "        [-0.0272],\n",
            "        [-0.0417],\n",
            "        [-0.0079],\n",
            "        [-0.0333],\n",
            "        [-0.0321],\n",
            "        [-0.0290],\n",
            "        [-0.0281]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 29\n",
            "Batch 29 Output: tensor([[ 0.0128],\n",
            "        [-0.0360],\n",
            "        [-0.0261],\n",
            "        [-0.0509],\n",
            "        [-0.0167],\n",
            "        [-0.0316],\n",
            "        [-0.0398],\n",
            "        [-0.0244]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 30\n",
            "Batch 30 Output: tensor([[-0.0334],\n",
            "        [-0.0504],\n",
            "        [-0.0188],\n",
            "        [-0.0692],\n",
            "        [-0.0316],\n",
            "        [-0.0258],\n",
            "        [-0.0568],\n",
            "        [-0.0246]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 31\n",
            "Batch 31 Output: tensor([[-0.0356],\n",
            "        [-0.0285],\n",
            "        [-0.0145],\n",
            "        [-0.0397],\n",
            "        [-0.0296],\n",
            "        [-0.0186],\n",
            "        [-0.0294],\n",
            "        [-0.0484]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 32\n",
            "Batch 32 Output: tensor([[-0.0442],\n",
            "        [-0.0260],\n",
            "        [-0.0504],\n",
            "        [-0.0183],\n",
            "        [-0.0178],\n",
            "        [-0.0290],\n",
            "        [-0.0374],\n",
            "        [-0.0340]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 33\n",
            "Batch 33 Output: tensor([[-0.0220],\n",
            "        [-0.0243],\n",
            "        [-0.0402],\n",
            "        [-0.0065],\n",
            "        [-0.0250],\n",
            "        [-0.0309],\n",
            "        [-0.0200],\n",
            "        [-0.0432]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 34\n",
            "Batch 34 Output: tensor([[-0.0134],\n",
            "        [-0.0254],\n",
            "        [-0.0418],\n",
            "        [-0.0246],\n",
            "        [-0.0216],\n",
            "        [-0.0369],\n",
            "        [-0.0456],\n",
            "        [-0.0307]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 35\n",
            "Batch 35 Output: tensor([[-0.0396],\n",
            "        [-0.0213],\n",
            "        [-0.0145],\n",
            "        [-0.0310],\n",
            "        [-0.0337],\n",
            "        [-0.0347],\n",
            "        [-0.0251],\n",
            "        [-0.0441]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 36\n",
            "Batch 36 Output: tensor([[-0.0315],\n",
            "        [-0.0307],\n",
            "        [-0.0256],\n",
            "        [-0.0255],\n",
            "        [-0.0395],\n",
            "        [-0.0412],\n",
            "        [-0.0394],\n",
            "        [-0.0264]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 37\n",
            "Batch 37 Output: tensor([[-0.0215],\n",
            "        [-0.0205],\n",
            "        [-0.0541],\n",
            "        [-0.0253]], grad_fn=<AddmmBackward0>)\n",
            "Epoch 7/10 started\n",
            "Processing Batch 0\n",
            "Batch 0 Output: tensor([[-0.0358],\n",
            "        [-0.0326],\n",
            "        [-0.0321],\n",
            "        [-0.0167],\n",
            "        [-0.0197],\n",
            "        [-0.0316],\n",
            "        [-0.0396],\n",
            "        [-0.0316]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 1\n",
            "Batch 1 Output: tensor([[-0.0192],\n",
            "        [-0.0362],\n",
            "        [-0.0275],\n",
            "        [-0.0126],\n",
            "        [-0.0149],\n",
            "        [-0.0290],\n",
            "        [-0.0307],\n",
            "        [-0.0310]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 2\n",
            "Batch 2 Output: tensor([[-0.0357],\n",
            "        [-0.0138],\n",
            "        [-0.0266],\n",
            "        [-0.0188],\n",
            "        [-0.0263],\n",
            "        [-0.0293],\n",
            "        [-0.0297],\n",
            "        [-0.0384]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 3\n",
            "Batch 3 Output: tensor([[-0.0250],\n",
            "        [-0.0347],\n",
            "        [-0.0281],\n",
            "        [-0.0383],\n",
            "        [-0.0281],\n",
            "        [-0.0304],\n",
            "        [-0.0261],\n",
            "        [-0.0285]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 4\n",
            "Batch 4 Output: tensor([[-0.0252],\n",
            "        [-0.0238],\n",
            "        [-0.0160],\n",
            "        [-0.0050],\n",
            "        [ 0.0048],\n",
            "        [-0.0127],\n",
            "        [-0.0042],\n",
            "        [-0.0402]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 5\n",
            "Batch 5 Output: tensor([[-0.0174],\n",
            "        [-0.0173],\n",
            "        [-0.0256],\n",
            "        [-0.0246],\n",
            "        [-0.0183],\n",
            "        [-0.0288],\n",
            "        [-0.0232],\n",
            "        [-0.0297]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 6\n",
            "Batch 6 Output: tensor([[-0.0432],\n",
            "        [-0.0186],\n",
            "        [-0.0048],\n",
            "        [-0.0365],\n",
            "        [-0.0128],\n",
            "        [-0.0417],\n",
            "        [-0.0455],\n",
            "        [-0.0229]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 7\n",
            "Batch 7 Output: tensor([[-0.0228],\n",
            "        [-0.0213],\n",
            "        [-0.0387],\n",
            "        [-0.0257],\n",
            "        [-0.0385],\n",
            "        [-0.0412],\n",
            "        [-0.0395],\n",
            "        [-0.0118]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 8\n",
            "Batch 8 Output: tensor([[-0.0283],\n",
            "        [-0.0315],\n",
            "        [-0.0073],\n",
            "        [-0.0327],\n",
            "        [-0.0342],\n",
            "        [-0.0205],\n",
            "        [-0.0224],\n",
            "        [-0.0585]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 9\n",
            "Batch 9 Output: tensor([[-0.0201],\n",
            "        [-0.0017],\n",
            "        [-0.0510],\n",
            "        [-0.0251],\n",
            "        [-0.0199],\n",
            "        [-0.0197],\n",
            "        [-0.0272],\n",
            "        [-0.0364]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 10\n",
            "Batch 10 Output: tensor([[-0.0468],\n",
            "        [-0.0379],\n",
            "        [-0.0279],\n",
            "        [-0.0395],\n",
            "        [-0.0418],\n",
            "        [-0.0399],\n",
            "        [-0.0075],\n",
            "        [-0.0435]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 11\n",
            "Batch 11 Output: tensor([[-0.0307],\n",
            "        [-0.0258],\n",
            "        [-0.0102],\n",
            "        [-0.0328],\n",
            "        [-0.0398],\n",
            "        [-0.0152],\n",
            "        [-0.0266],\n",
            "        [-0.0333]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 12\n",
            "Batch 12 Output: tensor([[-0.0354],\n",
            "        [ 0.0132],\n",
            "        [-0.0255],\n",
            "        [-0.0336],\n",
            "        [-0.0185],\n",
            "        [-0.0484],\n",
            "        [-0.0177],\n",
            "        [-0.0504]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 13\n",
            "Batch 13 Output: tensor([[-0.0200],\n",
            "        [-0.0118],\n",
            "        [-0.0207],\n",
            "        [-0.0334],\n",
            "        [-0.0272],\n",
            "        [-0.0406],\n",
            "        [-0.0451],\n",
            "        [-0.0475]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 14\n",
            "Batch 14 Output: tensor([[-0.0320],\n",
            "        [-0.0227],\n",
            "        [-0.0130],\n",
            "        [-0.0568],\n",
            "        [-0.0475],\n",
            "        [-0.0280],\n",
            "        [-0.0331],\n",
            "        [-0.0392]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 15\n",
            "Batch 15 Output: tensor([[-0.0342],\n",
            "        [-0.0430],\n",
            "        [-0.0340],\n",
            "        [-0.0397],\n",
            "        [-0.0260],\n",
            "        [-0.0296],\n",
            "        [-0.0327],\n",
            "        [-0.0307]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 16\n",
            "Batch 16 Output: tensor([[-0.0361],\n",
            "        [-0.0255],\n",
            "        [-0.0417],\n",
            "        [-0.0172],\n",
            "        [-0.0250],\n",
            "        [-0.0419],\n",
            "        [-0.0298],\n",
            "        [-0.0268]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 17\n",
            "Batch 17 Output: tensor([[-0.0254],\n",
            "        [-0.0288],\n",
            "        [-0.0227],\n",
            "        [-0.0368],\n",
            "        [-0.0180],\n",
            "        [-0.0183],\n",
            "        [-0.0293],\n",
            "        [-0.0246]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 18\n",
            "Batch 18 Output: tensor([[-0.0137],\n",
            "        [-0.0345],\n",
            "        [-0.0276],\n",
            "        [-0.0453],\n",
            "        [-0.0309],\n",
            "        [-0.0369],\n",
            "        [-0.0419],\n",
            "        [-0.0177]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 19\n",
            "Batch 19 Output: tensor([[-0.0108],\n",
            "        [-0.0365],\n",
            "        [ 0.0128],\n",
            "        [-0.0186],\n",
            "        [-0.0199],\n",
            "        [-0.0456],\n",
            "        [-0.0170],\n",
            "        [-0.0160]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 20\n",
            "Batch 20 Output: tensor([[-0.0243],\n",
            "        [-0.0231],\n",
            "        [-0.0288],\n",
            "        [-0.0300],\n",
            "        [-0.0299],\n",
            "        [-0.0261],\n",
            "        [-0.0453],\n",
            "        [-0.0264]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 21\n",
            "Batch 21 Output: tensor([[-0.0227],\n",
            "        [-0.0319],\n",
            "        [-0.0384],\n",
            "        [-0.0203],\n",
            "        [-0.0255],\n",
            "        [-0.0389],\n",
            "        [-0.0346],\n",
            "        [-0.0461]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 22\n",
            "Batch 22 Output: tensor([[-0.0178],\n",
            "        [-0.0215],\n",
            "        [-0.0198],\n",
            "        [-0.0259],\n",
            "        [-0.0285],\n",
            "        [-0.0245],\n",
            "        [-0.0356],\n",
            "        [-0.0273]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 23\n",
            "Batch 23 Output: tensor([[-0.0354],\n",
            "        [-0.0216],\n",
            "        [-0.0437],\n",
            "        [-0.0201],\n",
            "        [-0.0184],\n",
            "        [-0.0345],\n",
            "        [-0.0341],\n",
            "        [-0.0188]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 24\n",
            "Batch 24 Output: tensor([[-0.0287],\n",
            "        [-0.0226],\n",
            "        [-0.0124],\n",
            "        [-0.0101],\n",
            "        [-0.0224],\n",
            "        [-0.0167],\n",
            "        [-0.0327],\n",
            "        [-0.0501]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 25\n",
            "Batch 25 Output: tensor([[-0.0075],\n",
            "        [-0.0298],\n",
            "        [-0.0394],\n",
            "        [-0.0396],\n",
            "        [-0.0235],\n",
            "        [-0.0217],\n",
            "        [-0.0049],\n",
            "        [-0.0294]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 26\n",
            "Batch 26 Output: tensor([[-0.0336],\n",
            "        [-0.0294],\n",
            "        [-0.0268],\n",
            "        [-0.0269],\n",
            "        [-0.0296],\n",
            "        [-0.0385],\n",
            "        [-0.0209],\n",
            "        [-0.0436]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 27\n",
            "Batch 27 Output: tensor([[-0.0541],\n",
            "        [-0.0337],\n",
            "        [-0.0408],\n",
            "        [-0.0523],\n",
            "        [-0.0397],\n",
            "        [-0.0237],\n",
            "        [-0.0144],\n",
            "        [ 0.0075]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 28\n",
            "Batch 28 Output: tensor([[-0.0226],\n",
            "        [-0.0159],\n",
            "        [-0.0361],\n",
            "        [-0.0497],\n",
            "        [-0.0504],\n",
            "        [-0.0374],\n",
            "        [-0.0148],\n",
            "        [-0.0134]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 29\n",
            "Batch 29 Output: tensor([[-0.0244],\n",
            "        [-0.0159],\n",
            "        [-0.0326],\n",
            "        [-0.0203],\n",
            "        [-0.0079],\n",
            "        [-0.0716],\n",
            "        [-0.0163],\n",
            "        [-0.0360]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 30\n",
            "Batch 30 Output: tensor([[-0.0692],\n",
            "        [-0.0180],\n",
            "        [-0.0542],\n",
            "        [-0.0093],\n",
            "        [-0.0263],\n",
            "        [-0.0250],\n",
            "        [-0.0263],\n",
            "        [-0.0260]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 31\n",
            "Batch 31 Output: tensor([[-0.0303],\n",
            "        [-0.0044],\n",
            "        [-0.0284],\n",
            "        [-0.0186],\n",
            "        [-0.0442],\n",
            "        [-0.0283],\n",
            "        [-0.0375],\n",
            "        [-0.0253]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 32\n",
            "Batch 32 Output: tensor([[-0.0353],\n",
            "        [-0.0186],\n",
            "        [-0.0331],\n",
            "        [-0.0145],\n",
            "        [-0.0312],\n",
            "        [-0.0337],\n",
            "        [-0.0310],\n",
            "        [-0.0139]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 33\n",
            "Batch 33 Output: tensor([[-0.0352],\n",
            "        [-0.0480],\n",
            "        [-0.0350],\n",
            "        [-0.0324],\n",
            "        [-0.0453],\n",
            "        [-0.0509],\n",
            "        [-0.0332],\n",
            "        [-0.0406]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 34\n",
            "Batch 34 Output: tensor([[-0.0218],\n",
            "        [-0.0410],\n",
            "        [-0.0271],\n",
            "        [-0.0175],\n",
            "        [-0.0290],\n",
            "        [-0.0355],\n",
            "        [-0.0346],\n",
            "        [-0.0220]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 35\n",
            "Batch 35 Output: tensor([[-0.0315],\n",
            "        [-0.0396],\n",
            "        [-0.0248],\n",
            "        [-0.0146],\n",
            "        [-0.0273],\n",
            "        [-0.0065],\n",
            "        [-0.0200],\n",
            "        [-0.0119]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 36\n",
            "Batch 36 Output: tensor([[-0.0494],\n",
            "        [-0.0441],\n",
            "        [-0.0368],\n",
            "        [-0.0349],\n",
            "        [-0.0226],\n",
            "        [-0.0259],\n",
            "        [-0.0307],\n",
            "        [-0.0625]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 37\n",
            "Batch 37 Output: tensor([[-0.0145],\n",
            "        [-0.0126],\n",
            "        [-0.0243],\n",
            "        [-0.0246]], grad_fn=<AddmmBackward0>)\n",
            "Epoch 8/10 started\n",
            "Processing Batch 0\n",
            "Batch 0 Output: tensor([[-0.0285],\n",
            "        [-0.0152],\n",
            "        [-0.0290],\n",
            "        [ 0.0075],\n",
            "        [-0.0396],\n",
            "        [-0.0108],\n",
            "        [-0.0346],\n",
            "        [-0.0379]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 1\n",
            "Batch 1 Output: tensor([[-0.0283],\n",
            "        [-0.0307],\n",
            "        [-0.0227],\n",
            "        [-0.0245],\n",
            "        [-0.0361],\n",
            "        [-0.0310],\n",
            "        [-0.0213],\n",
            "        [-0.0218]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 2\n",
            "Batch 2 Output: tensor([[-0.0159],\n",
            "        [-0.0331],\n",
            "        [-0.0542],\n",
            "        [-0.0203],\n",
            "        [-0.0238],\n",
            "        [-0.0264],\n",
            "        [-0.0188],\n",
            "        [-0.0340]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 3\n",
            "Batch 3 Output: tensor([[-0.0174],\n",
            "        [-0.0341],\n",
            "        [-0.0435],\n",
            "        [-0.0216],\n",
            "        [-0.0250],\n",
            "        [-0.0160],\n",
            "        [-0.0232],\n",
            "        [-0.0226]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 4\n",
            "Batch 4 Output: tensor([[-0.0128],\n",
            "        [-0.0347],\n",
            "        [-0.0248],\n",
            "        [-0.0349],\n",
            "        [-0.0186],\n",
            "        [-0.0345],\n",
            "        [-0.0126],\n",
            "        [-0.0224]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 5\n",
            "Batch 5 Output: tensor([[-0.0254],\n",
            "        [-0.0509],\n",
            "        [-0.0178],\n",
            "        [-0.0138],\n",
            "        [-0.0229],\n",
            "        [-0.0177],\n",
            "        [-0.0197],\n",
            "        [-0.0205]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 6\n",
            "Batch 6 Output: tensor([[-0.0389],\n",
            "        [-0.0250],\n",
            "        [-0.0303],\n",
            "        [-0.0266],\n",
            "        [-0.0235],\n",
            "        [-0.0186],\n",
            "        [-0.0237],\n",
            "        [-0.0198]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 7\n",
            "Batch 7 Output: tensor([[-0.0252],\n",
            "        [-0.0504],\n",
            "        [-0.0384],\n",
            "        [-0.0257],\n",
            "        [-0.0316],\n",
            "        [-0.0368],\n",
            "        [-0.0290],\n",
            "        [-0.0307]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 8\n",
            "Batch 8 Output: tensor([[-0.0180],\n",
            "        [-0.0384],\n",
            "        [-0.0269],\n",
            "        [ 0.0128],\n",
            "        [-0.0397],\n",
            "        [-0.0201],\n",
            "        [-0.0523],\n",
            "        [-0.0075]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 9\n",
            "Batch 9 Output: tensor([[-0.0585],\n",
            "        [-0.0504],\n",
            "        [-0.0328],\n",
            "        [-0.0319],\n",
            "        [-0.0259],\n",
            "        [-0.0345],\n",
            "        [-0.0275],\n",
            "        [-0.0217]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 10\n",
            "Batch 10 Output: tensor([[-0.0398],\n",
            "        [-0.0337],\n",
            "        [-0.0365],\n",
            "        [-0.0397],\n",
            "        [-0.0284],\n",
            "        [-0.0362],\n",
            "        [-0.0119],\n",
            "        [-0.0568]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 11\n",
            "Batch 11 Output: tensor([[-0.0184],\n",
            "        [-0.0406],\n",
            "        [-0.0399],\n",
            "        [-0.0497],\n",
            "        [-0.0231],\n",
            "        [-0.0395],\n",
            "        [-0.0144],\n",
            "        [-0.0312]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 12\n",
            "Batch 12 Output: tensor([[-0.0541],\n",
            "        [-0.0299],\n",
            "        [-0.0327],\n",
            "        [-0.0297],\n",
            "        [-0.0261],\n",
            "        [-0.0173],\n",
            "        [-0.0419],\n",
            "        [-0.0280]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 13\n",
            "Batch 13 Output: tensor([[-0.0180],\n",
            "        [-0.0437],\n",
            "        [-0.0177],\n",
            "        [-0.0102],\n",
            "        [-0.0418],\n",
            "        [-0.0276],\n",
            "        [-0.0360],\n",
            "        [-0.0307]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 14\n",
            "Batch 14 Output: tensor([[-0.0353],\n",
            "        [-0.0307],\n",
            "        [-0.0432],\n",
            "        [-0.0263],\n",
            "        [-0.0716],\n",
            "        [-0.0387],\n",
            "        [-0.0197],\n",
            "        [-0.0145]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 15\n",
            "Batch 15 Output: tensor([[-0.0298],\n",
            "        [-0.0394],\n",
            "        [-0.0375],\n",
            "        [-0.0126],\n",
            "        [-0.0192],\n",
            "        [-0.0355],\n",
            "        [-0.0310],\n",
            "        [-0.0316]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 16\n",
            "Batch 16 Output: tensor([[-0.0385],\n",
            "        [-0.0167],\n",
            "        [-0.0079],\n",
            "        [-0.0453],\n",
            "        [-0.0201],\n",
            "        [-0.0170],\n",
            "        [-0.0281],\n",
            "        [-0.0358]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 17\n",
            "Batch 17 Output: tensor([[-0.0260],\n",
            "        [-0.0441],\n",
            "        [-0.0017],\n",
            "        [-0.0346],\n",
            "        [-0.0480],\n",
            "        [-0.0334],\n",
            "        [-0.0320],\n",
            "        [-0.0326]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 18\n",
            "Batch 18 Output: tensor([[-0.0406],\n",
            "        [-0.0183],\n",
            "        [-0.0309],\n",
            "        [-0.0251],\n",
            "        [-0.0075],\n",
            "        [-0.0134],\n",
            "        [-0.0365],\n",
            "        [-0.0243]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 19\n",
            "Batch 19 Output: tensor([[-0.0044],\n",
            "        [-0.0272],\n",
            "        [-0.0354],\n",
            "        [-0.0227],\n",
            "        [-0.0692],\n",
            "        [-0.0209],\n",
            "        [-0.0199],\n",
            "        [-0.0357]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 20\n",
            "Batch 20 Output: tensor([[-0.0315],\n",
            "        [-0.0065],\n",
            "        [-0.0228],\n",
            "        [-0.0288],\n",
            "        [-0.0175],\n",
            "        [-0.0392],\n",
            "        [-0.0298],\n",
            "        [-0.0436]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 21\n",
            "Batch 21 Output: tensor([[-0.0263],\n",
            "        [-0.0246],\n",
            "        [-0.0402],\n",
            "        [-0.0296],\n",
            "        [-0.0417],\n",
            "        [-0.0263],\n",
            "        [-0.0266],\n",
            "        [-0.0271]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 22\n",
            "Batch 22 Output: tensor([[-0.0268],\n",
            "        [-0.0163],\n",
            "        [-0.0361],\n",
            "        [-0.0279],\n",
            "        [-0.0342],\n",
            "        [-0.0461],\n",
            "        [-0.0246],\n",
            "        [-0.0203]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 23\n",
            "Batch 23 Output: tensor([[-0.0475],\n",
            "        [-0.0293],\n",
            "        [-0.0255],\n",
            "        [-0.0101],\n",
            "        [-0.0297],\n",
            "        [-0.0336],\n",
            "        [-0.0364],\n",
            "        [-0.0484]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 24\n",
            "Batch 24 Output: tensor([[-0.0042],\n",
            "        [-0.0093],\n",
            "        [-0.0324],\n",
            "        [-0.0273],\n",
            "        [-0.0475],\n",
            "        [-0.0337],\n",
            "        [-0.0124],\n",
            "        [-0.0259]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 25\n",
            "Batch 25 Output: tensor([[-0.0215],\n",
            "        [-0.0160],\n",
            "        [-0.0385],\n",
            "        [-0.0350],\n",
            "        [-0.0200],\n",
            "        [-0.0356],\n",
            "        [-0.0281],\n",
            "        [-0.0073]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 26\n",
            "Batch 26 Output: tensor([[-0.0186],\n",
            "        [-0.0383],\n",
            "        [-0.0226],\n",
            "        [-0.0453],\n",
            "        [-0.0220],\n",
            "        [-0.0137],\n",
            "        [-0.0139],\n",
            "        [-0.0352]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 27\n",
            "Batch 27 Output: tensor([[-0.0244],\n",
            "        [-0.0455],\n",
            "        [-0.0255],\n",
            "        [-0.0050],\n",
            "        [-0.0412],\n",
            "        [-0.0396],\n",
            "        [-0.0374],\n",
            "        [-0.0185]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 28\n",
            "Batch 28 Output: tensor([[-0.0354],\n",
            "        [-0.0226],\n",
            "        [-0.0456],\n",
            "        [-0.0255],\n",
            "        [-0.0417],\n",
            "        [-0.0419],\n",
            "        [-0.0246],\n",
            "        [-0.0167]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 29\n",
            "Batch 29 Output: tensor([[-0.0369],\n",
            "        [-0.0199],\n",
            "        [-0.0227],\n",
            "        [-0.0332],\n",
            "        [-0.0294],\n",
            "        [-0.0510],\n",
            "        [-0.0296],\n",
            "        [-0.0285]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 30\n",
            "Batch 30 Output: tensor([[-0.0327],\n",
            "        [-0.0283],\n",
            "        [-0.0336],\n",
            "        [-0.0287],\n",
            "        [-0.0331],\n",
            "        [-0.0268],\n",
            "        [-0.0625],\n",
            "        [-0.0272]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 31\n",
            "Batch 31 Output: tensor([[-0.0200],\n",
            "        [-0.0146],\n",
            "        [-0.0118],\n",
            "        [-0.0451],\n",
            "        [-0.0243],\n",
            "        [-0.0253],\n",
            "        [-0.0326],\n",
            "        [-0.0321]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 32\n",
            "Batch 32 Output: tensor([[-0.0183],\n",
            "        [-0.0172],\n",
            "        [-0.0501],\n",
            "        [-0.0315],\n",
            "        [-0.0333],\n",
            "        [-0.0408],\n",
            "        [-0.0260],\n",
            "        [-0.0368]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 33\n",
            "Batch 33 Output: tensor([[-0.0468],\n",
            "        [-0.0261],\n",
            "        [-0.0207],\n",
            "        [-0.0395],\n",
            "        [-0.0149],\n",
            "        [-0.0410],\n",
            "        [-0.0442],\n",
            "        [-0.0118]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 34\n",
            "Batch 34 Output: tensor([[-0.0294],\n",
            "        [-0.0148],\n",
            "        [-0.0049],\n",
            "        [-0.0159],\n",
            "        [-0.0396],\n",
            "        [-0.0130],\n",
            "        [-0.0250],\n",
            "        [-0.0258]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 35\n",
            "Batch 35 Output: tensor([[-0.0494],\n",
            "        [-0.0288],\n",
            "        [-0.0188],\n",
            "        [-0.0256],\n",
            "        [ 0.0132],\n",
            "        [-0.0127],\n",
            "        [-0.0186],\n",
            "        [-0.0327]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 36\n",
            "Batch 36 Output: tensor([[-0.0145],\n",
            "        [ 0.0048],\n",
            "        [-0.0288],\n",
            "        [-0.0430],\n",
            "        [-0.0273],\n",
            "        [-0.0453],\n",
            "        [-0.0224],\n",
            "        [-0.0300]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 37\n",
            "Batch 37 Output: tensor([[-0.0342],\n",
            "        [-0.0048],\n",
            "        [-0.0304],\n",
            "        [-0.0293]], grad_fn=<AddmmBackward0>)\n",
            "Epoch 9/10 started\n",
            "Processing Batch 0\n",
            "Batch 0 Output: tensor([[-0.0585],\n",
            "        [-0.0235],\n",
            "        [-0.0244],\n",
            "        [-0.0347],\n",
            "        [-0.0199],\n",
            "        [-0.0124],\n",
            "        [-0.0065],\n",
            "        [-0.0246]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 1\n",
            "Batch 1 Output: tensor([[-0.0203],\n",
            "        [-0.0316],\n",
            "        [-0.0260],\n",
            "        [-0.0232],\n",
            "        [-0.0128],\n",
            "        [-0.0342],\n",
            "        [-0.0456],\n",
            "        [-0.0186]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 2\n",
            "Batch 2 Output: tensor([[-0.0273],\n",
            "        [-0.0541],\n",
            "        [-0.0203],\n",
            "        [-0.0227],\n",
            "        [-0.0307],\n",
            "        [-0.0365],\n",
            "        [-0.0374],\n",
            "        [-0.0224]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 3\n",
            "Batch 3 Output: tensor([[-0.0255],\n",
            "        [-0.0293],\n",
            "        [-0.0127],\n",
            "        [-0.0337],\n",
            "        [-0.0398],\n",
            "        [-0.0144],\n",
            "        [-0.0263],\n",
            "        [-0.0320]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 4\n",
            "Batch 4 Output: tensor([[-0.0148],\n",
            "        [-0.0349],\n",
            "        [-0.0307],\n",
            "        [ 0.0132],\n",
            "        [-0.0159],\n",
            "        [-0.0298],\n",
            "        [-0.0442],\n",
            "        [-0.0188]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 5\n",
            "Batch 5 Output: tensor([[-0.0257],\n",
            "        [ 0.0048],\n",
            "        [-0.0269],\n",
            "        [-0.0258],\n",
            "        [-0.0287],\n",
            "        [-0.0290],\n",
            "        [-0.0453],\n",
            "        [-0.0296]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 6\n",
            "Batch 6 Output: tensor([[-0.0300],\n",
            "        [-0.0281],\n",
            "        [-0.0288],\n",
            "        [-0.0345],\n",
            "        [ 0.0128],\n",
            "        [-0.0172],\n",
            "        [-0.0326],\n",
            "        [-0.0453]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 7\n",
            "Batch 7 Output: tensor([[-0.0296],\n",
            "        [-0.0284],\n",
            "        [-0.0216],\n",
            "        [-0.0276],\n",
            "        [-0.0264],\n",
            "        [-0.0504],\n",
            "        [-0.0173],\n",
            "        [-0.0260]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 8\n",
            "Batch 8 Output: tensor([[-0.0261],\n",
            "        [-0.0197],\n",
            "        [-0.0285],\n",
            "        [-0.0220],\n",
            "        [-0.0368],\n",
            "        [-0.0170],\n",
            "        [-0.0160],\n",
            "        [-0.0185]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 9\n",
            "Batch 9 Output: tensor([[-0.0075],\n",
            "        [-0.0259],\n",
            "        [-0.0625],\n",
            "        [-0.0303],\n",
            "        [-0.0396],\n",
            "        [-0.0354],\n",
            "        [-0.0126],\n",
            "        [-0.0263]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 10\n",
            "Batch 10 Output: tensor([[-0.0118],\n",
            "        [-0.0224],\n",
            "        [-0.0231],\n",
            "        [-0.0309],\n",
            "        [-0.0213],\n",
            "        [-0.0342],\n",
            "        [-0.0253],\n",
            "        [-0.0285]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 11\n",
            "Batch 11 Output: tensor([[-0.0461],\n",
            "        [-0.0243],\n",
            "        [-0.0266],\n",
            "        [-0.0199],\n",
            "        [-0.0093],\n",
            "        [-0.0294],\n",
            "        [-0.0271],\n",
            "        [-0.0432]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 12\n",
            "Batch 12 Output: tensor([[-0.0266],\n",
            "        [-0.0412],\n",
            "        [-0.0375],\n",
            "        [-0.0337],\n",
            "        [-0.0345],\n",
            "        [-0.0130],\n",
            "        [-0.0406],\n",
            "        [-0.0226]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 13\n",
            "Batch 13 Output: tensor([[-0.0352],\n",
            "        [-0.0259],\n",
            "        [-0.0358],\n",
            "        [-0.0299],\n",
            "        [-0.0251],\n",
            "        [-0.0200],\n",
            "        [-0.0419],\n",
            "        [-0.0341]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 14\n",
            "Batch 14 Output: tensor([[-0.0198],\n",
            "        [-0.0256],\n",
            "        [-0.0275],\n",
            "        [-0.0177],\n",
            "        [-0.0542],\n",
            "        [-0.0178],\n",
            "        [-0.0510],\n",
            "        [-0.0312]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 15\n",
            "Batch 15 Output: tensor([[-0.0435],\n",
            "        [-0.0568],\n",
            "        [-0.0137],\n",
            "        [-0.0350],\n",
            "        [-0.0326],\n",
            "        [-0.0134],\n",
            "        [-0.0504],\n",
            "        [-0.0149]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 16\n",
            "Batch 16 Output: tensor([[-0.0430],\n",
            "        [-0.0331],\n",
            "        [-0.0118],\n",
            "        [-0.0397],\n",
            "        [-0.0288],\n",
            "        [-0.0394],\n",
            "        [-0.0159],\n",
            "        [-0.0042]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 17\n",
            "Batch 17 Output: tensor([[-0.0283],\n",
            "        [-0.0252],\n",
            "        [-0.0319],\n",
            "        [-0.0336],\n",
            "        [-0.0327],\n",
            "        [-0.0261],\n",
            "        [-0.0385],\n",
            "        [-0.0180]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 18\n",
            "Batch 18 Output: tensor([[-0.0243],\n",
            "        [-0.0049],\n",
            "        [-0.0410],\n",
            "        [-0.0395],\n",
            "        [-0.0248],\n",
            "        [-0.0716],\n",
            "        [-0.0361],\n",
            "        [-0.0324]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 19\n",
            "Batch 19 Output: tensor([[-0.0268],\n",
            "        [-0.0102],\n",
            "        [-0.0079],\n",
            "        [-0.0215],\n",
            "        [-0.0294],\n",
            "        [-0.0396],\n",
            "        [-0.0229],\n",
            "        [-0.0226]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 20\n",
            "Batch 20 Output: tensor([[-0.0280],\n",
            "        [-0.0361],\n",
            "        [-0.0245],\n",
            "        [-0.0362],\n",
            "        [-0.0207],\n",
            "        [-0.0315],\n",
            "        [-0.0201],\n",
            "        [-0.0250]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 21\n",
            "Batch 21 Output: tensor([[-0.0333],\n",
            "        [-0.0417],\n",
            "        [-0.0177],\n",
            "        [-0.0209],\n",
            "        [-0.0327],\n",
            "        [-0.0369],\n",
            "        [-0.0050],\n",
            "        [-0.0174]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 22\n",
            "Batch 22 Output: tensor([[-0.0255],\n",
            "        [-0.0328],\n",
            "        [-0.0332],\n",
            "        [-0.0441],\n",
            "        [-0.0281],\n",
            "        [-0.0226],\n",
            "        [-0.0138],\n",
            "        [-0.0395]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 23\n",
            "Batch 23 Output: tensor([[-0.0126],\n",
            "        [-0.0268],\n",
            "        [-0.0139],\n",
            "        [-0.0383],\n",
            "        [-0.0163],\n",
            "        [-0.0501],\n",
            "        [-0.0228],\n",
            "        [-0.0288]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 24\n",
            "Batch 24 Output: tensor([[-0.0340],\n",
            "        [-0.0075],\n",
            "        [-0.0167],\n",
            "        [-0.0254],\n",
            "        [-0.0334],\n",
            "        [-0.0475],\n",
            "        [-0.0494],\n",
            "        [-0.0315]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 25\n",
            "Batch 25 Output: tensor([[-0.0392],\n",
            "        [-0.0044],\n",
            "        [-0.0475],\n",
            "        [-0.0497],\n",
            "        [-0.0408],\n",
            "        [-0.0167],\n",
            "        [-0.0188],\n",
            "        [-0.0255]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 26\n",
            "Batch 26 Output: tensor([[-0.0146],\n",
            "        [-0.0283],\n",
            "        [-0.0017],\n",
            "        [-0.0384],\n",
            "        [-0.0293],\n",
            "        [-0.0180],\n",
            "        [-0.0304],\n",
            "        [-0.0184]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 27\n",
            "Batch 27 Output: tensor([[-0.0298],\n",
            "        [-0.0250],\n",
            "        [-0.0237],\n",
            "        [-0.0331],\n",
            "        [-0.0402],\n",
            "        [-0.0175],\n",
            "        [-0.0316],\n",
            "        [-0.0355]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 28\n",
            "Batch 28 Output: tensor([[-0.0183],\n",
            "        [-0.0336],\n",
            "        [-0.0119],\n",
            "        [-0.0387],\n",
            "        [-0.0145],\n",
            "        [-0.0353],\n",
            "        [-0.0321],\n",
            "        [-0.0217]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 29\n",
            "Batch 29 Output: tensor([[-0.0379],\n",
            "        [-0.0101],\n",
            "        [ 0.0075],\n",
            "        [-0.0272],\n",
            "        [-0.0183],\n",
            "        [-0.0389],\n",
            "        [-0.0201],\n",
            "        [-0.0468]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 30\n",
            "Batch 30 Output: tensor([[-0.0073],\n",
            "        [-0.0152],\n",
            "        [-0.0192],\n",
            "        [-0.0417],\n",
            "        [-0.0273],\n",
            "        [-0.0297],\n",
            "        [-0.0238],\n",
            "        [-0.0357]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 31\n",
            "Batch 31 Output: tensor([[-0.0307],\n",
            "        [-0.0385],\n",
            "        [-0.0399],\n",
            "        [-0.0523],\n",
            "        [-0.0360],\n",
            "        [-0.0310],\n",
            "        [-0.0346],\n",
            "        [-0.0310]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 32\n",
            "Batch 32 Output: tensor([[-0.0272],\n",
            "        [-0.0346],\n",
            "        [-0.0406],\n",
            "        [-0.0186],\n",
            "        [-0.0453],\n",
            "        [-0.0356],\n",
            "        [-0.0364],\n",
            "        [-0.0327]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 33\n",
            "Batch 33 Output: tensor([[-0.0455],\n",
            "        [-0.0692],\n",
            "        [-0.0218],\n",
            "        [-0.0290],\n",
            "        [-0.0227],\n",
            "        [-0.0263],\n",
            "        [-0.0484],\n",
            "        [-0.0418]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 34\n",
            "Batch 34 Output: tensor([[-0.0368],\n",
            "        [-0.0480],\n",
            "        [-0.0186],\n",
            "        [-0.0436],\n",
            "        [-0.0197],\n",
            "        [-0.0384],\n",
            "        [-0.0048],\n",
            "        [-0.0227]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 35\n",
            "Batch 35 Output: tensor([[-0.0307],\n",
            "        [-0.0186],\n",
            "        [-0.0397],\n",
            "        [-0.0108],\n",
            "        [-0.0246],\n",
            "        [-0.0354],\n",
            "        [-0.0437],\n",
            "        [-0.0365]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 36\n",
            "Batch 36 Output: tensor([[-0.0205],\n",
            "        [-0.0451],\n",
            "        [-0.0419],\n",
            "        [-0.0509],\n",
            "        [-0.0279],\n",
            "        [-0.0250],\n",
            "        [-0.0200],\n",
            "        [-0.0160]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 37\n",
            "Batch 37 Output: tensor([[-0.0246],\n",
            "        [-0.0297],\n",
            "        [-0.0396],\n",
            "        [-0.0145]], grad_fn=<AddmmBackward0>)\n",
            "Epoch 10/10 started\n",
            "Processing Batch 0\n",
            "Batch 0 Output: tensor([[-0.0128],\n",
            "        [-0.0163],\n",
            "        [-0.0324],\n",
            "        [-0.0716],\n",
            "        [-0.0130],\n",
            "        [-0.0347],\n",
            "        [-0.0283],\n",
            "        [-0.0408]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 1\n",
            "Batch 1 Output: tensor([[-0.0279],\n",
            "        [-0.0385],\n",
            "        [-0.0368],\n",
            "        [-0.0170],\n",
            "        [-0.0362],\n",
            "        [-0.0475],\n",
            "        [-0.0173],\n",
            "        [-0.0389]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 2\n",
            "Batch 2 Output: tensor([[-0.0397],\n",
            "        [-0.0183],\n",
            "        [-0.0350],\n",
            "        [ 0.0128],\n",
            "        [-0.0379],\n",
            "        [-0.0417],\n",
            "        [-0.0625],\n",
            "        [-0.0333]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 3\n",
            "Batch 3 Output: tensor([[-0.0384],\n",
            "        [-0.0369],\n",
            "        [-0.0145],\n",
            "        [-0.0209],\n",
            "        [-0.0455],\n",
            "        [-0.0334],\n",
            "        [-0.0260],\n",
            "        [-0.0316]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 4\n",
            "Batch 4 Output: tensor([[-0.0269],\n",
            "        [-0.0384],\n",
            "        [-0.0497],\n",
            "        [-0.0172],\n",
            "        [-0.0354],\n",
            "        [-0.0387],\n",
            "        [-0.0201],\n",
            "        [-0.0293]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 5\n",
            "Batch 5 Output: tensor([[-0.0073],\n",
            "        [-0.0304],\n",
            "        [-0.0296],\n",
            "        [-0.0244],\n",
            "        [-0.0417],\n",
            "        [-0.0243],\n",
            "        [-0.0126],\n",
            "        [-0.0568]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 6\n",
            "Batch 6 Output: tensor([[-0.0138],\n",
            "        [-0.0542],\n",
            "        [-0.0298],\n",
            "        [-0.0287],\n",
            "        [-0.0227],\n",
            "        [-0.0183],\n",
            "        [-0.0197],\n",
            "        [-0.0288]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 7\n",
            "Batch 7 Output: tensor([[-0.0118],\n",
            "        [-0.0354],\n",
            "        [-0.0259],\n",
            "        [-0.0337],\n",
            "        [-0.0042],\n",
            "        [-0.0394],\n",
            "        [-0.0307],\n",
            "        [-0.0246]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 8\n",
            "Batch 8 Output: tensor([[-0.0050],\n",
            "        [-0.0044],\n",
            "        [-0.0137],\n",
            "        [-0.0048],\n",
            "        [-0.0188],\n",
            "        [-0.0327],\n",
            "        [-0.0453],\n",
            "        [-0.0406]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 9\n",
            "Batch 9 Output: tensor([[-0.0430],\n",
            "        [-0.0399],\n",
            "        [-0.0288],\n",
            "        [-0.0297],\n",
            "        [-0.0283],\n",
            "        [-0.0272],\n",
            "        [-0.0273],\n",
            "        [-0.0216]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 10\n",
            "Batch 10 Output: tensor([[-0.0264],\n",
            "        [-0.0175],\n",
            "        [-0.0453],\n",
            "        [-0.0303],\n",
            "        [-0.0167],\n",
            "        [-0.0160],\n",
            "        [-0.0501],\n",
            "        [-0.0361]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 11\n",
            "Batch 11 Output: tensor([[-0.0177],\n",
            "        [-0.0312],\n",
            "        [-0.0227],\n",
            "        [-0.0336],\n",
            "        [-0.0199],\n",
            "        [-0.0224],\n",
            "        [-0.0119],\n",
            "        [-0.0326]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 12\n",
            "Batch 12 Output: tensor([[-0.0252],\n",
            "        [-0.0231],\n",
            "        [-0.0356],\n",
            "        [-0.0321],\n",
            "        [-0.0079],\n",
            "        [-0.0383],\n",
            "        [-0.0337],\n",
            "        [-0.0213]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 13\n",
            "Batch 13 Output: tensor([[-0.0200],\n",
            "        [-0.0232],\n",
            "        [-0.0437],\n",
            "        [-0.0397],\n",
            "        [-0.0134],\n",
            "        [-0.0102],\n",
            "        [-0.0442],\n",
            "        [-0.0263]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 14\n",
            "Batch 14 Output: tensor([[-0.0341],\n",
            "        [-0.0177],\n",
            "        [-0.0316],\n",
            "        [-0.0198],\n",
            "        [-0.0332],\n",
            "        [-0.0205],\n",
            "        [-0.0093],\n",
            "        [-0.0432]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 15\n",
            "Batch 15 Output: tensor([[-0.0124],\n",
            "        [-0.0017],\n",
            "        [-0.0315],\n",
            "        [-0.0275],\n",
            "        [-0.0218],\n",
            "        [-0.0224],\n",
            "        [-0.0192],\n",
            "        [-0.0200]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 16\n",
            "Batch 16 Output: tensor([[-0.0475],\n",
            "        [-0.0144],\n",
            "        [-0.0290],\n",
            "        [-0.0294],\n",
            "        [-0.0410],\n",
            "        [-0.0159],\n",
            "        [-0.0203],\n",
            "        [ 0.0132]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 17\n",
            "Batch 17 Output: tensor([[-0.0146],\n",
            "        [-0.0243],\n",
            "        [-0.0126],\n",
            "        [-0.0342],\n",
            "        [-0.0309],\n",
            "        [-0.0268],\n",
            "        [-0.0127],\n",
            "        [-0.0342]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 18\n",
            "Batch 18 Output: tensor([[-0.0250],\n",
            "        [-0.0352],\n",
            "        [-0.0207],\n",
            "        [-0.0185],\n",
            "        [ 0.0075],\n",
            "        [-0.0345],\n",
            "        [-0.0441],\n",
            "        [-0.0174]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 19\n",
            "Batch 19 Output: tensor([[-0.0368],\n",
            "        [-0.0257],\n",
            "        [-0.0461],\n",
            "        [-0.0188],\n",
            "        [-0.0374],\n",
            "        [-0.0075],\n",
            "        [-0.0217],\n",
            "        [-0.0319]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 20\n",
            "Batch 20 Output: tensor([[-0.0307],\n",
            "        [-0.0336],\n",
            "        [-0.0435],\n",
            "        [-0.0358],\n",
            "        [-0.0148],\n",
            "        [-0.0331],\n",
            "        [-0.0396],\n",
            "        [-0.0159]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 21\n",
            "Batch 21 Output: tensor([[-0.0307],\n",
            "        [-0.0310],\n",
            "        [-0.0504],\n",
            "        [-0.0145],\n",
            "        [-0.0294],\n",
            "        [-0.0296],\n",
            "        [-0.0398],\n",
            "        [-0.0361]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 22\n",
            "Batch 22 Output: tensor([[-0.0260],\n",
            "        [-0.0250],\n",
            "        [-0.0406],\n",
            "        [-0.0152],\n",
            "        [-0.0251],\n",
            "        [-0.0310],\n",
            "        [-0.0328],\n",
            "        [-0.0412]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 23\n",
            "Batch 23 Output: tensor([[-0.0180],\n",
            "        [-0.0326],\n",
            "        [-0.0178],\n",
            "        [-0.0280],\n",
            "        [-0.0504],\n",
            "        [-0.0285],\n",
            "        [-0.0299],\n",
            "        [-0.0402]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 24\n",
            "Batch 24 Output: tensor([[-0.0419],\n",
            "        [-0.0396],\n",
            "        [-0.0288],\n",
            "        [-0.0268],\n",
            "        [-0.0300],\n",
            "        [-0.0226],\n",
            "        [-0.0237],\n",
            "        [-0.0285]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 25\n",
            "Batch 25 Output: tensor([[-0.0226],\n",
            "        [-0.0246],\n",
            "        [-0.0261],\n",
            "        [-0.0256],\n",
            "        [-0.0315],\n",
            "        [-0.0049],\n",
            "        [-0.0509],\n",
            "        [-0.0167]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 26\n",
            "Batch 26 Output: tensor([[-0.0392],\n",
            "        [ 0.0048],\n",
            "        [-0.0585],\n",
            "        [-0.0365],\n",
            "        [-0.0365],\n",
            "        [-0.0494],\n",
            "        [-0.0246],\n",
            "        [-0.0118]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 27\n",
            "Batch 27 Output: tensor([[-0.0186],\n",
            "        [-0.0254],\n",
            "        [-0.0250],\n",
            "        [-0.0215],\n",
            "        [-0.0346],\n",
            "        [-0.0307],\n",
            "        [-0.0108],\n",
            "        [-0.0227]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 28\n",
            "Batch 28 Output: tensor([[-0.0101],\n",
            "        [-0.0353],\n",
            "        [-0.0186],\n",
            "        [-0.0355],\n",
            "        [-0.0297],\n",
            "        [-0.0281],\n",
            "        [-0.0261],\n",
            "        [-0.0253]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 29\n",
            "Batch 29 Output: tensor([[-0.0220],\n",
            "        [-0.0266],\n",
            "        [-0.0075],\n",
            "        [-0.0184],\n",
            "        [-0.0180],\n",
            "        [-0.0453],\n",
            "        [-0.0523],\n",
            "        [-0.0349]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 30\n",
            "Batch 30 Output: tensor([[-0.0395],\n",
            "        [-0.0484],\n",
            "        [-0.0364],\n",
            "        [-0.0375],\n",
            "        [-0.0263],\n",
            "        [-0.0281],\n",
            "        [-0.0149],\n",
            "        [-0.0395]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 31\n",
            "Batch 31 Output: tensor([[-0.0293],\n",
            "        [-0.0228],\n",
            "        [-0.0480],\n",
            "        [-0.0065],\n",
            "        [-0.0255],\n",
            "        [-0.0331],\n",
            "        [-0.0186],\n",
            "        [-0.0320]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 32\n",
            "Batch 32 Output: tensor([[-0.0258],\n",
            "        [-0.0360],\n",
            "        [-0.0385],\n",
            "        [-0.0235],\n",
            "        [-0.0345],\n",
            "        [-0.0357],\n",
            "        [-0.0197],\n",
            "        [-0.0272]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 33\n",
            "Batch 33 Output: tensor([[-0.0396],\n",
            "        [-0.0290],\n",
            "        [-0.0266],\n",
            "        [-0.0456],\n",
            "        [-0.0327],\n",
            "        [-0.0203],\n",
            "        [-0.0186],\n",
            "        [-0.0229]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 34\n",
            "Batch 34 Output: tensor([[-0.0541],\n",
            "        [-0.0160],\n",
            "        [-0.0327],\n",
            "        [-0.0139],\n",
            "        [-0.0692],\n",
            "        [-0.0259],\n",
            "        [-0.0245],\n",
            "        [-0.0346]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 35\n",
            "Batch 35 Output: tensor([[-0.0248],\n",
            "        [-0.0298],\n",
            "        [-0.0340],\n",
            "        [-0.0468],\n",
            "        [-0.0436],\n",
            "        [-0.0284],\n",
            "        [-0.0419],\n",
            "        [-0.0451]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 36\n",
            "Batch 36 Output: tensor([[-0.0273],\n",
            "        [-0.0238],\n",
            "        [-0.0418],\n",
            "        [-0.0263],\n",
            "        [-0.0510],\n",
            "        [-0.0276],\n",
            "        [-0.0199],\n",
            "        [-0.0271]], grad_fn=<AddmmBackward0>)\n",
            "Processing Batch 37\n",
            "Batch 37 Output: tensor([[-0.0255],\n",
            "        [-0.0201],\n",
            "        [-0.0226],\n",
            "        [-0.0255]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example of processing batch output\n",
        "def analyze_batch_outputs(batch_outputs):\n",
        "    min_vals = []\n",
        "    max_vals = []\n",
        "    mean_vals = []\n",
        "\n",
        "    for batch_num, output in enumerate(batch_outputs):\n",
        "        min_vals.append(output.min().item())\n",
        "        max_vals.append(output.max().item())\n",
        "        mean_vals.append(output.mean().item())\n",
        "\n",
        "        print(f\"Batch {batch_num}: Min={min_vals[-1]:.4f}, Max={max_vals[-1]:.4f}, Mean={mean_vals[-1]:.4f}\")\n",
        "\n",
        "    return min_vals, max_vals, mean_vals\n",
        "\n",
        "# Simulating the output tensors from training\n",
        "batch_outputs = [\n",
        "    torch.tensor([[-0.0392], [-0.0361], [-0.0203], [-0.0585], [-0.0268], [-0.0300], [-0.0226], [-0.0692]]),\n",
        "    torch.tensor([[-0.0159], [-0.0336], [-0.0126], [-0.0246], [-0.0178], [-0.0321], [-0.0264], [-0.0364]]),\n",
        "    torch.tensor([[-0.0288], [-0.0412], [-0.0285], [-0.0144], [-0.0310], [-0.0303], [-0.0384], [-0.0345]]),\n",
        "    torch.tensor([[-0.0268], [-0.0255], [-0.0227], [-0.0198], [-0.0310], [-0.0541], [-0.0272], [-0.0284]])\n",
        "]\n",
        "\n",
        "# Analyze the outputs\n",
        "min_vals, max_vals, mean_vals = analyze_batch_outputs(batch_outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btBqNpjWqslb",
        "outputId": "e1a85e59-394d-476f-e447-ca4cc4fb0faa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: Min=-0.0692, Max=-0.0203, Mean=-0.0378\n",
            "Batch 1: Min=-0.0364, Max=-0.0126, Mean=-0.0249\n",
            "Batch 2: Min=-0.0412, Max=-0.0144, Mean=-0.0309\n",
            "Batch 3: Min=-0.0541, Max=-0.0198, Mean=-0.0294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN CODE**"
      ],
      "metadata": {
        "id": "gAW0fdO5wehf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DATASET UPLOADING**"
      ],
      "metadata": {
        "id": "eeuh7UUx91wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack\"\n",
        "\n",
        "folders = [\"train_img\", \"train_lab\", \"test_img\", \"test_lab\"]\n",
        "for folder in folders:\n",
        "    folder_path = os.path.join(dataset_path, folder)\n",
        "    print(f\"{folder}: {len(os.listdir(folder_path))} files found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTVDMtgoq9d0",
        "outputId": "54724dc5-b2b2-4e74-acf0-d15bd22d1008"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "train_img: 300 files found\n",
            "train_lab: 300 files found\n",
            "test_img: 237 files found\n",
            "test_lab: 237 files found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TRAINING THE MODEL**"
      ],
      "metadata": {
        "id": "7_Wv9V1p9li7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Paths\n",
        "data_dir = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack\"\n",
        "train_img_dir = os.path.join(data_dir, \"train_img\")\n",
        "train_lab_dir = os.path.join(data_dir, \"train_lab\")\n",
        "test_img_dir = os.path.join(data_dir, \"test_img\")\n",
        "test_lab_dir = os.path.join(data_dir, \"test_lab\")\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 8\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "image_size = (256, 256)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Custom Dataset\n",
        "class CrackDataset(Dataset):\n",
        "    def __init__(self, img_dir, lab_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.lab_dir = lab_dir\n",
        "        self.img_list = sorted(os.listdir(img_dir))\n",
        "        self.lab_list = sorted(os.listdir(lab_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_list[idx])\n",
        "        lab_path = os.path.join(self.lab_dir, self.lab_list[idx])\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        label = Image.open(lab_path).convert(\"L\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "            label = self.transform(label)\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load Dataset\n",
        "dataset = CrackDataset(train_img_dir, train_lab_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# U-Net Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Encoder (Downsampling)\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # Reduces size to 128x128\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # Reduces size to 64x64\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)  # Reduces size to 32x32\n",
        "\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)  # Reduces size to 16x16\n",
        "\n",
        "        # Bottleneck\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder (Upsampling)\n",
        "        self.up6 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)  # Upsample to 32x32\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.up7 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)  # Upsample to 64x64\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.up8 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)  # Upsample to 128x128\n",
        "        self.conv8 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.up9 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)  # Upsample to 256x256\n",
        "        self.conv9 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.final = nn.Conv2d(64, 1, kernel_size=1)  # Output 1 channel\n",
        "\n",
        "    def forward(self, x):\n",
        "        c1 = self.conv1(x)\n",
        "        p1 = self.pool1(c1)\n",
        "\n",
        "        c2 = self.conv2(p1)\n",
        "        p2 = self.pool2(c2)\n",
        "\n",
        "        c3 = self.conv3(p2)\n",
        "        p3 = self.pool3(c3)\n",
        "\n",
        "        c4 = self.conv4(p3)\n",
        "        p4 = self.pool4(c4)\n",
        "\n",
        "        c5 = self.conv5(p4)\n",
        "\n",
        "        up_6 = self.up6(c5)\n",
        "        merge6 = torch.cat([up_6, c4], dim=1)\n",
        "        c6 = self.conv6(merge6)\n",
        "\n",
        "        up_7 = self.up7(c6)\n",
        "        merge7 = torch.cat([up_7, c3], dim=1)\n",
        "        c7 = self.conv7(merge7)\n",
        "\n",
        "        up_8 = self.up8(c7)\n",
        "        merge8 = torch.cat([up_8, c2], dim=1)\n",
        "        c8 = self.conv8(merge8)\n",
        "\n",
        "        up_9 = self.up9(c8)\n",
        "        merge9 = torch.cat([up_9, c1], dim=1)\n",
        "        c9 = self.conv9(merge9)\n",
        "\n",
        "        output = torch.sigmoid(self.final(c9))  # Apply sigmoid to normalize output (0 to 1)\n",
        "        return output\n",
        "\n",
        "\n",
        "model = UNet().to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(dataloader):.4f}\")\n",
        "\n",
        "# Save Model\n",
        "torch.save(model.state_dict(), \"crack_segmentation.pth\")\n",
        "print(\"Training complete and model saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezdYelcmwy13",
        "outputId": "e3c2dc4b-c29b-4888-e366-3bb9d9b83447"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 0.1386\n",
            "Epoch [2/20], Loss: 0.0628\n",
            "Epoch [3/20], Loss: 0.0595\n",
            "Epoch [4/20], Loss: 0.0517\n",
            "Epoch [5/20], Loss: 0.0517\n",
            "Epoch [6/20], Loss: 0.0507\n",
            "Epoch [7/20], Loss: 0.0499\n",
            "Epoch [8/20], Loss: 0.0492\n",
            "Epoch [9/20], Loss: 0.0500\n",
            "Epoch [10/20], Loss: 0.0474\n",
            "Epoch [11/20], Loss: 0.0478\n",
            "Epoch [12/20], Loss: 0.0486\n",
            "Epoch [13/20], Loss: 0.0509\n",
            "Epoch [14/20], Loss: 0.0453\n",
            "Epoch [15/20], Loss: 0.0452\n",
            "Epoch [16/20], Loss: 0.0467\n",
            "Epoch [17/20], Loss: 0.0477\n",
            "Epoch [18/20], Loss: 0.0453\n",
            "Epoch [19/20], Loss: 0.0444\n",
            "Epoch [20/20], Loss: 0.0444\n",
            "Training complete and model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TESTING THE MODEL**"
      ],
      "metadata": {
        "id": "yft47M68992n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define paths\n",
        "data_dir = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack\"\n",
        "test_img_dir = os.path.join(data_dir, \"test_img\")\n",
        "pred_output_dir = os.path.join(data_dir, \"predicted_output\")\n",
        "os.makedirs(pred_output_dir, exist_ok=True)\n",
        "\n",
        "# Load trained model\n",
        "model = UNet()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load(\"/content/crack_segmentation.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define image transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Process test images\n",
        "for img_name in os.listdir(test_img_dir):\n",
        "    img_path = os.path.join(test_img_dir, img_name)\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        output_image = output.squeeze().cpu().numpy()\n",
        "\n",
        "    # Normalize output to 0-255\n",
        "    output_image = (output_image - output_image.min()) / (output_image.max() - output_image.min())\n",
        "    output_image = (output_image * 255).astype(np.uint8)\n",
        "\n",
        "    # Save predicted image\n",
        "    output_img_path = os.path.join(pred_output_dir, img_name)\n",
        "    Image.fromarray(output_image).save(output_img_path)\n",
        "    print(f\"Saved: {output_img_path}\")\n",
        "\n",
        "print(\"Processing complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG-m1NXH2oxq",
        "outputId": "949befb9-460a-4f16-fc1d-451f167564b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6522-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6536-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6538-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6544-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6516-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6542-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6516-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6516-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6526-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6538-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6542-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6542-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6522-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6537-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6526-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6537-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6533-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6516-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6544-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6542-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6526-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6544-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6536-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6544-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6536-7.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6528-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6536-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6537-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6536-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6542-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6536-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6542-7.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6526-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6528-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6537-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6537-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11301-6.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-8.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6472-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11308-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11301-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-7.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6472-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11309-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6469-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6469-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11301-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_5778-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6514-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6472-7.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG6-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11305-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_5776-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-6.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6514-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11302-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-9.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6514-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11301-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11301-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6469-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11304.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG24-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6472-8.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6472-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_5776-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6469-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6469-7.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG109-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6472-6.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6472-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6469-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG24-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG24-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11305-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11308-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6472-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11302-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11300.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG_6469-6.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11301-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/IMG6-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11289-7.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-20.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-17.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11289-6.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-23.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11294.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-19.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-13.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-11.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11286-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11271-6.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11289-9.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-10.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11289-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-14.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11295.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11276-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-18.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11289-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11289-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-15.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11286-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11284-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-22.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11289-8.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11286-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11271-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11271-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11271-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-21.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-16.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11289-11.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11275-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11289-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11275-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11275-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11289-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11271-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11296-12.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11289-10.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11269-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11248-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11247-7.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11267-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11247-8.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11247-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11247-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11249-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11266-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11247-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11266-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11267-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11269-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11269-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11249-8.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11258-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11266-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11249-7.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11269-6.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11248-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11249-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11249-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11269-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11267-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11249-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11270-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11248-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11247-9.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11247-6.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11266-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11267-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11266-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11258-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11247-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11267-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11269-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11271-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11249-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11270-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11247-12.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11249-6.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11239-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11222-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11239-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11240-10.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11222-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11231-6.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11240-9.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11236-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11240-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11236-6.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11231-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11231-8.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11240-12.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11231-9.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11231-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11247-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11240-11.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11222-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11236-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11247-10.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11236-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11236-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11239-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11240-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11240-8.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11240-6.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11240-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11236-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11236-7.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11231-7.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11231-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11240-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11220-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11231-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11231-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11240-7.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11240-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11217.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11171.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-10.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-14.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11194.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/111212-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11169-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-5.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11169-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-4.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11142-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11218-2.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11218-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-7.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11125-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11218-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11142-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-6.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-12.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11129.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-3.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-15.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-9.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11216.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11125-1.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-13.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-8.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11215-11.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11174.jpg\n",
            "Saved: /content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output/11125-2.jpg\n",
            "Processing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ACCURACY CHECK**"
      ],
      "metadata": {
        "id": "jnSdnZ3O7hCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "predicted_output_dir = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output\"\n",
        "test_lab_dir = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/test_lab\"\n",
        "\n",
        "# Check if folders exist\n",
        "print(f\"Predicted Output Folder Exists: {os.path.exists(predicted_output_dir)}\")\n",
        "print(f\"Test Lab Folder Exists: {os.path.exists(test_lab_dir)}\")\n",
        "\n",
        "# Check number of files\n",
        "pred_files = os.listdir(predicted_output_dir)\n",
        "test_lab_files = os.listdir(test_lab_dir)\n",
        "\n",
        "print(f\"Total Predicted Files: {len(pred_files)}\")\n",
        "print(f\"Total Ground Truth Files: {len(test_lab_files)}\")\n",
        "\n",
        "# Print mismatched files\n",
        "mismatched_files = [f for f in pred_files if f not in test_lab_files]\n",
        "if mismatched_files:\n",
        "    print(f\"Mismatched Files (Missing in test_lab): {mismatched_files}\")\n",
        "else:\n",
        "    print(\" All predicted files have corresponding test_lab files.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akuC2nNo5ciI",
        "outputId": "4575b594-0e08-4553-de97-cb02438170fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Output Folder Exists: True\n",
            "Test Lab Folder Exists: True\n",
            "Total Predicted Files: 237\n",
            "Total Ground Truth Files: 237\n",
            "Mismatched Files (Missing in test_lab): ['IMG_6522-3.jpg', 'IMG_6536-4.jpg', 'IMG_6538-3.jpg', 'IMG_6544-1.jpg', 'IMG_6516-4.jpg', 'IMG_6542-2.jpg', 'IMG_6516-3.jpg', 'IMG_6516-2.jpg', 'IMG_6526-2.jpg', 'IMG_6538-1.jpg', 'IMG_6542-4.jpg', 'IMG_6542-3.jpg', 'IMG_6522-1.jpg', 'IMG_6537-4.jpg', 'IMG_6526-3.jpg', 'IMG_6537-1.jpg', 'IMG_6533-3.jpg', 'IMG_6516-1.jpg', 'IMG_6544-3.jpg', 'IMG_6542-1.jpg', 'IMG_6526-4.jpg', 'IMG_6544-4.jpg', 'IMG_6536-1.jpg', 'IMG_6544-2.jpg', 'IMG_6536-7.jpg', 'IMG_6528-3.jpg', 'IMG_6536-3.jpg', 'IMG_6537-2.jpg', 'IMG_6536-2.jpg', 'IMG_6542-5.jpg', 'IMG_6536-5.jpg', 'IMG_6542-7.jpg', 'IMG_6526-1.jpg', 'IMG_6528-1.jpg', 'IMG_6537-3.jpg', 'IMG_6537-5.jpg', '11301-6.jpg', '11296-8.jpg', 'IMG_6472-4.jpg', '11308-1.jpg', '11301-3.jpg', '11296-7.jpg', 'IMG_6472-5.jpg', '11309-1.jpg', 'IMG_6469-2.jpg', '11296-5.jpg', 'IMG_6469-4.jpg', '11301-4.jpg', 'IMG_5778-1.jpg', 'IMG_6514-3.jpg', 'IMG_6472-7.jpg', 'IMG6-1.jpg', '11305-2.jpg', 'IMG_5776-1.jpg', '11296-6.jpg', 'IMG_6514-2.jpg', '11302-2.jpg', '11296-9.jpg', 'IMG_6514-1.jpg', '11301-1.jpg', '11301-5.jpg', 'IMG_6469-5.jpg', '11304.jpg', 'IMG24-3.jpg', 'IMG_6472-8.jpg', 'IMG_6472-2.jpg', 'IMG_5776-2.jpg', 'IMG_6469-3.jpg', 'IMG_6469-7.jpg', 'IMG109-1.jpg', 'IMG_6472-6.jpg', 'IMG_6472-1.jpg', 'IMG_6469-1.jpg', 'IMG24-1.jpg', 'IMG24-2.jpg', '11305-1.jpg', '11308-2.jpg', 'IMG_6472-3.jpg', '11302-1.jpg', '11300.jpg', 'IMG_6469-6.jpg', '11301-2.jpg', 'IMG6-2.jpg', '11289-7.jpg', '11296-20.jpg', '11296-17.jpg', '11289-6.jpg', '11296-23.jpg', '11294.jpg', '11296-19.jpg', '11296-13.jpg', '11296-11.jpg', '11296-3.jpg', '11286-1.jpg', '11296-2.jpg', '11271-6.jpg', '11289-9.jpg', '11296-10.jpg', '11289-1.jpg', '11296-14.jpg', '11295.jpg', '11296-4.jpg', '11276-1.jpg', '11296-18.jpg', '11289-4.jpg', '11289-2.jpg', '11296-15.jpg', '11286-3.jpg', '11284-1.jpg', '11296-22.jpg', '11289-8.jpg', '11286-2.jpg', '11271-2.jpg', '11271-4.jpg', '11271-3.jpg', '11296-21.jpg', '11296-16.jpg', '11296-1.jpg', '11289-11.jpg', '11275-2.jpg', '11289-5.jpg', '11275-3.jpg', '11275-1.jpg', '11289-3.jpg', '11271-5.jpg', '11296-12.jpg', '11289-10.jpg', '11269-1.jpg', '11248-1.jpg', '11247-7.jpg', '11267-2.jpg', '11247-8.jpg', '11247-2.jpg', '11247-3.jpg', '11249-5.jpg', '11266-3.jpg', '11247-4.jpg', '11266-1.jpg', '11267-4.jpg', '11269-4.jpg', '11269-3.jpg', '11249-8.jpg', '11258-1.jpg', '11266-2.jpg', '11249-7.jpg', '11269-6.jpg', '11248-2.jpg', '11249-2.jpg', '11249-1.jpg', '11269-5.jpg', '11267-5.jpg', '11249-3.jpg', '11270-2.jpg', '11248-3.jpg', '11247-9.jpg', '11247-6.jpg', '11266-4.jpg', '11267-1.jpg', '11266-5.jpg', '11258-2.jpg', '11247-5.jpg', '11267-3.jpg', '11269-2.jpg', '11271-1.jpg', '11249-4.jpg', '11270-1.jpg', '11247-12.jpg', '11249-6.jpg', '11239-1.jpg', '11222-3.jpg', '11239-3.jpg', '11240-10.jpg', '11222-2.jpg', '11231-6.jpg', '11240-9.jpg', '11236-2.jpg', '11240-5.jpg', '11236-6.jpg', '11231-3.jpg', '11231-8.jpg', '11240-12.jpg', '11231-9.jpg', '11231-5.jpg', '11247-1.jpg', '11240-11.jpg', '11222-1.jpg', '11236-5.jpg', '11247-10.jpg', '11236-3.jpg', '11236-4.jpg', '11239-2.jpg', '11240-2.jpg', '11240-8.jpg', '11240-6.jpg', '11240-3.jpg', '11236-1.jpg', '11236-7.jpg', '11231-7.jpg', '11231-2.jpg', '11240-4.jpg', '11220-1.jpg', '11231-4.jpg', '11231-1.jpg', '11240-7.jpg', '11240-1.jpg', '11217.jpg', '11171.jpg', '11215-10.jpg', '11215-14.jpg', '11215-1.jpg', '11194.jpg', '111212-1.jpg', '11215-2.jpg', '11169-2.jpg', '11215-5.jpg', '11169-1.jpg', '11215-4.jpg', '11142-2.jpg', '11218-2.jpg', '11218-1.jpg', '11215-7.jpg', '11125-3.jpg', '11218-3.jpg', '11142-1.jpg', '11215-6.jpg', '11215-12.jpg', '11129.jpg', '11215-3.jpg', '11215-15.jpg', '11215-9.jpg', '11216.jpg', '11125-1.jpg', '11215-13.jpg', '11215-8.jpg', '11215-11.jpg', '11174.jpg', '11125-2.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RENAMING THE OUTPUT IMAGES TO MATCH**"
      ],
      "metadata": {
        "id": "Thssc7I6-GVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "predicted_output_dir = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output\"\n",
        "test_lab_dir = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/test_lab\"\n",
        "\n",
        "# Get filenames\n",
        "pred_files = os.listdir(predicted_output_dir)\n",
        "test_lab_files = os.listdir(test_lab_dir)\n",
        "\n",
        "# Extract base filenames (without extra numbers)\n",
        "def clean_filename(filename):\n",
        "    return re.sub(r\"-\\d+\", \"\", filename)  # Removes \"-1\", \"-2\", etc.\n",
        "\n",
        "# Create a mapping of clean names\n",
        "file_mapping = {}\n",
        "for file in pred_files:\n",
        "    cleaned_name = clean_filename(file)\n",
        "    if cleaned_name in test_lab_files:\n",
        "        file_mapping[file] = cleaned_name\n",
        "\n",
        "# Rename files to match test_lab\n",
        "for old_name, new_name in file_mapping.items():\n",
        "    old_path = os.path.join(predicted_output_dir, old_name)\n",
        "    new_path = os.path.join(predicted_output_dir, new_name)\n",
        "    os.rename(old_path, new_path)\n",
        "\n",
        "print(\" Files Renamed! Re-run the accuracy check.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_zh9IVH5mbd",
        "outputId": "124b3a88-13ee-4e0c-ed50-7e6675e41255"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Files Renamed! Re-run the accuracy check.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CONVERTING THE OUTPUT IMAGES FROM JPG TO PNG**"
      ],
      "metadata": {
        "id": "MSl7cJ_U-NSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "predicted_output_dir = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output\"\n",
        "\n",
        "for filename in os.listdir(predicted_output_dir):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        new_filename = filename.replace(\".jpg\", \".png\")\n",
        "        old_path = os.path.join(predicted_output_dir, filename)\n",
        "        new_path = os.path.join(predicted_output_dir, new_filename)\n",
        "        os.rename(old_path, new_path)\n",
        "        print(f\"Renamed: {filename}  {new_filename}\")\n",
        "\n",
        "print(\" All .jpg files renamed to .png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mICwyO-l7kmZ",
        "outputId": "446c2fd5-2189-48e2-838d-e2441962aafb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " All .jpg files renamed to .png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **COMPARING THE PREDICTED TO THE ACTUAL**"
      ],
      "metadata": {
        "id": "4QJ32MWp-XcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.metrics import jaccard_score, f1_score\n",
        "\n",
        "# Paths\n",
        "pred_folder = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/predicted_output\"\n",
        "gt_folder = \"/content/drive/MyDrive/INTUTE_AI/MARBLE_CRACK_PROJECT/DeepCrack/test_lab\"\n",
        "\n",
        "# Get predicted and ground truth filenames\n",
        "pred_files = {os.path.splitext(f)[0]: f for f in os.listdir(pred_folder)}\n",
        "gt_files = {os.path.splitext(f)[0]: f for f in os.listdir(gt_folder)}\n",
        "\n",
        "# Ensure we evaluate only on matching filenames\n",
        "common_files = set(pred_files.keys()) & set(gt_files.keys())\n",
        "\n",
        "ious = []\n",
        "dices = []\n",
        "\n",
        "for file in common_files:\n",
        "    pred_path = os.path.join(pred_folder, pred_files[file])\n",
        "    gt_path = os.path.join(gt_folder, gt_files[file])\n",
        "\n",
        "    # Load images\n",
        "    pred_img = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)\n",
        "    gt_img = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if pred_img is None or gt_img is None:\n",
        "        print(f\"Skipping {file}: Could not load images correctly.\")\n",
        "        continue\n",
        "\n",
        "    # Threshold to binary\n",
        "    _, pred_bin = cv2.threshold(pred_img, 127, 1, cv2.THRESH_BINARY)\n",
        "    _, gt_bin = cv2.threshold(gt_img, 127, 1, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Flatten arrays\n",
        "    pred_flat = pred_bin.flatten()\n",
        "    gt_flat = gt_bin.flatten()\n",
        "\n",
        "    # Compute IoU and Dice\n",
        "    from skimage.transform import resize\n",
        "\n",
        "# Resize predicted image to match ground truth dimensions\n",
        "pred_img_resized = resize(pred_img, gt_img.shape, anti_aliasing=True)\n",
        "\n",
        "# Convert to binary (thresholding)\n",
        "pred_bin = (pred_img_resized > 0.5).astype(int)\n",
        "gt_bin = (gt_img > 0.5).astype(int)\n",
        "\n",
        "# Flatten\n",
        "pred_flat = pred_bin.flatten()\n",
        "gt_flat = gt_bin.flatten()\n",
        "\n",
        "iou = jaccard_score(gt_flat, pred_flat, average='binary')\n",
        "dice = f1_score(gt_flat, pred_flat, average='binary')\n",
        "\n",
        "ious.append(iou)\n",
        "dices.append(dice)\n",
        "\n",
        "\n",
        "# Compute mean scores\n",
        "mean_iou = np.mean(ious) if ious else float('nan')\n",
        "mean_dice = np.mean(dices) if dices else float('nan')\n",
        "\n",
        "\n",
        "print(\"Model Evaluation Results:\")\n",
        "print(f\"Mean IoU Score: {mean_iou:.4f}\")\n",
        "print(f\"Mean Dice Coefficient: {mean_dice:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKbByLNd7xTd",
        "outputId": "084273b4-b099-4af2-a9f3-30cef3a6faba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Evaluation Results:\n",
            "Mean IoU Score: 0.4389\n",
            "Mean Dice Coefficient: 0.6101\n"
          ]
        }
      ]
    }
  ]
}